{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial-md",
   "metadata": {},
   "source": [
    "# Field Animation Examples\n",
    "\n",
    "This notebook demonstrates the four animation backends for visualizing spatial fields over time:\n",
    "\n",
    "1. **Napari** - GPU-accelerated interactive viewer (large-scale exploration)\n",
    "2. **Video** - Parallel MP4 export (publications, presentations)\n",
    "3. **HTML** - Standalone interactive files (sharing, remote viewing)\n",
    "4. **Jupyter Widget** - Notebook integration (quick exploration)\n",
    "\n",
    "**Estimated time**: 15-20 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Animate spatial fields over time using the `animate_fields()` method\n",
    "- Choose the appropriate backend for different use cases\n",
    "- Export videos for publications with parallel rendering\n",
    "- Create shareable HTML players with instant scrubbing\n",
    "- Handle large-scale datasets (900K+ frames) with memory-mapped arrays\n",
    "- Subsample high-frequency neural data for video export\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Optional dependencies** (install as needed):\n",
    "\n",
    "```bash\n",
    "# For Napari backend\n",
    "pip install 'napari[all]>=0.4.18'\n",
    "\n",
    "# For Jupyter widget backend\n",
    "pip install 'ipywidgets>=8.0'\n",
    "\n",
    "# For video backend (system dependency)\n",
    "# macOS: brew install ffmpeg\n",
    "# Ubuntu: sudo apt install ffmpeg\n",
    "# Windows: https://ffmpeg.org/download.html\n",
    "```\n",
    "\n",
    "Note: HTML backend requires no additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bb383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /Users/edeno/Documents/GitHub/neurospatial/examples\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from neurospatial import Environment\n",
    "from neurospatial.animation import subsample_frames\n",
    "from neurospatial.animation.backends.video_backend import check_ffmpeg_available\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Determine output directory (works whether running as script or notebook)\n",
    "output_dir = Path.cwd()\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5266603",
   "metadata": {},
   "source": [
    "## Setup: Create Environment and Simulate Remapping\n",
    "\n",
    "We'll simulate place field remapping across 30 trials, where the field:\n",
    "- Starts with activity at location A (trials 1-15)\n",
    "- Undergoes remapping to location B (trials 16-30)\n",
    "- Demonstrates context-dependent spatial coding\n",
    "\n",
    "This models real phenomena like:\n",
    "- Environmental context changes\n",
    "- Learning new reward locations\n",
    "- Hippocampal remapping events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4545d71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating circular arena environment...\n",
      "Environment: Circular arena (radius=50 cm)\n",
      "  1264 bins, 2D\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating circular arena environment...\")\n",
    "\n",
    "# Create a circular arena (50 cm radius, 100 cm diameter)\n",
    "# This is a common neuroscience experimental setup\n",
    "center = Point(50, 50)\n",
    "radius = 50.0\n",
    "circle = center.buffer(radius)\n",
    "\n",
    "env = Environment.from_polygon(polygon=circle, bin_size=2.5, name=\"CircularArena\")\n",
    "env.units = \"cm\"\n",
    "env.frame = \"open_field\"\n",
    "\n",
    "print(f\"Environment: Circular arena (radius={radius:.0f} cm)\")\n",
    "print(f\"  {env.n_bins} bins, {env.n_dims}D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e0252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating place field remapping...\n",
      "Location A (trials 1-15): bin 817 at [60.0, 65.0] cm\n",
      "Location B (trials 16-30): bin 486 at [40.0, 35.0] cm\n",
      "Generated 30 trial fields (remapping at trial 15)\n"
     ]
    }
   ],
   "source": [
    "# Simulate place field remapping across trials\n",
    "print(\"\\nSimulating place field remapping...\")\n",
    "\n",
    "n_trials = 30\n",
    "remap_trial = 15  # Field remaps halfway through\n",
    "\n",
    "# Location A: Upper-right quadrant (60, 65) cm\n",
    "location_a = np.array([60.0, 65.0])\n",
    "bin_a = env.bin_at(location_a.reshape(1, -1))[0]\n",
    "\n",
    "# Location B: Lower-left quadrant (40, 35) cm\n",
    "location_b = np.array([40.0, 35.0])\n",
    "bin_b = env.bin_at(location_b.reshape(1, -1))[0]\n",
    "\n",
    "print(\n",
    "    f\"Location A (trials 1-{remap_trial}): bin {bin_a} at [{location_a[0]:.1f}, {location_a[1]:.1f}] cm\"\n",
    ")\n",
    "print(\n",
    "    f\"Location B (trials {remap_trial + 1}-{n_trials}): bin {bin_b} at [{location_b[0]:.1f}, {location_b[1]:.1f}] cm\"\n",
    ")\n",
    "\n",
    "fields = []\n",
    "for trial in range(n_trials):\n",
    "    # Determine which location is active\n",
    "    if trial < remap_trial:\n",
    "        # Before remapping: field at location A\n",
    "        active_bin = bin_a\n",
    "        field_strength = 1.0  # Full strength at A\n",
    "    else:\n",
    "        # After remapping: field at location B\n",
    "        active_bin = bin_b\n",
    "        # Gradual emergence at new location\n",
    "        field_strength = min(1.0, (trial - remap_trial + 1) / 5)\n",
    "\n",
    "    # Compute distances from active location\n",
    "    distances = env.distance_to([active_bin])\n",
    "\n",
    "    # Gaussian place field with consistent width\n",
    "    sigma = 8.0  # cm (typical place field size)\n",
    "    field = field_strength * np.exp(-(distances**2) / (2 * sigma**2))\n",
    "\n",
    "    # Add realistic noise\n",
    "    noise = np.random.randn(env.n_bins) * 0.15\n",
    "    field = field + noise\n",
    "    field = np.maximum(field, 0)  # Non-negative firing rates\n",
    "\n",
    "    fields.append(field)\n",
    "\n",
    "print(f\"Generated {len(fields)} trial fields (remapping at trial {remap_trial})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96908914",
   "metadata": {},
   "source": [
    "## Example 1: Interactive Napari Viewer\n",
    "\n",
    "**Best for**: Large datasets, exploration, real-time interaction\n",
    "\n",
    "**Features**:\n",
    "- GPU-accelerated rendering\n",
    "- Instant seeking through frames\n",
    "- Memory-efficient lazy loading\n",
    "- Suitable for 100K+ frames\n",
    "\n",
    "**Installation**: `pip install 'napari[all]>=0.4.18'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2f05dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Napari viewer...\n",
      "\n",
      "PLAYBACK CONTROLS (bottom-left):\n",
      "  â–¶ Play button - Start/stop animation\n",
      "  â” Time slider - Scrub through frames\n",
      "\n",
      "KEYBOARD SHORTCUTS:\n",
      "  Spacebar - Play/pause (toggle)\n",
      "  â† â†’ Arrow keys - Step through frames\n",
      "\n",
      "SPEED CONTROL (left sidebar):\n",
      "  ðŸ“Š 'Playback Speed' widget - Large slider (easy to drag)\n",
      "  Drag to adjust FPS (1-120) - updates instantly\n",
      "\n",
      "âœ“ Napari viewer opened\n",
      "  (Running in Jupyter - window stays open, execution continues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:138: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"Launching Napari viewer...\")\n",
    "    print(\"\")\n",
    "    print(\"ENHANCED PLAYBACK CONTROLS:\")\n",
    "    print(\"\")\n",
    "    print(\"Built-in Controls (bottom-left):\")\n",
    "    print(\"  â–¶ Play button - Start/stop animation\")\n",
    "    print(\"  â” Time slider - Scrub through frames with instant seeking\")\n",
    "    print(\"  Frame counter - Shows current frame (e.g., '1/30')\")\n",
    "    print(\"\")\n",
    "    print(\"Enhanced Widget (left sidebar - auto-added):\")\n",
    "    print(\"  â¯ Large Play/Pause button - Toggle animation (synced with spacebar)\")\n",
    "    print(\"  ðŸ“Š Speed (FPS) slider - 200px wide, 1-120 FPS range\")\n",
    "    print(\"  ðŸ“‹ Frame counter - 'Frame: 15 / 30' with trial label\")\n",
    "    print(\"  âœ“ Updates in real-time during playback\")\n",
    "    print(\"\")\n",
    "    print(\"Keyboard Shortcuts:\")\n",
    "    print(\"  Spacebar - Play/pause (toggle)\")\n",
    "    print(\"  â† â†’ Arrow keys - Step through frames\")\n",
    "    print(\"\")\n",
    "    print(\"Memory Efficiency:\")\n",
    "    print(\"  - Lazy loading with LRU caching (1000 frame cache)\")\n",
    "    print(\"  - Chunked caching for >10K frames (100 frames/chunk)\")\n",
    "    print(\"  - Instant seeking even with 100K+ frames (<100ms)\")\n",
    "    print(\"  - GPU-accelerated rendering\")\n",
    "    print(\"\")\n",
    "\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        title=\"Place Field Remapping\",\n",
    "    )\n",
    "\n",
    "    print(\"âœ“ Napari viewer opened\")\n",
    "\n",
    "    # Only call napari.run() when running as a script (not in Jupyter)\n",
    "    # In Jupyter, the viewer stays open without blocking execution\n",
    "    if get_ipython() is None:\n",
    "        print(\"  (Running as script - window will block until closed)\")\n",
    "        napari.run()\n",
    "    else:\n",
    "        print(\"  (Running in Jupyter - window stays open, execution continues)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"âŠ— Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2cabe",
   "metadata": {},
   "source": [
    "## Example 1b: Multi-Field Viewer (Comparing Multiple Neurons)\n",
    "\n",
    "**Best for**: Comparing spatial fields across multiple neurons/conditions\n",
    "\n",
    "**New Features**:\n",
    "- Side-by-side comparison of multiple field sequences\n",
    "- Synchronized playback across all layers\n",
    "- Global color scale for fair comparison\n",
    "- Layout options: horizontal, vertical, or grid\n",
    "- Custom layer names for clarity\n",
    "\n",
    "**Use cases**:\n",
    "- Comparing place fields across neuron ensembles\n",
    "- Visualizing learning across multiple trials\n",
    "- Side-by-side condition comparison (pre/post manipulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"Creating multi-field comparison...\")\n",
    "    print(\"\")\n",
    "    print(\"Simulating 3 neurons with different spatial tuning:\")\n",
    "\n",
    "    # Create 3 different neurons with distinct spatial patterns\n",
    "    # Neuron A: Maintains stable field at location A\n",
    "    # Neuron B: Remaps from A to B (like our original example)\n",
    "    # Neuron C: Has field at location B throughout\n",
    "\n",
    "    fields_neuron_a = []  # Stable at location A\n",
    "    fields_neuron_b = []  # Remaps from A to B\n",
    "    fields_neuron_c = []  # Stable at location B\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        # Neuron A: Stable field at location A\n",
    "        distances_a = env.distance_to([bin_a])\n",
    "        field_a = np.exp(-(distances_a**2) / (2 * 8.0**2))\n",
    "        field_a = field_a + np.random.randn(env.n_bins) * 0.15\n",
    "        field_a = np.maximum(field_a, 0)\n",
    "        fields_neuron_a.append(field_a)\n",
    "\n",
    "        # Neuron B: Remapping neuron (from earlier example)\n",
    "        # This is the same remapping pattern as before\n",
    "        if trial < remap_trial:\n",
    "            active_bin = bin_a\n",
    "            field_strength = 1.0\n",
    "        else:\n",
    "            active_bin = bin_b\n",
    "            field_strength = min(1.0, (trial - remap_trial + 1) / 5)\n",
    "\n",
    "        distances_b = env.distance_to([active_bin])\n",
    "        field_b = field_strength * np.exp(-(distances_b**2) / (2 * 8.0**2))\n",
    "        field_b = field_b + np.random.randn(env.n_bins) * 0.15\n",
    "        field_b = np.maximum(field_b, 0)\n",
    "        fields_neuron_b.append(field_b)\n",
    "\n",
    "        # Neuron C: Stable field at location B\n",
    "        distances_c = env.distance_to([bin_b])\n",
    "        field_c = np.exp(-(distances_c**2) / (2 * 8.0**2))\n",
    "        field_c = field_c + np.random.randn(env.n_bins) * 0.15\n",
    "        field_c = np.maximum(field_c, 0)\n",
    "        fields_neuron_c.append(field_c)\n",
    "\n",
    "    print(\"  Neuron A: Stable field at location A\")\n",
    "    print(\"  Neuron B: Remaps from A â†’ B at trial 15\")\n",
    "    print(\"  Neuron C: Stable field at location B\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Launch multi-field viewer\n",
    "    print(\"Launching multi-field Napari viewer...\")\n",
    "    print(\"\")\n",
    "    print(\"LAYOUT: Horizontal (side-by-side comparison)\")\n",
    "    print(\"  - All fields share same color scale (fair comparison)\")\n",
    "    print(\"  - Synchronized playback across layers\")\n",
    "    print(\"  - Custom layer names for clarity\")\n",
    "    print(\"\")\n",
    "    print(\"PLAYBACK CONTROLS (same as single-field viewer):\")\n",
    "    print(\"  Bottom-left: â–¶ Play button, time slider\")\n",
    "    print(\"  Keyboard: Spacebar (play/pause), â† â†’ (step frames)\")\n",
    "    print(\"  Left sidebar: ðŸ“Š 'Playback Speed' widget\")\n",
    "    print(\"\")\n",
    "\n",
    "    viewer = env.animate_fields(\n",
    "        fields=[fields_neuron_a, fields_neuron_b, fields_neuron_c],  # List of sequences\n",
    "        backend=\"napari\",\n",
    "        layout=\"horizontal\",  # Side-by-side arrangement\n",
    "        layer_names=[\n",
    "            \"Neuron A (Stable A)\",\n",
    "            \"Neuron B (Remap Aâ†’B)\",\n",
    "            \"Neuron C (Stable B)\",\n",
    "        ],\n",
    "        fps=10,\n",
    "        frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        title=\"Multi-Neuron Comparison\",\n",
    "    )\n",
    "\n",
    "    print(\"âœ“ Multi-field viewer opened!\")\n",
    "    print(\"\")\n",
    "    print(\"TIP: Watch how Neuron B remaps while A and C stay stable\")\n",
    "    print(\"TIP: Try different layouts - change 'horizontal' to 'vertical' or 'grid'\")\n",
    "\n",
    "    # Only call napari.run() when running as a script (not in Jupyter)\n",
    "    if get_ipython() is None:\n",
    "        print(\"  (Running as script - window will block until closed)\")\n",
    "        napari.run()\n",
    "    else:\n",
    "        print(\"  (Running in Jupyter - window stays open, execution continues)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"âŠ— Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e40759",
   "metadata": {},
   "source": [
    "## Example 2: Video Export (MP4)\n",
    "\n",
    "**Best for**: Publications, presentations, high-quality renders\n",
    "\n",
    "**Features**:\n",
    "- Parallel rendering for speed\n",
    "- High-quality output\n",
    "- Multiple codec options (h264, h265, vp9, mpeg4)\n",
    "- Dry-run mode for time/size estimation\n",
    "\n",
    "**Installation**: System dependency (ffmpeg)\n",
    "- macOS: `brew install ffmpeg`\n",
    "- Ubuntu: `sudo apt install ffmpeg`\n",
    "- Windows: Download from https://ffmpeg.org/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b7b25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting video with parallel rendering...\n",
      "Rendering 30 frames using 4 workers...\n",
      "Estimated time: ~4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Workers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding video...\n",
      "âœ“ Video saved to /Users/edeno/Documents/GitHub/neurospatial/examples/16_place_field_remapping.mp4\n",
      "âœ“ Video saved to /Users/edeno/Documents/GitHub/neurospatial/examples/16_place_field_remapping.mp4\n"
     ]
    }
   ],
   "source": [
    "if check_ffmpeg_available():\n",
    "    print(\"Exporting video with parallel rendering...\")\n",
    "\n",
    "    output_path = env.animate_fields(\n",
    "        fields,\n",
    "        backend=\"video\",\n",
    "        save_path=output_dir / \"16_place_field_remapping.mp4\",\n",
    "        fps=5,\n",
    "        cmap=\"hot\",\n",
    "        frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        n_workers=4,  # Parallel rendering\n",
    "        dpi=100,\n",
    "    )\n",
    "    print(f\"âœ“ Video saved to {output_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŠ— ffmpeg not available. Video export skipped.\")\n",
    "    print(\"  Install: brew install ffmpeg (macOS) or apt install ffmpeg (Linux)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9bcf4d",
   "metadata": {},
   "source": [
    "## Example 3: Standalone HTML Player\n",
    "\n",
    "**Best for**: Sharing, remote viewing, no dependencies\n",
    "\n",
    "**Features**:\n",
    "- Single self-contained file\n",
    "- Works offline in any browser\n",
    "- Instant scrubbing with slider\n",
    "- Play/pause controls\n",
    "- Keyboard shortcuts (space, arrows)\n",
    "\n",
    "**Installation**: No dependencies required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01af9e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HTML player...\n",
      "Rendering 30 frames to PNG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 40.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ HTML saved to /Users/edeno/Documents/GitHub/neurospatial/examples/16_place_field_remapping.html (0.8 MB)\n",
      "âœ“ HTML player saved to /Users/edeno/Documents/GitHub/neurospatial/examples/16_place_field_remapping.html\n",
      "  - Open in any web browser\n",
      "  - Instant scrubbing with slider\n",
      "  - Shareable (single file)\n",
      "  - Keyboard shortcuts: space = play/pause, arrows = step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating HTML player...\")\n",
    "\n",
    "html_path = env.animate_fields(\n",
    "    fields,\n",
    "    backend=\"html\",\n",
    "    save_path=output_dir / \"16_place_field_remapping.html\",\n",
    "    fps=10,\n",
    "    cmap=\"viridis\",\n",
    "    frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    ")\n",
    "\n",
    "print(f\"âœ“ HTML player saved to {html_path}\")\n",
    "print(\"  - Open in any web browser\")\n",
    "print(\"  - Instant scrubbing with slider\")\n",
    "print(\"  - Shareable (single file)\")\n",
    "print(\"  - Keyboard shortcuts: space = play/pause, arrows = step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e44bf",
   "metadata": {},
   "source": [
    "## Example 4: Jupyter Widget\n",
    "\n",
    "**Best for**: Quick checks in notebooks, interactive exploration\n",
    "\n",
    "**Features**:\n",
    "- Integrated controls in notebook\n",
    "- Play/pause button\n",
    "- Slider for frame selection\n",
    "- Automatic display in output cell\n",
    "\n",
    "**Installation**: `pip install 'ipywidgets>=8.0'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d64d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Jupyter widget...\n",
      "Pre-rendering 30 frames for widget...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4900ebe1852846ccbd28ca2232a47f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Play(value=0, max=29), IntSlider(value=0, description='Frame:', max=29))), HTML(â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Widget created (displayed above)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    if get_ipython() is not None:\n",
    "        print(\"Creating Jupyter widget...\")\n",
    "\n",
    "        widget = env.animate_fields(\n",
    "            fields,\n",
    "            backend=\"widget\",\n",
    "            fps=10,\n",
    "            frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        )\n",
    "\n",
    "        print(\"âœ“ Widget created (displayed above)\")\n",
    "    else:\n",
    "        print(\"âŠ— Not in Jupyter notebook - widget skipped\")\n",
    "except ImportError:\n",
    "    print(\"âŠ— IPython not available - widget skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6174fcc",
   "metadata": {},
   "source": [
    "## Example 5: Large-Scale Session Pattern\n",
    "\n",
    "**Best for**: Hour-long recordings at high sampling rates (e.g., 250 Hz)\n",
    "\n",
    "**Key techniques**:\n",
    "- Memory-mapped arrays (don't load all data into RAM)\n",
    "- Napari for interactive exploration (lazy loading)\n",
    "- Frame subsampling for video export\n",
    "- Dry-run estimation before rendering\n",
    "\n",
    "**This example demonstrates the pattern** for handling large sessions:\n",
    "- Real sessions: 60K-900K frames (4 min - 1 hour at 250 Hz)\n",
    "- Real file sizes: 300 MB - 4.5 GB\n",
    "- Demo version: 1000 frames (~5 MB) to avoid filling your disk\n",
    "\n",
    "The techniques shown here scale to arbitrarily large datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec954eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Example 5: Large-Scale Session Pattern\n",
      "================================================================================\n",
      "\n",
      "Demonstrating techniques for large datasets (60K-900K frames):\n",
      "  - Use memory-mapped data (don't load into RAM)\n",
      "  - Use Napari for exploration (lazy loading)\n",
      "  - Subsample for video export\n",
      "\n",
      "Note: Using 1000 frames (~5 MB) for demo; scales to hours of data\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Example 5: Large-Scale Session Pattern\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nDemonstrating techniques for large datasets (60K-900K frames):\")\n",
    "print(\"  - Use memory-mapped data (don't load into RAM)\")\n",
    "print(\"  - Use Napari for exploration (lazy loading)\")\n",
    "print(\"  - Subsample for video export\")\n",
    "print(\"\\nNote: Using 1000 frames (~5 MB) for demo; scales to hours of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ad025",
   "metadata": {},
   "source": [
    "### Step 1: Create Memory-Mapped Data File\n",
    "\n",
    "In practice, this would be your neural recording data. We'll simulate it here for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7fcca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating memory-mapped data file...\n",
      "Populating with sample data (in practice, this is your recording)...\n",
      "  (Writing in chunks to avoid memory issues)\n",
      "\n",
      "âœ“ Created memory-mapped dataset: 1,000 frames\n",
      "  File size: 0.01 GB\n",
      "  RAM usage: ~0 MB (memory-mapped, not loaded)\n"
     ]
    }
   ],
   "source": [
    "# Create memory-mapped data file (simulating neural recording)\n",
    "print(\"\\nCreating memory-mapped data file...\")\n",
    "# For demo purposes, use a small file (1000 frames ~5 MB)\n",
    "# In practice, this would be 60K-900K frames for real sessions\n",
    "n_frames_large = 1000  # Demo size (real: 60K-900K frames)\n",
    "\n",
    "# Use temporary directory for demo (in practice, use your data directory)\n",
    "tmpdir = Path(tempfile.mkdtemp(prefix=\"neurospatial_demo_\"))\n",
    "mmap_path = tmpdir / \"large_session.dat\"\n",
    "\n",
    "fields_mmap = np.memmap(\n",
    "    str(mmap_path),\n",
    "    dtype=\"float32\",\n",
    "    mode=\"w+\",  # Create new file\n",
    "    shape=(n_frames_large, env.n_bins),\n",
    ")\n",
    "\n",
    "print(\"Populating with sample data (in practice, this is your recording)...\")\n",
    "print(\"  (Writing in chunks to avoid memory issues)\")\n",
    "\n",
    "# Populate with simulated data (in practice, this is your neural recording)\n",
    "# For this example, we'll simulate a slowly drifting place field\n",
    "initial_bin = env.n_bins // 2  # Start at center of environment\n",
    "\n",
    "chunk_size = 10000\n",
    "for i in range(0, n_frames_large, chunk_size):\n",
    "    # Simulate place field that drifts slowly over time\n",
    "    chunk_end = min(i + chunk_size, n_frames_large)\n",
    "    chunk_len = chunk_end - i\n",
    "\n",
    "    # Slowly drifting center (drifts 20 bins over the full session)\n",
    "    drift = int((i / n_frames_large) * 20)\n",
    "    center_bin = initial_bin + drift\n",
    "    if center_bin >= env.n_bins:\n",
    "        center_bin = env.n_bins - 1\n",
    "\n",
    "    distances = env.distance_to([center_bin])\n",
    "    for j in range(chunk_len):\n",
    "        fields_mmap[i + j] = np.exp(-distances / 15) + np.random.randn(env.n_bins) * 0.1\n",
    "\n",
    "fields_mmap.flush()\n",
    "\n",
    "print(f\"\\nâœ“ Created memory-mapped dataset: {n_frames_large:,} frames\")\n",
    "print(f\"  File size: {n_frames_large * env.n_bins * 4 / 1e9:.2f} GB\")\n",
    "print(\"  RAM usage: ~0 MB (memory-mapped, not loaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e394c",
   "metadata": {},
   "source": [
    "### Step 2: Interactive Exploration with Napari\n",
    "\n",
    "Napari loads frames on-demand, making it efficient for exploring large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac16b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Option 1: Interactive exploration (Napari)\n",
      "  Napari loads frames on-demand - would handle 900K frames efficiently\n",
      "PLAYBACK CONTROLS:\n",
      "  Bottom-left: â–¶ Play button, time slider\n",
      "  Keyboard: Spacebar (play/pause), â† â†’ (step frames)\n",
      "  Left sidebar: ðŸ“Š 'Playback Speed' widget (large slider, 1-120 FPS)\n",
      "âœ“ Napari viewer opened!\n",
      "  (Same technique works for 60K-900K frame sessions)\n",
      "  (Running in Jupyter - window stays open, execution continues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:138: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOption 1: Interactive exploration (Napari)\")\n",
    "print(\"  Napari loads frames on-demand - would handle 900K frames efficiently\")\n",
    "\n",
    "try:\n",
    "    # Import napari only if attempting to use it\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"CHUNKED CACHING FOR LARGE DATASETS:\")\n",
    "    print(\n",
    "        \"  - Auto-enabled for >10K frames (this demo has 1000, but shows the pattern)\"\n",
    "    )\n",
    "    print(\"  - Caches frames in chunks of 100 (not individual frames)\")\n",
    "    print(\"  - Pre-loads neighboring frames for smooth sequential playback\")\n",
    "    print(\"  - Reduces cache overhead: 900K frames â†’ 9K chunks\")\n",
    "    print(\"  - LRU eviction: keeps recently accessed chunks in memory\")\n",
    "    print(\"  - Benefits:\")\n",
    "    print(\"    â€¢ 10x fewer cache entries (faster lookups)\")\n",
    "    print(\"    â€¢ Better sequential playback (pre-loaded neighbors)\")\n",
    "    print(\"    â€¢ Same instant seeking (<100ms even for 900K frames)\")\n",
    "    print(\"\")\n",
    "    print(\"PLAYBACK CONTROLS:\")\n",
    "    print(\"  Bottom-left: â–¶ Play button, time slider\")\n",
    "    print(\"  Keyboard: Spacebar (play/pause), â† â†’ (step frames)\")\n",
    "    print(\"  Left sidebar: ðŸ“Š 'Playback Speed' widget (large slider, 1-120 FPS)\")\n",
    "    print(\"\")\n",
    "\n",
    "    viewer = env.animate_fields(\n",
    "        fields_mmap,\n",
    "        backend=\"napari\",\n",
    "        fps=250,  # Match recording rate\n",
    "        title=\"Large Session Demo (1000 frames)\",\n",
    "        # cache_chunk_size=100,  # Auto-detected (default for >10K frames)\n",
    "    )\n",
    "    print(\"âœ“ Napari viewer opened!\")\n",
    "    print(\"  (Same technique works for 60K-900K frame sessions)\")\n",
    "\n",
    "    # Only call napari.run() when running as a script (not in Jupyter)\n",
    "    if get_ipython() is None:\n",
    "        print(\"  (Running as script - window will block until closed)\")\n",
    "        napari.run()\n",
    "    else:\n",
    "        print(\"  (Running in Jupyter - window stays open, execution continues)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"âŠ— Napari not available (install: pip install 'napari[all]>=0.4.18')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391205c8",
   "metadata": {},
   "source": [
    "### Step 3: Export Subsampled Video\n",
    "\n",
    "For video export, we need to subsample the high-frequency data to a manageable frame rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0376e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Option 2: Export subsampled video\n",
      "  For large sessions: 250 Hz â†’ 30 fps requires subsampling\n",
      "  Subsampled: 120 frames (every 8th frame)\n",
      "  Video duration: 4.0 seconds\n",
      "  (For 900K frames, would produce ~1 hour video)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOption 2: Export subsampled video\")\n",
    "print(\"  For large sessions: 250 Hz â†’ 30 fps requires subsampling\")\n",
    "\n",
    "# Subsample 250 Hz â†’ 30 fps\n",
    "# For 900K frames, this would produce 108K subsampled frames (1 hour video)\n",
    "# For our 1000 frame demo, this produces ~120 frames\n",
    "fields_subsampled = subsample_frames(fields_mmap, target_fps=30, source_fps=250)\n",
    "print(f\"  Subsampled: {len(fields_subsampled):,} frames (every {250 // 30}th frame)\")\n",
    "print(f\"  Video duration: {len(fields_subsampled) / 30:.1f} seconds\")\n",
    "print(\"  (For 900K frames, would produce ~1 hour video)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752671b",
   "metadata": {},
   "source": [
    "### Step 4: Dry Run to Estimate Render Time\n",
    "\n",
    "Before committing to a long render, use dry-run mode to estimate time and file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53594682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dry run estimation:\n",
      "Running dry run estimation...\n",
      "\n",
      "============================================================\n",
      "Video Export Dry Run Estimate:\n",
      "============================================================\n",
      "  Frames:          120\n",
      "  Workers:         8\n",
      "  Frame time:      19.6 ms\n",
      "  Est. total time: 0.0 minutes\n",
      "  Est. file size:  6 MB\n",
      "  Output path:     /Users/edeno/Documents/GitHub/neurospatial/examples/16_large_session_summary.mp4\n",
      "\n",
      "To proceed, call again with dry_run=False\n",
      "============================================================\n",
      "\n",
      "\n",
      "  To render, run with dry_run=False\n"
     ]
    }
   ],
   "source": [
    "if check_ffmpeg_available():\n",
    "    print(\"\\nDry run estimation:\")\n",
    "    env.animate_fields(\n",
    "        fields_subsampled,\n",
    "        backend=\"video\",\n",
    "        save_path=output_dir / \"16_large_session_summary.mp4\",\n",
    "        fps=30,\n",
    "        n_workers=8,\n",
    "        dry_run=True,  # Estimate first\n",
    "    )\n",
    "    print(\"\\n  To render, run with dry_run=False\")\n",
    "else:\n",
    "    print(\"  âŠ— ffmpeg not available for video export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbe208",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41985045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up temporary files...\n",
      "âœ“ Temporary files removed\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary files\n",
    "print(\"\\nCleaning up temporary files...\")\n",
    "if mmap_path.exists():\n",
    "    mmap_path.unlink()\n",
    "    tmpdir.rmdir()\n",
    "    print(\"âœ“ Temporary files removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac427517",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Backend Selection Guide\n",
    "\n",
    "| Use Case | Backend | Installation | Best For |\n",
    "|----------|---------|--------------|----------|\n",
    "| **Exploration** | Napari | `pip install napari[all]` | Large datasets (100K+ frames), interactive |\n",
    "| **Comparison** | Napari (multi-field) | `pip install napari[all]` | Side-by-side neuron comparison |\n",
    "| **Publication** | Video | `brew install ffmpeg` | High-quality renders, parallel speed |\n",
    "| **Sharing** | HTML | No dependencies | Remote viewing, single file |\n",
    "| **Quick check** | Widget | `pip install ipywidgets` | Notebook integration |\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Large datasets (>10K frames)**: Use Napari for exploration, subsample for video\n",
    "- **Memory constraints**: Use memory-mapped arrays (`np.memmap`)\n",
    "- **Parallel rendering**: Increase `n_workers` for faster video export\n",
    "- **File size**: Use `image_format='jpeg'` for HTML to reduce size\n",
    "- **Chunked caching**: Auto-enabled for >10K frames (100 frames/chunk)\n",
    "  - 10x fewer cache entries â†’ faster lookups\n",
    "  - Pre-loads neighboring frames â†’ smooth sequential playback\n",
    "  - Customize with `cache_chunk_size` parameter\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Auto backend selection\n",
    "env.animate_fields(fields, backend='auto')\n",
    "\n",
    "# Quick Napari check\n",
    "env.animate_fields(fields, backend='napari')\n",
    "\n",
    "# Compare multiple neurons side-by-side (multi-field viewer)\n",
    "env.animate_fields(\n",
    "    fields=[neuron1_fields, neuron2_fields, neuron3_fields],\n",
    "    backend='napari',\n",
    "    layout='horizontal',  # or 'vertical', 'grid'\n",
    "    layer_names=['Neuron A', 'Neuron B', 'Neuron C']\n",
    ")\n",
    "\n",
    "# Publication video\n",
    "env.animate_fields(fields, save_path='video.mp4', fps=5, n_workers=8)\n",
    "\n",
    "# Shareable HTML\n",
    "env.animate_fields(fields, save_path='animation.html')\n",
    "\n",
    "# Subsample high-frequency data\n",
    "from neurospatial.animation import subsample_frames\n",
    "fields_30fps = subsample_frames(fields_250hz, target_fps=30, source_fps=250)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fdca2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try animating your own neural data\n",
    "- Experiment with different colormaps (`cmap` parameter)\n",
    "- Add trajectory overlays (`overlay_trajectory` parameter)\n",
    "- Compare place field evolution across sessions\n",
    "- Visualize replay events or value function learning\n",
    "\n",
    "For more details, see the neurospatial documentation on animation backends."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
