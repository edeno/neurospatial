{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial-md",
   "metadata": {},
   "source": [
    "# Field Animation Examples\n",
    "\n",
    "This notebook demonstrates the four animation backends for visualizing spatial fields over time:\n",
    "\n",
    "1. **Napari** - GPU-accelerated interactive viewer (large-scale exploration)\n",
    "2. **Video** - Parallel MP4 export (publications, presentations)\n",
    "3. **HTML** - Standalone interactive files (sharing, remote viewing)\n",
    "4. **Jupyter Widget** - Notebook integration (quick exploration)\n",
    "\n",
    "**Estimated time**: 15-20 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Animate spatial fields over time using the `animate_fields()` method\n",
    "- Choose the appropriate backend for different use cases\n",
    "- Export videos for publications with parallel rendering\n",
    "- Create shareable HTML players with instant scrubbing\n",
    "- Handle large-scale datasets (900K+ frames) with memory-mapped arrays\n",
    "- Subsample high-frequency neural data for video export\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Optional dependencies** (install as needed):\n",
    "\n",
    "```bash\n",
    "# For Napari backend\n",
    "pip install 'napari[all]>=0.4.18'\n",
    "\n",
    "# For Jupyter widget backend\n",
    "pip install 'ipywidgets>=8.0'\n",
    "\n",
    "# For video backend (system dependency)\n",
    "# macOS: brew install ffmpeg\n",
    "# Ubuntu: sudo apt install ffmpeg\n",
    "# Windows: https://ffmpeg.org/download.html\n",
    "```\n",
    "\n",
    "Note: HTML backend requires no additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bb383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from neurospatial import Environment\n",
    "from neurospatial.animation import subsample_frames\n",
    "from neurospatial.animation.backends.video_backend import check_ffmpeg_available\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5266603",
   "metadata": {},
   "source": [
    "## Setup: Create Environment and Simulate Learning\n",
    "\n",
    "We'll simulate place field formation over 30 trials, where the field:\n",
    "- Gradually sharpens (decreasing spatial width)\n",
    "- Becomes more reliable (decreasing noise)\n",
    "- Centers on a goal location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4545d71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating environment...\n",
      "Environment: 695 bins\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating environment...\")\n",
    "\n",
    "# Create a 100x100 cm open field arena with full coverage\n",
    "# This ensures no wasted space and realistic spatial structure\n",
    "arena_size = 100.0  # cm\n",
    "n_grid = 50\n",
    "\n",
    "x = np.linspace(0, arena_size, n_grid)\n",
    "y = np.linspace(0, arena_size, n_grid)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "arena_data = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "env = Environment.from_samples(\n",
    "    arena_data,\n",
    "    bin_size=5.0,\n",
    "    bin_count_threshold=1,\n",
    ")\n",
    "env.units = \"cm\"\n",
    "env.frame = \"open_field\"\n",
    "\n",
    "print(f\"Environment: {arena_size:.0f}x{arena_size:.0f} cm open field\")\n",
    "print(f\"  {env.n_bins} bins, {env.n_dims}D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e0252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating place field learning...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target bin indices must be in range [0, 695), got invalid indices: [-1]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_trials):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Field gradually sharpens\u001b[39;00m\n\u001b[32m     10\u001b[39m     sigma = \u001b[32m30\u001b[39m - trial * \u001b[32m0.5\u001b[39m  \u001b[38;5;66;03m# Decreasing width\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     distances = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistance_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgoal_bin\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Add noise that decreases over trials\u001b[39;00m\n\u001b[32m     14\u001b[39m     noise_level = \u001b[32m0.3\u001b[39m * (\u001b[32m1\u001b[39m - trial / n_trials)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/queries.py:547\u001b[39m, in \u001b[36mEnvironmentQueries.distance_to\u001b[39m\u001b[34m(self, targets, metric)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.any(target_array < \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m np.any(target_array >= \u001b[38;5;28mself\u001b[39m.n_bins):\n\u001b[32m    546\u001b[39m     invalid = target_array[(target_array < \u001b[32m0\u001b[39m) | (target_array >= \u001b[38;5;28mself\u001b[39m.n_bins)]\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    548\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget bin indices must be in range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_bins\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    549\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgot invalid indices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid.tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    550\u001b[39m     )\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# Compute distances based on metric\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metric == \u001b[33m\"\u001b[39m\u001b[33meuclidean\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    554\u001b[39m     \u001b[38;5;66;03m# Euclidean distance: straight-line distance to nearest target\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# Vectorized implementation using broadcasting for performance\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Target bin indices must be in range [0, 695), got invalid indices: [-1]"
     ]
    }
   ],
   "source": [
    "# Simulate place field formation over trials\n",
    "print(\"\\nSimulating place field learning...\")\n",
    "\n",
    "n_trials = 30\n",
    "# Place goal in upper-right quadrant of the arena (60, 70) cm\n",
    "goal_position = np.array([60.0, 70.0])\n",
    "goal_bin = env.bin_at(goal_position.reshape(1, -1))[0]\n",
    "print(\n",
    "    f\"Goal bin: {goal_bin} at position [{goal_position[0]:.1f}, {goal_position[1]:.1f}] cm\"\n",
    ")\n",
    "\n",
    "fields = []\n",
    "for trial in range(n_trials):\n",
    "    # Field gradually sharpens\n",
    "    sigma = 30 - trial * 0.5  # Decreasing width\n",
    "    distances = env.distance_to([goal_bin])\n",
    "\n",
    "    # Add noise that decreases over trials\n",
    "    noise_level = 0.3 * (1 - trial / n_trials)\n",
    "    noise = np.random.randn(env.n_bins) * noise_level\n",
    "\n",
    "    field = np.exp(-(distances**2) / (2 * sigma**2)) + noise\n",
    "    field = np.maximum(field, 0)  # Non-negative\n",
    "\n",
    "    fields.append(field)\n",
    "\n",
    "print(f\"Generated {len(fields)} trial fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96908914",
   "metadata": {},
   "source": [
    "## Example 1: Interactive Napari Viewer\n",
    "\n",
    "**Best for**: Large datasets, exploration, real-time interaction\n",
    "\n",
    "**Features**:\n",
    "- GPU-accelerated rendering\n",
    "- Instant seeking through frames\n",
    "- Memory-efficient lazy loading\n",
    "- Suitable for 100K+ frames\n",
    "\n",
    "**Installation**: `pip install 'napari[all]>=0.4.18'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import napari\n",
    "\n",
    "    print(\"Launching Napari viewer...\")\n",
    "    print(\"  - Use slider to scrub through trials\")\n",
    "    print(\"  - Instant seeking through all frames\")\n",
    "    print(\"  - GPU accelerated\")\n",
    "\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        title=\"Place Field Learning\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer launched\")\n",
    "    print(\"  (Close the viewer window to continue)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e40759",
   "metadata": {},
   "source": [
    "## Example 2: Video Export (MP4)\n",
    "\n",
    "**Best for**: Publications, presentations, high-quality renders\n",
    "\n",
    "**Features**:\n",
    "- Parallel rendering for speed\n",
    "- High-quality output\n",
    "- Multiple codec options (h264, h265, vp9, mpeg4)\n",
    "- Dry-run mode for time/size estimation\n",
    "\n",
    "**Installation**: System dependency (ffmpeg)\n",
    "- macOS: `brew install ffmpeg`\n",
    "- Ubuntu: `sudo apt install ffmpeg`\n",
    "- Windows: Download from https://ffmpeg.org/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_ffmpeg_available():\n",
    "    print(\"Exporting video with parallel rendering...\")\n",
    "\n",
    "    output_path = env.animate_fields(\n",
    "        fields,\n",
    "        backend=\"video\",\n",
    "        save_path=\"examples/16_place_field_learning.mp4\",\n",
    "        fps=5,\n",
    "        cmap=\"hot\",\n",
    "        frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        n_workers=4,  # Parallel rendering\n",
    "        dpi=100,\n",
    "    )\n",
    "    print(f\"✓ Video saved to {output_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"⊗ ffmpeg not available. Video export skipped.\")\n",
    "    print(\"  Install: brew install ffmpeg (macOS) or apt install ffmpeg (Linux)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9bcf4d",
   "metadata": {},
   "source": [
    "## Example 3: Standalone HTML Player\n",
    "\n",
    "**Best for**: Sharing, remote viewing, no dependencies\n",
    "\n",
    "**Features**:\n",
    "- Single self-contained file\n",
    "- Works offline in any browser\n",
    "- Instant scrubbing with slider\n",
    "- Play/pause controls\n",
    "- Keyboard shortcuts (space, arrows)\n",
    "\n",
    "**Installation**: No dependencies required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating HTML player...\")\n",
    "\n",
    "html_path = env.animate_fields(\n",
    "    fields,\n",
    "    backend=\"html\",\n",
    "    save_path=\"examples/16_place_field_learning.html\",\n",
    "    fps=10,\n",
    "    cmap=\"viridis\",\n",
    "    frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    ")\n",
    "\n",
    "print(f\"✓ HTML player saved to {html_path}\")\n",
    "print(\"  - Open in any web browser\")\n",
    "print(\"  - Instant scrubbing with slider\")\n",
    "print(\"  - Shareable (single file)\")\n",
    "print(\"  - Keyboard shortcuts: space = play/pause, arrows = step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e44bf",
   "metadata": {},
   "source": [
    "## Example 4: Jupyter Widget\n",
    "\n",
    "**Best for**: Quick checks in notebooks, interactive exploration\n",
    "\n",
    "**Features**:\n",
    "- Integrated controls in notebook\n",
    "- Play/pause button\n",
    "- Slider for frame selection\n",
    "- Automatic display in output cell\n",
    "\n",
    "**Installation**: `pip install 'ipywidgets>=8.0'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d64d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    if get_ipython() is not None:\n",
    "        print(\"Creating Jupyter widget...\")\n",
    "\n",
    "        widget = env.animate_fields(\n",
    "            fields,\n",
    "            backend=\"widget\",\n",
    "            fps=10,\n",
    "            frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        )\n",
    "\n",
    "        print(\"✓ Widget created (displayed above)\")\n",
    "    else:\n",
    "        print(\"⊗ Not in Jupyter notebook - widget skipped\")\n",
    "except ImportError:\n",
    "    print(\"⊗ IPython not available - widget skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6174fcc",
   "metadata": {},
   "source": [
    "## Example 5: Large-Scale Session (900K frames)\n",
    "\n",
    "**Best for**: Hour-long recordings at high sampling rates (e.g., 250 Hz)\n",
    "\n",
    "**Key techniques**:\n",
    "- Memory-mapped arrays (don't load all data into RAM)\n",
    "- Napari for interactive exploration (lazy loading)\n",
    "- Frame subsampling for video export\n",
    "- Dry-run estimation before rendering\n",
    "\n",
    "This example demonstrates handling a realistic neuroscience session:\n",
    "- 1 hour of recording\n",
    "- 250 Hz sampling rate\n",
    "- 900,000 total frames\n",
    "- ~3.6 GB of data (float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec954eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Example 5: Large-Scale Session (900K frames)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nFor hour-long sessions with 900K frames:\")\n",
    "print(\"  - Use memory-mapped data (don't load into RAM)\")\n",
    "print(\"  - Use Napari for exploration (lazy loading)\")\n",
    "print(\"  - Subsample for video export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ad025",
   "metadata": {},
   "source": [
    "### Step 1: Create Memory-Mapped Data File\n",
    "\n",
    "In practice, this would be your neural recording data. We'll simulate it here for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fcca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create memory-mapped data file (simulating neural recording)\n",
    "print(\"\\nCreating memory-mapped data file...\")\n",
    "n_frames_large = 900_000  # 1 hour at 250 Hz\n",
    "\n",
    "# Use temporary directory for demo (in practice, use your data directory)\n",
    "tmpdir = Path(tempfile.mkdtemp(prefix=\"neurospatial_demo_\"))\n",
    "mmap_path = tmpdir / \"large_session.dat\"\n",
    "\n",
    "fields_mmap = np.memmap(\n",
    "    str(mmap_path),\n",
    "    dtype=\"float32\",\n",
    "    mode=\"w+\",  # Create new file\n",
    "    shape=(n_frames_large, env.n_bins),\n",
    ")\n",
    "\n",
    "print(\"Populating with sample data (in practice, this is your recording)...\")\n",
    "print(\"  (Writing in chunks to avoid memory issues)\")\n",
    "\n",
    "# Populate with simulated data (in practice, this is your neural recording)\n",
    "chunk_size = 10000\n",
    "for i in range(0, n_frames_large, chunk_size):\n",
    "    # Simulate place field that drifts slowly over time\n",
    "    chunk_end = min(i + chunk_size, n_frames_large)\n",
    "    chunk_len = chunk_end - i\n",
    "\n",
    "    # Slowly drifting center\n",
    "    drift = (i / n_frames_large) * 20  # Drifts 20 bins over session\n",
    "    center_bin = goal_bin + int(drift)\n",
    "    if center_bin >= env.n_bins:\n",
    "        center_bin = env.n_bins - 1\n",
    "\n",
    "    distances = env.distance_to([center_bin])\n",
    "    for j in range(chunk_len):\n",
    "        fields_mmap[i + j] = np.exp(-distances / 15) + np.random.randn(env.n_bins) * 0.1\n",
    "\n",
    "fields_mmap.flush()\n",
    "\n",
    "print(f\"\\n✓ Created memory-mapped dataset: {n_frames_large:,} frames\")\n",
    "print(f\"  File size: {n_frames_large * env.n_bins * 4 / 1e9:.2f} GB\")\n",
    "print(\"  RAM usage: ~0 MB (memory-mapped, not loaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e394c",
   "metadata": {},
   "source": [
    "### Step 2: Interactive Exploration with Napari\n",
    "\n",
    "Napari loads frames on-demand, making it efficient for exploring 900K+ frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOption 1: Interactive exploration (Napari)\")\n",
    "print(\"  Napari loads frames on-demand - handles 900K frames efficiently\")\n",
    "\n",
    "try:\n",
    "    # Import napari only if attempting to use it\n",
    "    import napari  # noqa: F401\n",
    "\n",
    "    viewer = env.animate_fields(\n",
    "        fields_mmap,\n",
    "        backend=\"napari\",\n",
    "        fps=250,  # Match recording rate\n",
    "        title=\"Hour-Long Session (900K frames)\",\n",
    "    )\n",
    "    print(\"✓ Napari viewer launched - scrub through 900K frames instantly!\")\n",
    "    print(\"  (Close the viewer window to continue)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available (install: pip install 'napari[all]>=0.4.18')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391205c8",
   "metadata": {},
   "source": [
    "### Step 3: Export Subsampled Video\n",
    "\n",
    "For video export, we need to subsample the high-frequency data to a manageable frame rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOption 2: Export subsampled video\")\n",
    "print(\"  900K frames → 30 fps video requires subsampling\")\n",
    "\n",
    "# Subsample 250 Hz → 30 fps\n",
    "fields_subsampled = subsample_frames(fields_mmap, target_fps=30, source_fps=250)\n",
    "print(f\"  Subsampled: {len(fields_subsampled):,} frames (every {250 // 30}th frame)\")\n",
    "print(f\"  Video duration: {len(fields_subsampled) / 30:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752671b",
   "metadata": {},
   "source": [
    "### Step 4: Dry Run to Estimate Render Time\n",
    "\n",
    "Before committing to a long render, use dry-run mode to estimate time and file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53594682",
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_ffmpeg_available():\n",
    "    print(\"\\nDry run estimation:\")\n",
    "    env.animate_fields(\n",
    "        fields_subsampled,\n",
    "        backend=\"video\",\n",
    "        save_path=\"examples/16_large_session_summary.mp4\",\n",
    "        fps=30,\n",
    "        n_workers=8,\n",
    "        dry_run=True,  # Estimate first\n",
    "    )\n",
    "    print(\"\\n  To render, run with dry_run=False\")\n",
    "else:\n",
    "    print(\"  ⊗ ffmpeg not available for video export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbe208",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41985045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "print(\"\\nCleaning up temporary files...\")\n",
    "if mmap_path.exists():\n",
    "    mmap_path.unlink()\n",
    "    tmpdir.rmdir()\n",
    "    print(\"✓ Temporary files removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac427517",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Backend Selection Guide\n",
    "\n",
    "| Use Case | Backend | Installation | Best For |\n",
    "|----------|---------|--------------|----------|\n",
    "| **Exploration** | Napari | `pip install napari[all]` | Large datasets (100K+ frames), interactive |\n",
    "| **Publication** | Video | `brew install ffmpeg` | High-quality renders, parallel speed |\n",
    "| **Sharing** | HTML | No dependencies | Remote viewing, single file |\n",
    "| **Quick check** | Widget | `pip install ipywidgets` | Notebook integration |\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Large datasets (>10K frames)**: Use Napari for exploration, subsample for video\n",
    "- **Memory constraints**: Use memory-mapped arrays (`np.memmap`)\n",
    "- **Parallel rendering**: Increase `n_workers` for faster video export\n",
    "- **File size**: Use `image_format='jpeg'` for HTML to reduce size\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Auto backend selection\n",
    "env.animate_fields(fields, backend='auto')\n",
    "\n",
    "# Quick Napari check\n",
    "env.animate_fields(fields, backend='napari')\n",
    "\n",
    "# Publication video\n",
    "env.animate_fields(fields, save_path='video.mp4', fps=5, n_workers=8)\n",
    "\n",
    "# Shareable HTML\n",
    "env.animate_fields(fields, save_path='animation.html')\n",
    "\n",
    "# Subsample high-frequency data\n",
    "from neurospatial.animation import subsample_frames\n",
    "fields_30fps = subsample_frames(fields_250hz, target_fps=30, source_fps=250)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fdca2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try animating your own neural data\n",
    "- Experiment with different colormaps (`cmap` parameter)\n",
    "- Add trajectory overlays (`overlay_trajectory` parameter)\n",
    "- Compare place field evolution across sessions\n",
    "- Visualize replay events or value function learning\n",
    "\n",
    "For more details, see the neurospatial documentation on animation backends."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
