{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial-md",
   "metadata": {},
   "source": [
    "# Field Animation Examples\n",
    "\n",
    "This notebook demonstrates the four animation backends for visualizing spatial fields over time:\n",
    "\n",
    "1. **Napari** - GPU-accelerated interactive viewer (large-scale exploration)\n",
    "2. **Video** - Parallel MP4 export (publications, presentations)\n",
    "3. **HTML** - Standalone interactive files (sharing, remote viewing)\n",
    "4. **Jupyter Widget** - Notebook integration (quick exploration)\n",
    "\n",
    "**Estimated time**: 15-20 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Animate spatial fields over time using the `animate_fields()` method\n",
    "- Choose the appropriate backend for different use cases\n",
    "- Export videos for publications with parallel rendering\n",
    "- Create shareable HTML players with instant scrubbing\n",
    "- Handle large-scale datasets (900K+ frames) with memory-mapped arrays\n",
    "- Subsample high-frequency neural data for video export\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Optional dependencies** (install as needed):\n",
    "\n",
    "```bash\n",
    "# For Napari backend\n",
    "pip install 'napari[all]>=0.4.18'\n",
    "\n",
    "# For Jupyter widget backend\n",
    "pip install 'ipywidgets>=8.0'\n",
    "\n",
    "# For video backend (system dependency)\n",
    "# macOS: brew install ffmpeg\n",
    "# Ubuntu: sudo apt install ffmpeg\n",
    "# Windows: https://ffmpeg.org/download.html\n",
    "```\n",
    "\n",
    "Note: HTML backend requires no additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bb383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from neurospatial import Environment\n",
    "from neurospatial.animation import subsample_frames\n",
    "from neurospatial.animation.backends.video_backend import check_ffmpeg_available\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Determine output directory (works whether running as script or notebook)\n",
    "output_dir = Path.cwd()\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5266603",
   "metadata": {},
   "source": [
    "## Setup: Create Environment and Simulate Remapping\n",
    "\n",
    "We'll simulate place field remapping across 30 trials, where the field:\n",
    "- Starts with activity at location A (trials 1-15)\n",
    "- Undergoes remapping to location B (trials 16-30)\n",
    "- Demonstrates context-dependent spatial coding\n",
    "\n",
    "This models real phenomena like:\n",
    "- Environmental context changes\n",
    "- Learning new reward locations\n",
    "- Hippocampal remapping events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4545d71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating circular arena environment...\n",
      "Environment: Circular arena (radius=50 cm)\n",
      "  489 bins, 2D\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating circular arena environment...\")\n",
    "\n",
    "# Create a circular arena (50 cm radius, 100 cm diameter)\n",
    "# This is a common neuroscience experimental setup\n",
    "center = Point(50, 50)\n",
    "radius = 50.0\n",
    "circle = center.buffer(radius)\n",
    "\n",
    "env = Environment.from_polygon(polygon=circle, bin_size=2.5, name=\"CircularArena\")\n",
    "env.units = \"cm\"\n",
    "env.frame = \"open_field\"\n",
    "\n",
    "print(f\"Environment: Circular arena (radius={radius:.0f} cm)\")\n",
    "print(f\"  {env.n_bins} bins, {env.n_dims}D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e0252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating place field remapping...\n",
      "Location A (trials 1-15): bin 323 at [60.0, 65.0] cm\n",
      "Location B (trials 16-30): bin 190 at [40.0, 35.0] cm\n",
      "Generated 30 trial fields (remapping at trial 15)\n"
     ]
    }
   ],
   "source": [
    "# Simulate place field remapping across trials\n",
    "print(\"\\nSimulating place field remapping...\")\n",
    "\n",
    "n_trials = 30\n",
    "remap_trial = 15  # Field remaps halfway through\n",
    "\n",
    "# Location A: Upper-right quadrant (60, 65) cm\n",
    "location_a = np.array([60.0, 65.0])\n",
    "bin_a = env.bin_at(location_a.reshape(1, -1))[0]\n",
    "\n",
    "# Location B: Lower-left quadrant (40, 35) cm\n",
    "location_b = np.array([40.0, 35.0])\n",
    "bin_b = env.bin_at(location_b.reshape(1, -1))[0]\n",
    "\n",
    "print(\n",
    "    f\"Location A (trials 1-{remap_trial}): bin {bin_a} at [{location_a[0]:.1f}, {location_a[1]:.1f}] cm\"\n",
    ")\n",
    "print(\n",
    "    f\"Location B (trials {remap_trial + 1}-{n_trials}): bin {bin_b} at [{location_b[0]:.1f}, {location_b[1]:.1f}] cm\"\n",
    ")\n",
    "\n",
    "fields = []\n",
    "for trial in range(n_trials):\n",
    "    # Determine which location is active\n",
    "    if trial < remap_trial:\n",
    "        # Before remapping: field at location A\n",
    "        active_bin = bin_a\n",
    "        field_strength = 1.0  # Full strength at A\n",
    "    else:\n",
    "        # After remapping: field at location B\n",
    "        active_bin = bin_b\n",
    "        # Gradual emergence at new location\n",
    "        field_strength = min(1.0, (trial - remap_trial + 1) / 5)\n",
    "\n",
    "    # Compute distances from active location\n",
    "    distances = env.distance_to([active_bin])\n",
    "\n",
    "    # Gaussian place field with consistent width\n",
    "    sigma = 8.0  # cm (typical place field size)\n",
    "    field = field_strength * np.exp(-(distances**2) / (2 * sigma**2))\n",
    "\n",
    "    # Add realistic noise\n",
    "    noise = np.random.randn(env.n_bins) * 0.15\n",
    "    field = field + noise\n",
    "    field = np.maximum(field, 0)  # Non-negative firing rates\n",
    "\n",
    "    fields.append(field)\n",
    "\n",
    "print(f\"Generated {len(fields)} trial fields (remapping at trial {remap_trial})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96908914",
   "metadata": {},
   "source": [
    "## Example 1: Interactive Napari Viewer\n",
    "\n",
    "**Best for**: Large datasets, exploration, real-time interaction\n",
    "\n",
    "**Features**:\n",
    "- GPU-accelerated rendering\n",
    "- Instant seeking through frames\n",
    "- Memory-efficient lazy loading\n",
    "- Suitable for 100K+ frames\n",
    "\n",
    "**Installation**: `pip install 'napari[all]>=0.4.18'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2f05dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Napari viewer...\n",
      "  - Use slider to scrub through trials\n",
      "  - Instant seeking through all frames\n",
      "  - GPU accelerated\n",
      "âœ“ Napari viewer launched\n",
      "  (Close the viewer window to continue)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"Launching Napari viewer...\")\n",
    "    print(\"\")\n",
    "    print(\"PLAYBACK CONTROLS (bottom-left):\")\n",
    "    print(\"  â–¶ Play button - Start/stop animation\")\n",
    "    print(\"  â” Time slider - Scrub through frames\")\n",
    "    print(\"\")\n",
    "    print(\"KEYBOARD SHORTCUTS:\")\n",
    "    print(\"  Spacebar - Play/pause (toggle)\")\n",
    "    print(\"  â† â†’ Arrow keys - Step through frames\")\n",
    "    print(\"\")\n",
    "    print(\"SPEED CONTROL (left sidebar):\")\n",
    "    print(\"  ðŸ“Š 'Playback Speed' widget - Large slider (easy to drag)\")\n",
    "    print(\"  Drag to adjust FPS (1-120) - updates instantly\")\n",
    "    print(\"\")\n",
    "\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        title=\"Place Field Remapping\",\n",
    "    )\n",
    "\n",
    "    print(\"âœ“ Napari viewer opened\")\n",
    "\n",
    "    # Only call napari.run() when running as a script (not in Jupyter)\n",
    "    # In Jupyter, the viewer stays open without blocking execution\n",
    "    if get_ipython() is None:\n",
    "        print(\"  (Running as script - window will block until closed)\")\n",
    "        napari.run()\n",
    "    else:\n",
    "        print(\"  (Running in Jupyter - window stays open, execution continues)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"âŠ— Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e40759",
   "metadata": {},
   "source": [
    "## Example 2: Video Export (MP4)\n",
    "\n",
    "**Best for**: Publications, presentations, high-quality renders\n",
    "\n",
    "**Features**:\n",
    "- Parallel rendering for speed\n",
    "- High-quality output\n",
    "- Multiple codec options (h264, h265, vp9, mpeg4)\n",
    "- Dry-run mode for time/size estimation\n",
    "\n",
    "**Installation**: System dependency (ffmpeg)\n",
    "- macOS: `brew install ffmpeg`\n",
    "- Ubuntu: `sudo apt install ffmpeg`\n",
    "- Windows: Download from https://ffmpeg.org/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b7b25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting video with parallel rendering...\n",
      "Rendering 30 frames using 4 workers...\n",
      "Estimated time: ~4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Workers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding video...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ffmpeg encoding failed:\nffmpeg version 8.0 Copyright (c) 2000-2025 the FFmpeg developers\n  built with Apple clang version 15.0.0 (clang-1500.1.0.2.5)\n  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/8.0_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n  libavutil      60.  8.100 / 60.  8.100\n  libavcodec     62. 11.100 / 62. 11.100\n  libavformat    62.  3.100 / 62.  3.100\n  libavdevice    62.  1.100 / 62.  1.100\n  libavfilter    11.  4.100 / 11.  4.100\n  libswscale      9.  1.100 /  9.  1.100\n  libswresample   6.  1.100 /  6.  1.100\nInput #0, image2, from '/var/folders/86/m147b4k17lddvs_xsw0mj2zw0000gn/T/neurospatial_animation_7dkixrjy/frame_%02d.png':\n  Duration: 00:00:06.00, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc, gbr/unknown/unknown), 554x552 [SAR 3937:3937 DAR 277:276], 5 fps, 5 tbr, 5 tbn\n[out#0/mp4 @ 0x136904650] Error opening output examples/16_place_field_remapping.mp4: No such file or directory\nError opening output file examples/16_place_field_remapping.mp4.\nError opening output files: No such file or directory\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_ffmpeg_available():\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExporting video with parallel rendering...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     output_path = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43manimate_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvideo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexamples/16_place_field_remapping.mp4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrial \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Parallel rendering\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Video saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/decorators.py:148\u001b[39m, in \u001b[36mcheck_fitted.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_fitted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EnvironmentNotFittedError(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, method.\u001b[34m__name__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/visualization.py:660\u001b[39m, in \u001b[36mEnvironmentVisualization.animate_fields\u001b[39m\u001b[34m(self, fields, backend, save_path, fps, cmap, vmin, vmax, frame_labels, overlay_trajectory, title, dpi, codec, bitrate, n_workers, dry_run, image_format, max_html_frames, contrast_limits, show_colorbar, colorbar_label)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Animate spatial fields over time with multiple backend options.\u001b[39;00m\n\u001b[32m    476\u001b[39m \n\u001b[32m    477\u001b[39m \u001b[33;03mCreates animations of spatial field data across different time points,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    656\u001b[39m \n\u001b[32m    657\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animate_fields \u001b[38;5;28;01mas\u001b[39;00m _animate\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_animate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/core.py:157\u001b[39m, in \u001b[36manimate_fields\u001b[39m\u001b[34m(env, fields, backend, save_path, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    146\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    147\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVideo backend with parallel rendering requires environment to be pickle-able.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    148\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  3. Use backend=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhtml\u001b[39m\u001b[33m'\u001b[39m\u001b[33m instead (no pickling)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    155\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrender_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mhtml\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhtml_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_html\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/video_backend.py:266\u001b[39m, in \u001b[36mrender_video\u001b[39m\u001b[34m(env, fields, save_path, fps, cmap, vmin, vmax, frame_labels, dpi, codec, bitrate, n_workers, dry_run, title, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m result = subprocess.run(\n\u001b[32m    259\u001b[39m     cmd,\n\u001b[32m    260\u001b[39m     capture_output=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    261\u001b[39m     text=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    262\u001b[39m     check=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mffmpeg encoding failed:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult.stderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    268\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Video saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_path\n",
      "\u001b[31mRuntimeError\u001b[39m: ffmpeg encoding failed:\nffmpeg version 8.0 Copyright (c) 2000-2025 the FFmpeg developers\n  built with Apple clang version 15.0.0 (clang-1500.1.0.2.5)\n  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/8.0_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n  libavutil      60.  8.100 / 60.  8.100\n  libavcodec     62. 11.100 / 62. 11.100\n  libavformat    62.  3.100 / 62.  3.100\n  libavdevice    62.  1.100 / 62.  1.100\n  libavfilter    11.  4.100 / 11.  4.100\n  libswscale      9.  1.100 /  9.  1.100\n  libswresample   6.  1.100 /  6.  1.100\nInput #0, image2, from '/var/folders/86/m147b4k17lddvs_xsw0mj2zw0000gn/T/neurospatial_animation_7dkixrjy/frame_%02d.png':\n  Duration: 00:00:06.00, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc, gbr/unknown/unknown), 554x552 [SAR 3937:3937 DAR 277:276], 5 fps, 5 tbr, 5 tbn\n[out#0/mp4 @ 0x136904650] Error opening output examples/16_place_field_remapping.mp4: No such file or directory\nError opening output file examples/16_place_field_remapping.mp4.\nError opening output files: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "if check_ffmpeg_available():\n",
    "    print(\"Exporting video with parallel rendering...\")\n",
    "\n",
    "    output_path = env.animate_fields(\n",
    "        fields,\n",
    "        backend=\"video\",\n",
    "        save_path=output_dir / \"16_place_field_remapping.mp4\",\n",
    "        fps=5,\n",
    "        cmap=\"hot\",\n",
    "        frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        n_workers=4,  # Parallel rendering\n",
    "        dpi=100,\n",
    "    )\n",
    "    print(f\"âœ“ Video saved to {output_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŠ— ffmpeg not available. Video export skipped.\")\n",
    "    print(\"  Install: brew install ffmpeg (macOS) or apt install ffmpeg (Linux)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9bcf4d",
   "metadata": {},
   "source": [
    "## Example 3: Standalone HTML Player\n",
    "\n",
    "**Best for**: Sharing, remote viewing, no dependencies\n",
    "\n",
    "**Features**:\n",
    "- Single self-contained file\n",
    "- Works offline in any browser\n",
    "- Instant scrubbing with slider\n",
    "- Play/pause controls\n",
    "- Keyboard shortcuts (space, arrows)\n",
    "\n",
    "**Installation**: No dependencies required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01af9e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HTML player...\n",
      "Rendering 30 frames to PNG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 33.05it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'examples/16_place_field_remapping.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating HTML player...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m html_path = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43manimate_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhtml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexamples/16_place_field_remapping.html\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mviridis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrial \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ HTML player saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhtml_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  - Open in any web browser\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/decorators.py:148\u001b[39m, in \u001b[36mcheck_fitted.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_fitted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EnvironmentNotFittedError(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, method.\u001b[34m__name__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/visualization.py:660\u001b[39m, in \u001b[36mEnvironmentVisualization.animate_fields\u001b[39m\u001b[34m(self, fields, backend, save_path, fps, cmap, vmin, vmax, frame_labels, overlay_trajectory, title, dpi, codec, bitrate, n_workers, dry_run, image_format, max_html_frames, contrast_limits, show_colorbar, colorbar_label)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Animate spatial fields over time with multiple backend options.\u001b[39;00m\n\u001b[32m    476\u001b[39m \n\u001b[32m    477\u001b[39m \u001b[33;03mCreates animations of spatial field data across different time points,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    656\u001b[39m \n\u001b[32m    657\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animate_fields \u001b[38;5;28;01mas\u001b[39;00m _animate\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_animate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/core.py:164\u001b[39m, in \u001b[36manimate_fields\u001b[39m\u001b[34m(env, fields, backend, save_path, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m save_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    163\u001b[39m         save_path = \u001b[33m\"\u001b[39m\u001b[33manimation.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrender_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mwidget\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwidget_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_widget\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/html_backend.py:187\u001b[39m, in \u001b[36mrender_html\u001b[39m\u001b[34m(env, fields, save_path, fps, cmap, vmin, vmax, frame_labels, dpi, image_format, max_html_frames, title, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Write to file\u001b[39;00m\n\u001b[32m    186\u001b[39m output_path = Path(save_path)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[43moutput_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m file_size_mb = output_path.stat().st_size / \u001b[32m1e6\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ HTML saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_size_mb\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/pathlib/_local.py:555\u001b[39m, in \u001b[36mPath.write_text\u001b[39m\u001b[34m(self, data, encoding, errors, newline)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# Call io.text_encoding() here to ensure any warning is raised at an\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[38;5;66;03m# appropriate stack level.\u001b[39;00m\n\u001b[32m    554\u001b[39m encoding = io.text_encoding(encoding)\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPathBase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/pathlib/_abc.py:651\u001b[39m, in \u001b[36mPathBase.write_text\u001b[39m\u001b[34m(self, data, encoding, errors, newline)\u001b[39m\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mdata must be str, not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m %\n\u001b[32m    650\u001b[39m                     data.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    652\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f.write(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/pathlib/_local.py:537\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    536\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'examples/16_place_field_remapping.html'"
     ]
    }
   ],
   "source": [
    "print(\"Generating HTML player...\")\n",
    "\n",
    "html_path = env.animate_fields(\n",
    "    fields,\n",
    "    backend=\"html\",\n",
    "    save_path=output_dir / \"16_place_field_remapping.html\",\n",
    "    fps=10,\n",
    "    cmap=\"viridis\",\n",
    "    frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    ")\n",
    "\n",
    "print(f\"âœ“ HTML player saved to {html_path}\")\n",
    "print(\"  - Open in any web browser\")\n",
    "print(\"  - Instant scrubbing with slider\")\n",
    "print(\"  - Shareable (single file)\")\n",
    "print(\"  - Keyboard shortcuts: space = play/pause, arrows = step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e44bf",
   "metadata": {},
   "source": [
    "## Example 4: Jupyter Widget\n",
    "\n",
    "**Best for**: Quick checks in notebooks, interactive exploration\n",
    "\n",
    "**Features**:\n",
    "- Integrated controls in notebook\n",
    "- Play/pause button\n",
    "- Slider for frame selection\n",
    "- Automatic display in output cell\n",
    "\n",
    "**Installation**: `pip install 'ipywidgets>=8.0'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d64d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Jupyter widget...\n",
      "Pre-rendering 30 frames for widget...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30884bef180740a1a1e936d4ab2f9cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame:', max=29), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59d2276c7e8467d9cbd14028c8fe6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Play(value=0, max=29), IntSlider(value=0, description='Frame:', max=29)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Widget created (displayed above)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    if get_ipython() is not None:\n",
    "        print(\"Creating Jupyter widget...\")\n",
    "\n",
    "        widget = env.animate_fields(\n",
    "            fields,\n",
    "            backend=\"widget\",\n",
    "            fps=10,\n",
    "            frame_labels=[f\"Trial {i + 1}\" for i in range(n_trials)],\n",
    "        )\n",
    "\n",
    "        print(\"âœ“ Widget created (displayed above)\")\n",
    "    else:\n",
    "        print(\"âŠ— Not in Jupyter notebook - widget skipped\")\n",
    "except ImportError:\n",
    "    print(\"âŠ— IPython not available - widget skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6174fcc",
   "metadata": {},
   "source": [
    "## Example 5: Large-Scale Session Pattern\n",
    "\n",
    "**Best for**: Hour-long recordings at high sampling rates (e.g., 250 Hz)\n",
    "\n",
    "**Key techniques**:\n",
    "- Memory-mapped arrays (don't load all data into RAM)\n",
    "- Napari for interactive exploration (lazy loading)\n",
    "- Frame subsampling for video export\n",
    "- Dry-run estimation before rendering\n",
    "\n",
    "**This example demonstrates the pattern** for handling large sessions:\n",
    "- Real sessions: 60K-900K frames (4 min - 1 hour at 250 Hz)\n",
    "- Real file sizes: 300 MB - 4.5 GB\n",
    "- Demo version: 1000 frames (~5 MB) to avoid filling your disk\n",
    "\n",
    "The techniques shown here scale to arbitrarily large datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec954eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Example 5: Large-Scale Session (900K frames)\n",
      "================================================================================\n",
      "\n",
      "For hour-long sessions with 900K frames:\n",
      "  - Use memory-mapped data (don't load into RAM)\n",
      "  - Use Napari for exploration (lazy loading)\n",
      "  - Subsample for video export\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Example 5: Large-Scale Session Pattern\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nDemonstrating techniques for large datasets (60K-900K frames):\")\n",
    "print(\"  - Use memory-mapped data (don't load into RAM)\")\n",
    "print(\"  - Use Napari for exploration (lazy loading)\")\n",
    "print(\"  - Subsample for video export\")\n",
    "print(\"\\nNote: Using 1000 frames (~5 MB) for demo; scales to hours of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ad025",
   "metadata": {},
   "source": [
    "### Step 1: Create Memory-Mapped Data File\n",
    "\n",
    "In practice, this would be your neural recording data. We'll simulate it here for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7fcca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating memory-mapped data file...\n",
      "Populating with sample data (in practice, this is your recording)...\n",
      "  (Writing in chunks to avoid memory issues)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'goal_bin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Slowly drifting center\u001b[39;00m\n\u001b[32m     27\u001b[39m drift = (i / n_frames_large) * \u001b[32m20\u001b[39m  \u001b[38;5;66;03m# Drifts 20 bins over session\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m center_bin = \u001b[43mgoal_bin\u001b[49m + \u001b[38;5;28mint\u001b[39m(drift)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m center_bin >= env.n_bins:\n\u001b[32m     30\u001b[39m     center_bin = env.n_bins - \u001b[32m1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'goal_bin' is not defined"
     ]
    }
   ],
   "source": [
    "# Create memory-mapped data file (simulating neural recording)\n",
    "print(\"\\nCreating memory-mapped data file...\")\n",
    "# For demo purposes, use a small file (1000 frames ~5 MB)\n",
    "# In practice, this would be 60K-900K frames for real sessions\n",
    "n_frames_large = 1000  # Demo size (real: 60K-900K frames)\n",
    "\n",
    "# Use temporary directory for demo (in practice, use your data directory)\n",
    "tmpdir = Path(tempfile.mkdtemp(prefix=\"neurospatial_demo_\"))\n",
    "mmap_path = tmpdir / \"large_session.dat\"\n",
    "\n",
    "fields_mmap = np.memmap(\n",
    "    str(mmap_path),\n",
    "    dtype=\"float32\",\n",
    "    mode=\"w+\",  # Create new file\n",
    "    shape=(n_frames_large, env.n_bins),\n",
    ")\n",
    "\n",
    "print(\"Populating with sample data (in practice, this is your recording)...\")\n",
    "print(\"  (Writing in chunks to avoid memory issues)\")\n",
    "\n",
    "# Populate with simulated data (in practice, this is your neural recording)\n",
    "# For this example, we'll simulate a slowly drifting place field\n",
    "initial_bin = env.n_bins // 2  # Start at center of environment\n",
    "\n",
    "chunk_size = 10000\n",
    "for i in range(0, n_frames_large, chunk_size):\n",
    "    # Simulate place field that drifts slowly over time\n",
    "    chunk_end = min(i + chunk_size, n_frames_large)\n",
    "    chunk_len = chunk_end - i\n",
    "\n",
    "    # Slowly drifting center (drifts 20 bins over the full session)\n",
    "    drift = int((i / n_frames_large) * 20)\n",
    "    center_bin = initial_bin + drift\n",
    "    if center_bin >= env.n_bins:\n",
    "        center_bin = env.n_bins - 1\n",
    "\n",
    "    distances = env.distance_to([center_bin])\n",
    "    for j in range(chunk_len):\n",
    "        fields_mmap[i + j] = np.exp(-distances / 15) + np.random.randn(env.n_bins) * 0.1\n",
    "\n",
    "fields_mmap.flush()\n",
    "\n",
    "print(f\"\\nâœ“ Created memory-mapped dataset: {n_frames_large:,} frames\")\n",
    "print(f\"  File size: {n_frames_large * env.n_bins * 4 / 1e9:.2f} GB\")\n",
    "print(\"  RAM usage: ~0 MB (memory-mapped, not loaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e394c",
   "metadata": {},
   "source": [
    "### Step 2: Interactive Exploration with Napari\n",
    "\n",
    "Napari loads frames on-demand, making it efficient for exploring large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac16b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Option 1: Interactive exploration (Napari)\n",
      "  Napari loads frames on-demand - handles 900K frames efficiently\n",
      "âœ“ Napari viewer launched - scrub through 900K frames instantly!\n",
      "  (Close the viewer window to continue)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOption 1: Interactive exploration (Napari)\")\n",
    "print(\"  Napari loads frames on-demand - would handle 900K frames efficiently\")\n",
    "\n",
    "try:\n",
    "    # Import napari only if attempting to use it\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"PLAYBACK CONTROLS:\")\n",
    "    print(\"  Bottom-left: â–¶ Play button, time slider\")\n",
    "    print(\"  Keyboard: Spacebar (play/pause), â† â†’ (step frames)\")\n",
    "    print(\"  Left sidebar: ðŸ“Š 'Playback Speed' widget (large slider, 1-120 FPS)\")\n",
    "\n",
    "    viewer = env.animate_fields(\n",
    "        fields_mmap,\n",
    "        backend=\"napari\",\n",
    "        fps=250,  # Match recording rate\n",
    "        title=\"Large Session Demo (1000 frames)\",\n",
    "    )\n",
    "    print(\"âœ“ Napari viewer opened!\")\n",
    "    print(\"  (Same technique works for 60K-900K frame sessions)\")\n",
    "\n",
    "    # Only call napari.run() when running as a script (not in Jupyter)\n",
    "    if get_ipython() is None:\n",
    "        print(\"  (Running as script - window will block until closed)\")\n",
    "        napari.run()\n",
    "    else:\n",
    "        print(\"  (Running in Jupyter - window stays open, execution continues)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"âŠ— Napari not available (install: pip install 'napari[all]>=0.4.18')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391205c8",
   "metadata": {},
   "source": [
    "### Step 3: Export Subsampled Video\n",
    "\n",
    "For video export, we need to subsample the high-frequency data to a manageable frame rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0376e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Option 2: Export subsampled video\n",
      "  900K frames â†’ 30 fps video requires subsampling\n",
      "  Subsampled: 108,000 frames (every 8th frame)\n",
      "  Video duration: 3600.0 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOption 2: Export subsampled video\")\n",
    "print(\"  For large sessions: 250 Hz â†’ 30 fps requires subsampling\")\n",
    "\n",
    "# Subsample 250 Hz â†’ 30 fps\n",
    "# For 900K frames, this would produce 108K subsampled frames (1 hour video)\n",
    "# For our 1000 frame demo, this produces ~120 frames\n",
    "fields_subsampled = subsample_frames(fields_mmap, target_fps=30, source_fps=250)\n",
    "print(f\"  Subsampled: {len(fields_subsampled):,} frames (every {250 // 30}th frame)\")\n",
    "print(f\"  Video duration: {len(fields_subsampled) / 30:.1f} seconds\")\n",
    "print(\"  (For 900K frames, would produce ~1 hour video)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752671b",
   "metadata": {},
   "source": [
    "### Step 4: Dry Run to Estimate Render Time\n",
    "\n",
    "Before committing to a long render, use dry-run mode to estimate time and file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53594682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dry run estimation:\n",
      "Running dry run estimation...\n",
      "\n",
      "============================================================\n",
      "Video Export Dry Run Estimate:\n",
      "============================================================\n",
      "  Frames:          108,000\n",
      "  Workers:         8\n",
      "  Frame time:      22.1 ms\n",
      "  Est. total time: 5.0 minutes\n",
      "  Est. file size:  5273 MB\n",
      "  Output path:     examples/16_large_session_summary.mp4\n",
      "\n",
      "To proceed, call again with dry_run=False\n",
      "============================================================\n",
      "\n",
      "\n",
      "  To render, run with dry_run=False\n"
     ]
    }
   ],
   "source": [
    "if check_ffmpeg_available():\n",
    "    print(\"\\nDry run estimation:\")\n",
    "    env.animate_fields(\n",
    "        fields_subsampled,\n",
    "        backend=\"video\",\n",
    "        save_path=output_dir / \"16_large_session_summary.mp4\",\n",
    "        fps=30,\n",
    "        n_workers=8,\n",
    "        dry_run=True,  # Estimate first\n",
    "    )\n",
    "    print(\"\\n  To render, run with dry_run=False\")\n",
    "else:\n",
    "    print(\"  âŠ— ffmpeg not available for video export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbe208",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41985045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up temporary files...\n",
      "âœ“ Temporary files removed\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary files\n",
    "print(\"\\nCleaning up temporary files...\")\n",
    "if mmap_path.exists():\n",
    "    mmap_path.unlink()\n",
    "    tmpdir.rmdir()\n",
    "    print(\"âœ“ Temporary files removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac427517",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Backend Selection Guide\n",
    "\n",
    "| Use Case | Backend | Installation | Best For |\n",
    "|----------|---------|--------------|----------|\n",
    "| **Exploration** | Napari | `pip install napari[all]` | Large datasets (100K+ frames), interactive |\n",
    "| **Publication** | Video | `brew install ffmpeg` | High-quality renders, parallel speed |\n",
    "| **Sharing** | HTML | No dependencies | Remote viewing, single file |\n",
    "| **Quick check** | Widget | `pip install ipywidgets` | Notebook integration |\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Large datasets (>10K frames)**: Use Napari for exploration, subsample for video\n",
    "- **Memory constraints**: Use memory-mapped arrays (`np.memmap`)\n",
    "- **Parallel rendering**: Increase `n_workers` for faster video export\n",
    "- **File size**: Use `image_format='jpeg'` for HTML to reduce size\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Auto backend selection\n",
    "env.animate_fields(fields, backend='auto')\n",
    "\n",
    "# Quick Napari check\n",
    "env.animate_fields(fields, backend='napari')\n",
    "\n",
    "# Publication video\n",
    "env.animate_fields(fields, save_path='video.mp4', fps=5, n_workers=8)\n",
    "\n",
    "# Shareable HTML\n",
    "env.animate_fields(fields, save_path='animation.html')\n",
    "\n",
    "# Subsample high-frequency data\n",
    "from neurospatial.animation import subsample_frames\n",
    "fields_30fps = subsample_frames(fields_250hz, target_fps=30, source_fps=250)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fdca2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try animating your own neural data\n",
    "- Experiment with different colormaps (`cmap` parameter)\n",
    "- Add trajectory overlays (`overlay_trajectory` parameter)\n",
    "- Compare place field evolution across sessions\n",
    "- Visualize replay events or value function learning\n",
    "\n",
    "For more details, see the neurospatial documentation on animation backends."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
