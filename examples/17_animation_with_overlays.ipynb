{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Animation Overlays\n",
    "\n",
    "This notebook demonstrates the new overlay system for visualizing animal behavior alongside spatial fields:\n",
    "\n",
    "1. **Position Overlays** - Trajectories with decaying trails\n",
    "2. **Bodypart Overlays** - Pose tracking with skeleton rendering\n",
    "3. **Head Direction Overlays** - Orientation arrows\n",
    "4. **Multi-Animal Support** - Track multiple animals simultaneously\n",
    "5. **Regions** - Highlight spatial regions of interest\n",
    "6. **Temporal Alignment** - Sync overlays at different sampling rates\n",
    "7. **Backend Comparison** - Same data across all backends\n",
    "\n",
    "**Estimated time**: 20-25 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Overlay trajectories on animated spatial fields\n",
    "- Visualize pose tracking data with skeletons\n",
    "- Display head direction as dynamic arrows\n",
    "- Track multiple animals in the same animation\n",
    "- Highlight regions of interest with transparency\n",
    "- Align overlays at different sampling rates using `frame_times`\n",
    "- Choose the right backend for overlay visualization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Optional dependencies** (install as needed):\n",
    "\n",
    "```bash\n",
    "# For Napari backend (recommended for overlays)\n",
    "pip install 'napari[all]>=0.4.18'\n",
    "\n",
    "# For video export\n",
    "# macOS: brew install ffmpeg\n",
    "# Ubuntu: sudo apt install ffmpeg\n",
    "```\n",
    "\n",
    "**Note**: HTML backend supports position and region overlays only (no pose or head direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /Users/edeno/Documents/GitHub/neurospatial/examples\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from neurospatial import (\n",
    "    BodypartOverlay,\n",
    "    Environment,\n",
    "    HeadDirectionOverlay,\n",
    "    PositionOverlay,\n",
    ")\n",
    "from neurospatial.animation.backends.video_backend import check_ffmpeg_available\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path.cwd()\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup: Create Environment and Simulate Data\n",
    "\n",
    "We'll create a circular arena and simulate:\n",
    "- A place field that tracks with the animal\n",
    "- Animal trajectory exploring the arena\n",
    "- Head direction as the animal moves\n",
    "- Pose data (nose, body center, tail base) for skeleton visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup_env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating circular arena environment...\n",
      "Environment: 1264 bins, 2D\n",
      "Regions: ['reward']\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating circular arena environment...\")\n",
    "\n",
    "# Circular arena (50 cm radius)\n",
    "center = Point(50, 50)\n",
    "radius = 50.0\n",
    "circle = center.buffer(radius)\n",
    "\n",
    "env = Environment.from_polygon(polygon=circle, bin_size=2.5, name=\"CircularArena\")\n",
    "env.units = \"cm\"\n",
    "env.frame = \"open_field\"\n",
    "\n",
    "# Add region of interest (reward zone in upper-right quadrant)\n",
    "reward_zone = Point(65, 65)\n",
    "env.regions.add(\"reward\", point=reward_zone)\n",
    "\n",
    "print(f\"Environment: {env.n_bins} bins, {env.n_dims}D\")\n",
    "print(f\"Regions: {list(env.regions.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simulate_trajectory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating animal trajectory...\n",
      "Trajectory: 50 frames\n",
      "  Position range: [13.9, 89.4] cm\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSimulating animal trajectory...\")\n",
    "\n",
    "n_frames = 50  # 50 time points\n",
    "t = np.linspace(0, 4 * np.pi, n_frames)  # 2 revolutions\n",
    "\n",
    "# Spiral trajectory from center outward\n",
    "r = np.linspace(5, 40, n_frames)  # Radius increases\n",
    "theta = t + np.random.randn(n_frames) * 0.1  # Angle with noise\n",
    "\n",
    "# Convert to Cartesian (center at 50, 50)\n",
    "trajectory = np.column_stack([50 + r * np.cos(theta), 50 + r * np.sin(theta)])\n",
    "\n",
    "# Head direction (tangent to spiral)\n",
    "head_angles = theta + np.pi / 2  # Perpendicular to radius\n",
    "\n",
    "print(f\"Trajectory: {n_frames} frames\")\n",
    "print(f\"  Position range: [{trajectory.min():.1f}, {trajectory.max():.1f}] cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "simulate_pose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating pose data (nose, body, tail)...\n",
      "Pose: 3 keypoints (nose, body, tail)\n",
      "Skeleton: 2 edges\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSimulating pose data (nose, body, tail)...\")\n",
    "\n",
    "# Pose: 3 keypoints with skeleton\n",
    "body_length = 10.0  # cm\n",
    "\n",
    "# Nose: ahead of body center\n",
    "nose_offset = body_length * 0.5\n",
    "nose_x = trajectory[:, 0] + nose_offset * np.cos(head_angles)\n",
    "nose_y = trajectory[:, 1] + nose_offset * np.sin(head_angles)\n",
    "\n",
    "# Body center: trajectory position\n",
    "body_x = trajectory[:, 0]\n",
    "body_y = trajectory[:, 1]\n",
    "\n",
    "# Tail: behind body center\n",
    "tail_offset = body_length * 0.5\n",
    "tail_x = trajectory[:, 0] - tail_offset * np.cos(head_angles)\n",
    "tail_y = trajectory[:, 1] - tail_offset * np.sin(head_angles)\n",
    "\n",
    "# Pose dictionary\n",
    "pose_data = {\n",
    "    \"nose\": np.column_stack([nose_x, nose_y]),\n",
    "    \"body\": np.column_stack([body_x, body_y]),\n",
    "    \"tail\": np.column_stack([tail_x, tail_y]),\n",
    "}\n",
    "\n",
    "# Skeleton: connections between keypoints\n",
    "skeleton = [(\"tail\", \"body\"), (\"body\", \"nose\")]\n",
    "\n",
    "print(\"Pose: 3 keypoints (nose, body, tail)\")\n",
    "print(f\"Skeleton: {len(skeleton)} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "simulate_fields",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating place field that tracks with animal...\n",
      "Fields: (50, 1264) (frames x bins)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSimulating place field that tracks with animal...\")\n",
    "\n",
    "# Place field centered on animal position at each frame\n",
    "fields = []\n",
    "for i in range(n_frames):\n",
    "    # Find bin closest to animal position\n",
    "    pos = trajectory[i : i + 1]  # Shape (1, 2)\n",
    "    center_bin = env.bin_at(pos)[0]\n",
    "\n",
    "    # Gaussian field around animal\n",
    "    distances = env.distance_to([center_bin])\n",
    "    sigma = 12.0  # cm\n",
    "    field = np.exp(-(distances**2) / (2 * sigma**2))\n",
    "\n",
    "    # Add noise\n",
    "    field = field + np.random.randn(env.n_bins) * 0.1\n",
    "    field = np.maximum(field, 0)\n",
    "\n",
    "    fields.append(field)\n",
    "\n",
    "fields = np.array(fields)\n",
    "print(f\"Fields: {fields.shape} (frames x bins)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1",
   "metadata": {},
   "source": [
    "## Example 1: Position Overlay with Trail\n",
    "\n",
    "Overlay the animal's trajectory on the animated field with a decaying trail showing recent positions.\n",
    "\n",
    "**Key features**:\n",
    "- `trail_length=10` shows last 10 frames\n",
    "- Trail fades from current (opaque) to past (transparent)\n",
    "- Current position rendered as a marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ex1_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Position Overlay with Trail\n",
      "  Trajectory: 50 frames\n",
      "  Trail length: 10 frames (decaying alpha)\n",
      "  Color: red, Size: 12.0\n",
      "\n",
      "Launching Napari viewer...\n",
      "✓ Napari viewer opened\n",
      "  Watch the red trail follow the animal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:599: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/layers/tracks/tracks.py:652: UserWarning: Previous color_by key 'color' not present in features. Falling back to track_id\n",
      "  warn(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:158: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create position overlay with trail\n",
    "position_overlay = PositionOverlay(\n",
    "    data=trajectory,\n",
    "    color=\"red\",\n",
    "    size=12.0,\n",
    "    trail_length=10,  # Show last 10 frames as trail\n",
    ")\n",
    "\n",
    "print(\"Example 1: Position Overlay with Trail\")\n",
    "print(f\"  Trajectory: {trajectory.shape[0]} frames\")\n",
    "print(\"  Trail length: 10 frames (decaying alpha)\")\n",
    "print(\"  Color: red, Size: 12.0\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay],\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Position Overlay with Trail\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch the red trail follow the animal\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2",
   "metadata": {},
   "source": [
    "## Example 2: Pose Tracking with Skeleton\n",
    "\n",
    "Overlay full pose data (nose, body, tail) with skeleton connecting the keypoints.\n",
    "\n",
    "**Key features**:\n",
    "- `data` is a dict mapping bodypart names to trajectories\n",
    "- `skeleton` defines edges between bodyparts\n",
    "- `colors` can customize per-bodypart colors\n",
    "- Skeleton rendered with specified color and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ex2_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2: Pose Tracking with Skeleton\n",
      "  Bodyparts: ['nose', 'body', 'tail']\n",
      "  Skeleton edges: [('tail', 'body'), ('body', 'nose')]\n",
      "  Colors: nose=yellow, body=red, tail=blue\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:599: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:236: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Napari viewer opened\n",
      "  Watch the skeleton follow the animal pose\n"
     ]
    }
   ],
   "source": [
    "# Create bodypart overlay with skeleton\n",
    "bodypart_overlay = BodypartOverlay(\n",
    "    data=pose_data,\n",
    "    skeleton=skeleton,\n",
    "    colors={\"nose\": \"yellow\", \"body\": \"red\", \"tail\": \"blue\"},\n",
    "    skeleton_color=\"white\",\n",
    "    skeleton_width=2.0,\n",
    ")\n",
    "\n",
    "print(\"Example 2: Pose Tracking with Skeleton\")\n",
    "print(f\"  Bodyparts: {list(pose_data.keys())}\")\n",
    "print(f\"  Skeleton edges: {skeleton}\")\n",
    "print(\"  Colors: nose=yellow, body=red, tail=blue\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[bodypart_overlay],\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Pose Tracking with Skeleton\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch the skeleton follow the animal pose\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3",
   "metadata": {},
   "source": [
    "## Example 3: Head Direction Visualization\n",
    "\n",
    "Overlay head direction as dynamic arrows pointing in the direction of travel.\n",
    "\n",
    "**Key features**:\n",
    "- `data` can be angles (radians) or unit vectors\n",
    "- Arrows rendered with specified color and length\n",
    "- Arrow origin is at the animal's position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ex3_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3: Head Direction Visualization\n",
      "  Head angles: 50 frames\n",
      "  Arrow color: yellow, Length: 15.0 cm\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:599: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/layers/tracks/tracks.py:652: UserWarning: Previous color_by key 'color' not present in features. Falling back to track_id\n",
      "  warn(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:158: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Napari viewer opened\n",
      "  Watch the yellow arrow show heading direction\n"
     ]
    }
   ],
   "source": [
    "# Create head direction overlay (angles in radians)\n",
    "head_direction_overlay = HeadDirectionOverlay(\n",
    "    data=head_angles,\n",
    "    color=\"yellow\",\n",
    "    length=15.0,  # Arrow length in cm\n",
    ")\n",
    "\n",
    "print(\"Example 3: Head Direction Visualization\")\n",
    "print(f\"  Head angles: {head_angles.shape[0]} frames\")\n",
    "print(\"  Arrow color: yellow, Length: 15.0 cm\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "\n",
    "    # Combine position + head direction overlays\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay, head_direction_overlay],\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Position + Head Direction\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch the yellow arrow show heading direction\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4",
   "metadata": {},
   "source": [
    "## Example 4: Multi-Animal Tracking\n",
    "\n",
    "Track multiple animals simultaneously by providing multiple overlay instances.\n",
    "\n",
    "**Key features**:\n",
    "- Pass a list of overlays for each animal\n",
    "- Each overlay automatically gets a suffix (e.g., \"Position_1\", \"Position_2\")\n",
    "- All animals rendered in the same animation with different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ex4_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4: Multi-Animal Tracking\n",
      "\n",
      "Simulating second animal...\n",
      "  Animal 1: red\n",
      "  Animal 2: blue (offset trajectory)\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:599: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/layers/tracks/tracks.py:652: UserWarning: Previous color_by key 'color' not present in features. Falling back to track_id\n",
      "  warn(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:158: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/layers/tracks/tracks.py:652: UserWarning: Previous color_by key 'color' not present in features. Falling back to track_id\n",
      "  warn(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:158: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Napari viewer opened\n",
      "  Watch both animals explore simultaneously\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 4: Multi-Animal Tracking\")\n",
    "print(\"\\nSimulating second animal...\")\n",
    "\n",
    "# Second animal with offset trajectory\n",
    "trajectory_2 = trajectory + np.array([10, -10])  # Offset spatially\n",
    "trajectory_2 = np.clip(trajectory_2, 5, 95)  # Keep in bounds\n",
    "\n",
    "# Create overlays for both animals\n",
    "animal1_overlay = PositionOverlay(\n",
    "    data=trajectory, color=\"red\", size=12.0, trail_length=8\n",
    ")\n",
    "\n",
    "animal2_overlay = PositionOverlay(\n",
    "    data=trajectory_2, color=\"blue\", size=12.0, trail_length=8\n",
    ")\n",
    "\n",
    "print(\"  Animal 1: red\")\n",
    "print(\"  Animal 2: blue (offset trajectory)\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[animal1_overlay, animal2_overlay],  # Multiple overlays\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Multi-Animal Tracking\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch both animals explore simultaneously\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex5",
   "metadata": {},
   "source": [
    "## Example 5: Regions Overlay with Spatial Fields\n",
    "\n",
    "Highlight spatial regions of interest (e.g., reward zones) alongside overlays.\n",
    "\n",
    "**Key features**:\n",
    "- `show_regions=True` displays all defined regions\n",
    "- `show_regions=[\"reward\"]` displays specific regions only\n",
    "- `region_alpha=0.3` controls transparency\n",
    "- Regions rendered as colored polygons/points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ex5_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 5: Regions Overlay\n",
      "  Showing region: ['reward']\n",
      "  Region alpha: 0.3 (30% transparent)\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:599: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/layers/tracks/tracks.py:652: UserWarning: Previous color_by key 'color' not present in features. Falling back to track_id\n",
      "  warn(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:158: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Napari viewer opened\n",
      "  Watch the animal approach the reward region\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 5: Regions Overlay\")\n",
    "print(f\"  Showing region: {list(env.regions.keys())}\")\n",
    "print(\"  Region alpha: 0.3 (30% transparent)\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay],\n",
    "        show_regions=True,  # Show all regions\n",
    "        region_alpha=0.3,  # 30% transparent\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Position + Reward Region\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch the animal approach the reward region\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex6",
   "metadata": {},
   "source": [
    "## Example 6: Mixed-Rate Temporal Alignment\n",
    "\n",
    "Align overlays sampled at different rates using temporal timestamps.\n",
    "\n",
    "**Key features**:\n",
    "- Overlay `times` parameter specifies timestamps for each frame\n",
    "- `frame_times` parameter specifies field frame timestamps\n",
    "- Linear interpolation automatically aligns overlay to field frames\n",
    "- Works even when overlay and fields have different sampling rates\n",
    "\n",
    "**Example**: Position tracked at 120 Hz, fields computed at 10 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ex6_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 6: Mixed-Rate Temporal Alignment\n",
      "\n",
      "Simulating high-frequency position tracking...\n",
      "  Position tracking: 600 samples at 120 Hz\n",
      "  Field computation: 50 frames at 10 Hz\n",
      "\n",
      "✓ Overlay will be interpolated to match field frame times\n",
      "  (Linear interpolation: 120 Hz → 10 Hz)\n",
      "\n",
      "Launching Napari viewer...\n",
      "✓ Napari viewer opened\n",
      "  Position automatically aligned to field frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:599: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/layers/tracks/tracks.py:652: UserWarning: Previous color_by key 'color' not present in features. Falling back to track_id\n",
      "  warn(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:158: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 6: Mixed-Rate Temporal Alignment\")\n",
    "print(\"\\nSimulating high-frequency position tracking...\")\n",
    "\n",
    "# High-frequency position tracking (120 Hz)\n",
    "duration = 5.0  # seconds\n",
    "fps_high = 120  # Hz\n",
    "n_samples_high = int(duration * fps_high)  # 600 samples\n",
    "\n",
    "# Generate high-frequency trajectory\n",
    "t_high = np.linspace(0, duration, n_samples_high)\n",
    "theta_high = t_high * 2 * np.pi + np.random.randn(n_samples_high) * 0.05\n",
    "r_high = 20 + 15 * np.sin(t_high * 3)\n",
    "\n",
    "trajectory_high_freq = np.column_stack(\n",
    "    [50 + r_high * np.cos(theta_high), 50 + r_high * np.sin(theta_high)]\n",
    ")\n",
    "timestamps_high = t_high\n",
    "\n",
    "print(f\"  Position tracking: {n_samples_high} samples at {fps_high} Hz\")\n",
    "\n",
    "# Low-frequency fields (10 Hz)\n",
    "fps_low = 10  # Hz\n",
    "n_frames_low = int(duration * fps_low)  # 50 frames\n",
    "frame_times = np.linspace(0, duration, n_frames_low)\n",
    "\n",
    "print(f\"  Field computation: {n_frames_low} frames at {fps_low} Hz\")\n",
    "\n",
    "# Compute fields at low frequency\n",
    "fields_low_freq = []\n",
    "for t in frame_times:\n",
    "    # Find closest high-freq position\n",
    "    idx = np.argmin(np.abs(timestamps_high - t))\n",
    "    pos = trajectory_high_freq[idx : idx + 1]\n",
    "    center_bin = env.bin_at(pos)[0]\n",
    "\n",
    "    distances = env.distance_to([center_bin])\n",
    "    field = np.exp(-(distances**2) / (2 * 12.0**2))\n",
    "    field = field + np.random.randn(env.n_bins) * 0.1\n",
    "    field = np.maximum(field, 0)\n",
    "    fields_low_freq.append(field)\n",
    "\n",
    "fields_low_freq = np.array(fields_low_freq)\n",
    "\n",
    "# Create overlay with timestamps\n",
    "position_overlay_timed = PositionOverlay(\n",
    "    data=trajectory_high_freq,\n",
    "    times=timestamps_high,  # 120 Hz timestamps\n",
    "    color=\"red\",\n",
    "    size=10.0,\n",
    "    trail_length=15,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Overlay will be interpolated to match field frame times\")\n",
    "print(\"  (Linear interpolation: 120 Hz → 10 Hz)\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields_low_freq,\n",
    "        overlays=[position_overlay_timed],\n",
    "        frame_times=frame_times,  # Explicit field timestamps\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Mixed-Rate Alignment (120 Hz → 10 Hz)\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Position automatically aligned to field frames\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex7",
   "metadata": {},
   "source": [
    "## Example 7: Backend Comparison\n",
    "\n",
    "Compare overlay rendering across all backends with the same data.\n",
    "\n",
    "**Backend capabilities**:\n",
    "\n",
    "| Backend | Position | Bodypart | HeadDirection | Regions |\n",
    "|---------|----------|----------|---------------|--------|\n",
    "| Napari  | ✓ | ✓ | ✓ | ✓ |\n",
    "| Video   | ✓ | ✓ | ✓ | ✓ |\n",
    "| HTML    | ✓ | ✗ | ✗ | ✓ |\n",
    "| Widget  | ✓ | ✓ | ✓ | ✓ |\n",
    "\n",
    "**Note**: HTML backend warns when given unsupported overlay types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ex7_napari",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7a: Napari Backend (Full Support)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:599: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/layers/tracks/tracks.py:652: UserWarning: Previous color_by key 'color' not present in features. Falling back to track_id\n",
      "  warn(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:158: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:236: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Napari: Position + Pose + Head Direction + Regions\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 7a: Napari Backend (Full Support)\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    # All overlay types supported\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay, bodypart_overlay, head_direction_overlay],\n",
    "        show_regions=True,\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Napari: All Overlays\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari: Position + Pose + Head Direction + Regions\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ex7_video",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7b: Video Backend (Full Support)\n",
      "Rendering 50 frames using 4 workers...\n",
      "Estimated time: ~6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Workers: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding video...\n",
      "✓ Video saved to /Users/edeno/Documents/GitHub/neurospatial/examples/17_all_overlays.mp4\n",
      "✓ Video: Saved to /Users/edeno/Documents/GitHub/neurospatial/examples/17_all_overlays.mp4\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 7b: Video Backend (Full Support)\")\n",
    "\n",
    "if check_ffmpeg_available():\n",
    "    # All overlay types supported\n",
    "    output_path = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay, bodypart_overlay, head_direction_overlay],\n",
    "        show_regions=True,\n",
    "        backend=\"video\",\n",
    "        save_path=output_dir / \"17_all_overlays.mp4\",\n",
    "        fps=10,\n",
    "        n_workers=4,\n",
    "    )\n",
    "    print(f\"✓ Video: Saved to {output_path}\")\n",
    "else:\n",
    "    print(\"⊗ ffmpeg not available for video export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex7_html",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7c: HTML Backend (Position + Regions Only)\n",
      "  WARNING: HTML backend does NOT support bodypart or head direction overlays\n",
      "  (Warnings will be emitted if provided)\n",
      "\n",
      "Rendering 50 frames to PNG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding frames: 100%|██████████| 50/50 [00:01<00:00, 49.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HTML saved to /Users/edeno/Documents/GitHub/neurospatial/examples/17_position_only.html (1.3 MB)\n",
      "✓ HTML: Saved to /Users/edeno/Documents/GitHub/neurospatial/examples/17_position_only.html\n",
      "  (Position + Regions rendered; pose/head direction not supported)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 21:50:29.157 python[12232:484170584] IMKClient Stall detected, *please Report* your user scenario attaching a spindump (or sysdiagnose) that captures the problem - (imkxpc_bundleIdentifierWithReply:) block performed very slowly (1.02 secs).\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 7c: HTML Backend (Position + Regions Only)\")\n",
    "print(\"  WARNING: HTML backend does NOT support bodypart or head direction overlays\")\n",
    "print(\"  (Warnings will be emitted if provided)\\n\")\n",
    "\n",
    "# HTML: Only position and regions supported\n",
    "html_path = env.animate_fields(\n",
    "    fields,\n",
    "    overlays=[position_overlay],  # Only position overlay\n",
    "    show_regions=True,\n",
    "    backend=\"html\",\n",
    "    save_path=output_dir / \"17_position_only.html\",\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "print(f\"✓ HTML: Saved to {html_path}\")\n",
    "print(\"  (Position + Regions rendered; pose/head direction not supported)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex7_widget",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7d: Widget Backend (Full Support)\n",
      "Pre-rendering 50 frames for widget...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8093ce619055477f9ece8c3ae9f8e381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Play(value=0, max=49), IntSlider(value=0, description='Frame:', max=49))), HTML(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Widget: Position + Pose + Head Direction + Regions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:599: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 7d: Widget Backend (Full Support)\")\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    if get_ipython() is not None:\n",
    "        # All overlay types supported\n",
    "        widget = env.animate_fields(\n",
    "            fields,\n",
    "            overlays=[position_overlay, bodypart_overlay, head_direction_overlay],\n",
    "            show_regions=True,\n",
    "            backend=\"widget\",\n",
    "            fps=10,\n",
    "        )\n",
    "        print(\"✓ Widget: Position + Pose + Head Direction + Regions\")\n",
    "    else:\n",
    "        print(\"⊗ Not in Jupyter notebook\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ IPython/ipywidgets not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Overlay Types\n",
    "\n",
    "1. **PositionOverlay**: Trajectories with decaying trails\n",
    "   - `data`: (n_frames, n_dims) array\n",
    "   - `trail_length`: Number of past frames to show\n",
    "   - `color`, `size`: Marker appearance\n",
    "\n",
    "2. **BodypartOverlay**: Pose tracking with skeletons\n",
    "   - `data`: Dict mapping bodypart names to (n_frames, n_dims) arrays\n",
    "   - `skeleton`: List of (bodypart1, bodypart2) edge tuples\n",
    "   - `colors`: Per-bodypart colors\n",
    "   - `skeleton_color`, `skeleton_width`: Skeleton appearance\n",
    "\n",
    "3. **HeadDirectionOverlay**: Orientation arrows\n",
    "   - `data`: (n_frames,) angles in radians OR (n_frames, n_dims) unit vectors\n",
    "   - `color`, `length`: Arrow appearance\n",
    "\n",
    "### Temporal Alignment\n",
    "\n",
    "- Add `times` parameter to overlay for timestamps\n",
    "- Add `frame_times` parameter to `animate_fields()` for field timestamps\n",
    "- Linear interpolation automatically aligns overlay to field frames\n",
    "- Works even when overlay and fields have different sampling rates\n",
    "\n",
    "### Backend Capabilities\n",
    "\n",
    "- **Napari**: Full support (all overlay types + regions)\n",
    "- **Video**: Full support (all overlay types + regions)\n",
    "- **HTML**: Partial support (position + regions only, warns for others)\n",
    "- **Widget**: Full support (all overlay types + regions)\n",
    "\n",
    "### Multi-Animal Support\n",
    "\n",
    "- Pass multiple overlay instances in a list\n",
    "- Each overlay automatically gets a suffix (e.g., \"Position_1\", \"Position_2\")\n",
    "- Use different colors to distinguish animals\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Simple trajectory overlay\n",
    "from neurospatial import PositionOverlay\n",
    "overlay = PositionOverlay(data=trajectory, color=\"red\", trail_length=10)\n",
    "env.animate_fields(fields, overlays=[overlay], backend=\"napari\")\n",
    "\n",
    "# Pose with skeleton\n",
    "from neurospatial import BodypartOverlay\n",
    "overlay = BodypartOverlay(\n",
    "    data={\"nose\": nose_traj, \"body\": body_traj, \"tail\": tail_traj},\n",
    "    skeleton=[(\"tail\", \"body\"), (\"body\", \"nose\")],\n",
    "    skeleton_color=\"white\"\n",
    ")\n",
    "env.animate_fields(fields, overlays=[overlay], backend=\"napari\")\n",
    "\n",
    "# Mixed-rate alignment\n",
    "overlay = PositionOverlay(data=trajectory_120hz, times=times_120hz)\n",
    "env.animate_fields(\n",
    "    fields_10hz,\n",
    "    overlays=[overlay],\n",
    "    frame_times=times_10hz,  # Automatic interpolation\n",
    "    backend=\"napari\"\n",
    ")\n",
    "\n",
    "# Multi-animal\n",
    "env.animate_fields(\n",
    "    fields,\n",
    "    overlays=[overlay_animal1, overlay_animal2],\n",
    "    backend=\"napari\"\n",
    ")\n",
    "\n",
    "# Show regions\n",
    "env.animate_fields(\n",
    "    fields,\n",
    "    overlays=[overlay],\n",
    "    show_regions=True,\n",
    "    region_alpha=0.3,\n",
    "    backend=\"napari\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Video export**: Use `n_workers > 1` for parallel rendering\n",
    "- **Large datasets**: Use Napari for exploration, subsample for video\n",
    "- **HTML file size**: Limit frames (default max 500) or use video backend\n",
    "- **Parallel rendering**: Call `env.clear_cache()` before video export with `n_workers > 1`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Apply overlays to your own behavioral tracking data\n",
    "- Combine multiple overlay types for rich visualizations\n",
    "- Export publication-quality videos with overlays\n",
    "- Use temporal alignment for multi-modal data (tracking + neural recordings)\n",
    "\n",
    "For more details, see:\n",
    "- `docs/animation_overlays.md` - Complete overlay documentation\n",
    "- `examples/16_field_animation.ipynb` - Animation backends without overlays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
