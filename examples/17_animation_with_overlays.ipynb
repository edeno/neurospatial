{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Animation Overlays\n",
    "\n",
    "This notebook demonstrates the new overlay system for visualizing animal behavior alongside spatial fields:\n",
    "\n",
    "1. **Position Overlays** - Trajectories with decaying trails\n",
    "2. **Bodypart Overlays** - Pose tracking with skeleton rendering\n",
    "3. **Head Direction Overlays** - Orientation arrows\n",
    "4. **Multi-Animal Support** - Track multiple animals simultaneously\n",
    "5. **Regions** - Highlight spatial regions of interest\n",
    "6. **Temporal Alignment** - Sync overlays at different sampling rates\n",
    "7. **Backend Comparison** - Same data across all backends\n",
    "\n",
    "**Estimated time**: 20-25 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Overlay trajectories on animated spatial fields\n",
    "- Visualize pose tracking data with skeletons\n",
    "- Display head direction as dynamic arrows\n",
    "- Track multiple animals in the same animation\n",
    "- Highlight regions of interest with transparency\n",
    "- Align overlays at different sampling rates using `frame_times`\n",
    "- Choose the right backend for overlay visualization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Optional dependencies** (install as needed):\n",
    "\n",
    "```bash\n",
    "# For Napari backend (recommended for overlays)\n",
    "pip install 'napari[all]>=0.4.18'\n",
    "\n",
    "# For video export\n",
    "# macOS: brew install ffmpeg\n",
    "# Ubuntu: sudo apt install ffmpeg\n",
    "```\n",
    "\n",
    "**Note**: HTML backend supports position and region overlays only (no pose or head direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /Users/edeno/Documents/GitHub/neurospatial/examples\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from neurospatial import (\n",
    "    BodypartOverlay,\n",
    "    Environment,\n",
    "    HeadDirectionOverlay,\n",
    "    PositionOverlay,\n",
    ")\n",
    "from neurospatial.animation.backends.video_backend import check_ffmpeg_available\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path.cwd()\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup: Create Environment and Simulate Data\n",
    "\n",
    "We'll create a circular arena and simulate:\n",
    "- A place field that tracks with the animal\n",
    "- Animal trajectory exploring the arena\n",
    "- Head direction as the animal moves\n",
    "- Pose data (nose, body center, tail base) for skeleton visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup_env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating circular arena environment...\n",
      "Environment: 1264 bins, 2D\n",
      "Regions: ['reward']\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating circular arena environment...\")\n",
    "\n",
    "# Circular arena (50 cm radius)\n",
    "center = Point(50, 50)\n",
    "radius = 50.0\n",
    "circle = center.buffer(radius)\n",
    "\n",
    "env = Environment.from_polygon(polygon=circle, bin_size=2.5, name=\"CircularArena\")\n",
    "env.units = \"cm\"\n",
    "env.frame = \"open_field\"\n",
    "\n",
    "# Add region of interest (reward zone in upper-right quadrant)\n",
    "reward_zone = Point(65, 65)\n",
    "env.regions.add(\"reward\", point=reward_zone)\n",
    "\n",
    "print(f\"Environment: {env.n_bins} bins, {env.n_dims}D\")\n",
    "print(f\"Regions: {list(env.regions.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simulate_trajectory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating animal trajectory...\n",
      "Trajectory: 50 frames\n",
      "  Position range: [13.9, 89.4] cm\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSimulating animal trajectory...\")\n",
    "\n",
    "n_frames = 50  # 50 time points\n",
    "t = np.linspace(0, 4 * np.pi, n_frames)  # 2 revolutions\n",
    "\n",
    "# Spiral trajectory from center outward\n",
    "r = np.linspace(5, 40, n_frames)  # Radius increases\n",
    "theta = t + np.random.randn(n_frames) * 0.1  # Angle with noise\n",
    "\n",
    "# Convert to Cartesian (center at 50, 50)\n",
    "trajectory = np.column_stack([50 + r * np.cos(theta), 50 + r * np.sin(theta)])\n",
    "\n",
    "# Head direction (tangent to spiral)\n",
    "head_angles = theta + np.pi / 2  # Perpendicular to radius\n",
    "\n",
    "print(f\"Trajectory: {n_frames} frames\")\n",
    "print(f\"  Position range: [{trajectory.min():.1f}, {trajectory.max():.1f}] cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "simulate_pose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating pose data (nose, body, tail)...\n",
      "Pose: 3 keypoints (nose, body, tail)\n",
      "Skeleton: 2 edges\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSimulating pose data (nose, body, tail)...\")\n",
    "\n",
    "# Pose: 3 keypoints with skeleton\n",
    "body_length = 10.0  # cm\n",
    "\n",
    "# Nose: ahead of body center\n",
    "nose_offset = body_length * 0.5\n",
    "nose_x = trajectory[:, 0] + nose_offset * np.cos(head_angles)\n",
    "nose_y = trajectory[:, 1] + nose_offset * np.sin(head_angles)\n",
    "\n",
    "# Body center: trajectory position\n",
    "body_x = trajectory[:, 0]\n",
    "body_y = trajectory[:, 1]\n",
    "\n",
    "# Tail: behind body center\n",
    "tail_offset = body_length * 0.5\n",
    "tail_x = trajectory[:, 0] - tail_offset * np.cos(head_angles)\n",
    "tail_y = trajectory[:, 1] - tail_offset * np.sin(head_angles)\n",
    "\n",
    "# Pose dictionary\n",
    "pose_data = {\n",
    "    \"nose\": np.column_stack([nose_x, nose_y]),\n",
    "    \"body\": np.column_stack([body_x, body_y]),\n",
    "    \"tail\": np.column_stack([tail_x, tail_y]),\n",
    "}\n",
    "\n",
    "# Skeleton: connections between keypoints\n",
    "skeleton = [(\"tail\", \"body\"), (\"body\", \"nose\")]\n",
    "\n",
    "print(\"Pose: 3 keypoints (nose, body, tail)\")\n",
    "print(f\"Skeleton: {len(skeleton)} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "simulate_fields",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating place field that tracks with animal...\n",
      "Fields: (50, 1264) (frames x bins)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSimulating place field that tracks with animal...\")\n",
    "\n",
    "# Place field centered on animal position at each frame\n",
    "fields = []\n",
    "for i in range(n_frames):\n",
    "    # Find bin closest to animal position\n",
    "    pos = trajectory[i : i + 1]  # Shape (1, 2)\n",
    "    center_bin = env.bin_at(pos)[0]\n",
    "\n",
    "    # Gaussian field around animal\n",
    "    distances = env.distance_to([center_bin])\n",
    "    sigma = 12.0  # cm\n",
    "    field = np.exp(-(distances**2) / (2 * sigma**2))\n",
    "\n",
    "    # Add noise\n",
    "    field = field + np.random.randn(env.n_bins) * 0.1\n",
    "    field = np.maximum(field, 0)\n",
    "\n",
    "    fields.append(field)\n",
    "\n",
    "fields = np.array(fields)\n",
    "print(f\"Fields: {fields.shape} (frames x bins)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1",
   "metadata": {},
   "source": [
    "## Example 1: Position Overlay with Trail\n",
    "\n",
    "Overlay the animal's trajectory on the animated field with a decaying trail showing recent positions.\n",
    "\n",
    "**Key features**:\n",
    "- `trail_length=10` shows last 10 frames\n",
    "- Trail fades from current (opaque) to past (transparent)\n",
    "- Current position rendered as a marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ex1_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Position Overlay with Trail\n",
      "  Trajectory: 50 frames\n",
      "  Trail length: 10 frames (decaying alpha)\n",
      "  Color: red, Size: 12.0\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:547: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_tracks() got an unexpected keyword argument 'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_ipython\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLaunching Napari viewer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m viewer = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43manimate_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition_overlay\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnapari\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPosition Overlay with Trail\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Napari viewer opened\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Watch the red trail follow the animal\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/decorators.py:148\u001b[39m, in \u001b[36mcheck_fitted.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_fitted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EnvironmentNotFittedError(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, method.\u001b[34m__name__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/visualization.py:822\u001b[39m, in \u001b[36mEnvironmentVisualization.animate_fields\u001b[39m\u001b[34m(self, fields, backend, save_path, fps, cmap, vmin, vmax, frame_labels, overlay_trajectory, title, dpi, codec, bitrate, n_workers, dry_run, image_format, max_html_frames, contrast_limits, show_colorbar, colorbar_label, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Animate spatial fields over time with multiple backend options.\u001b[39;00m\n\u001b[32m    489\u001b[39m \n\u001b[32m    490\u001b[39m \u001b[33;03mCreates animations of spatial field data across different time points,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    818\u001b[39m \n\u001b[32m    819\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animate_fields \u001b[38;5;28;01mas\u001b[39;00m _animate\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_animate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Forward backend-specific parameters (e.g., layout, layer_names for napari)\u001b[39;49;00m\n\u001b[32m    849\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/core.py:206\u001b[39m, in \u001b[36manimate_fields\u001b[39m\u001b[34m(env, fields, backend, save_path, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mnapari\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnapari_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_napari\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrender_napari\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]  # Backend signatures updated in future milestone\u001b[39;49;00m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvideo_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_video\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:934\u001b[39m, in \u001b[36mrender_napari\u001b[39m\u001b[34m(env, fields, fps, cmap, vmin, vmax, frame_labels, title, cache_size, chunk_size, max_chunks, layout, layer_names, overlay_data, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, pos_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.positions):\n\u001b[32m    933\u001b[39m     suffix = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(overlay_data.positions) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m     \u001b[43m_render_position_overlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[38;5;66;03m# Render bodypart overlays (points + skeleton)\u001b[39;00m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, bodypart_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.bodypart_sets):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:92\u001b[39m, in \u001b[36m_render_position_overlay\u001b[39m\u001b[34m(viewer, position_data, name_suffix)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_data.trail_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Create track data: (track_id, time, y, x)\u001b[39;00m\n\u001b[32m     83\u001b[39m     track_data = np.column_stack(\n\u001b[32m     84\u001b[39m         [\n\u001b[32m     85\u001b[39m             np.zeros(n_frames),  \u001b[38;5;66;03m# Single track\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m         ]\n\u001b[32m     90\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     layer = \u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_tracks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrack_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPosition Trail\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname_suffix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtail_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrail_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     layers.append(layer)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Add points layer for current position marker\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Create points with time dimension: (time, y, x)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: add_tracks() got an unexpected keyword argument 'color'"
     ]
    }
   ],
   "source": [
    "# Create position overlay with trail\n",
    "position_overlay = PositionOverlay(\n",
    "    data=trajectory,\n",
    "    color=\"red\",\n",
    "    size=12.0,\n",
    "    trail_length=10,  # Show last 10 frames as trail\n",
    ")\n",
    "\n",
    "print(\"Example 1: Position Overlay with Trail\")\n",
    "print(f\"  Trajectory: {trajectory.shape[0]} frames\")\n",
    "print(\"  Trail length: 10 frames (decaying alpha)\")\n",
    "print(\"  Color: red, Size: 12.0\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay],\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Position Overlay with Trail\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch the red trail follow the animal\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2",
   "metadata": {},
   "source": [
    "## Example 2: Pose Tracking with Skeleton\n",
    "\n",
    "Overlay full pose data (nose, body, tail) with skeleton connecting the keypoints.\n",
    "\n",
    "**Key features**:\n",
    "- `data` is a dict mapping bodypart names to trajectories\n",
    "- `skeleton` defines edges between bodyparts\n",
    "- `colors` can customize per-bodypart colors\n",
    "- Skeleton rendered with specified color and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ex2_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2: Pose Tracking with Skeleton\n",
      "  Bodyparts: ['nose', 'body', 'tail']\n",
      "  Skeleton edges: [('tail', 'body'), ('body', 'nose')]\n",
      "  Colors: nose=yellow, body=red, tail=blue\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:547: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n",
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:186: FutureWarning: Argument 'edge_width' is deprecated, please use 'border_width' instead. The argument 'edge_width' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  layer = viewer.add_points(\n",
      "/Users/edeno/Documents/GitHub/neurospatial/.venv/lib/python3.13/site-packages/napari/utils/migrations.py:101: FutureWarning: Argument 'edge_color' is deprecated, please use 'border_color' instead. The argument 'edge_color' was deprecated in 0.5.0 and it will be removed in 0.6.0.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Napari viewer opened\n",
      "  Watch the skeleton follow the animal pose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:547: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    }
   ],
   "source": [
    "# Create bodypart overlay with skeleton\n",
    "bodypart_overlay = BodypartOverlay(\n",
    "    data=pose_data,\n",
    "    skeleton=skeleton,\n",
    "    colors={\"nose\": \"yellow\", \"body\": \"red\", \"tail\": \"blue\"},\n",
    "    skeleton_color=\"white\",\n",
    "    skeleton_width=2.0,\n",
    ")\n",
    "\n",
    "print(\"Example 2: Pose Tracking with Skeleton\")\n",
    "print(f\"  Bodyparts: {list(pose_data.keys())}\")\n",
    "print(f\"  Skeleton edges: {skeleton}\")\n",
    "print(\"  Colors: nose=yellow, body=red, tail=blue\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[bodypart_overlay],\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Pose Tracking with Skeleton\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch the skeleton follow the animal pose\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3",
   "metadata": {},
   "source": [
    "## Example 3: Head Direction Visualization\n",
    "\n",
    "Overlay head direction as dynamic arrows pointing in the direction of travel.\n",
    "\n",
    "**Key features**:\n",
    "- `data` can be angles (radians) or unit vectors\n",
    "- Arrows rendered with specified color and length\n",
    "- Arrow origin is at the animal's position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ex3_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3: Head Direction Visualization\n",
      "  Head angles: 50 frames\n",
      "  Arrow color: yellow, Length: 15.0 cm\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:547: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_tracks() got an unexpected keyword argument 'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLaunching Napari viewer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Combine position + head direction overlays\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m viewer = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43manimate_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition_overlay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_direction_overlay\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnapari\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPosition + Head Direction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Napari viewer opened\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Watch the yellow arrow show heading direction\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/decorators.py:148\u001b[39m, in \u001b[36mcheck_fitted.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_fitted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EnvironmentNotFittedError(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, method.\u001b[34m__name__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/visualization.py:822\u001b[39m, in \u001b[36mEnvironmentVisualization.animate_fields\u001b[39m\u001b[34m(self, fields, backend, save_path, fps, cmap, vmin, vmax, frame_labels, overlay_trajectory, title, dpi, codec, bitrate, n_workers, dry_run, image_format, max_html_frames, contrast_limits, show_colorbar, colorbar_label, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Animate spatial fields over time with multiple backend options.\u001b[39;00m\n\u001b[32m    489\u001b[39m \n\u001b[32m    490\u001b[39m \u001b[33;03mCreates animations of spatial field data across different time points,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    818\u001b[39m \n\u001b[32m    819\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animate_fields \u001b[38;5;28;01mas\u001b[39;00m _animate\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_animate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Forward backend-specific parameters (e.g., layout, layer_names for napari)\u001b[39;49;00m\n\u001b[32m    849\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/core.py:206\u001b[39m, in \u001b[36manimate_fields\u001b[39m\u001b[34m(env, fields, backend, save_path, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mnapari\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnapari_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_napari\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrender_napari\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]  # Backend signatures updated in future milestone\u001b[39;49;00m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvideo_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_video\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:934\u001b[39m, in \u001b[36mrender_napari\u001b[39m\u001b[34m(env, fields, fps, cmap, vmin, vmax, frame_labels, title, cache_size, chunk_size, max_chunks, layout, layer_names, overlay_data, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, pos_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.positions):\n\u001b[32m    933\u001b[39m     suffix = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(overlay_data.positions) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m     \u001b[43m_render_position_overlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[38;5;66;03m# Render bodypart overlays (points + skeleton)\u001b[39;00m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, bodypart_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.bodypart_sets):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:92\u001b[39m, in \u001b[36m_render_position_overlay\u001b[39m\u001b[34m(viewer, position_data, name_suffix)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_data.trail_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Create track data: (track_id, time, y, x)\u001b[39;00m\n\u001b[32m     83\u001b[39m     track_data = np.column_stack(\n\u001b[32m     84\u001b[39m         [\n\u001b[32m     85\u001b[39m             np.zeros(n_frames),  \u001b[38;5;66;03m# Single track\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m         ]\n\u001b[32m     90\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     layer = \u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_tracks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrack_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPosition Trail\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname_suffix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtail_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrail_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     layers.append(layer)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Add points layer for current position marker\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Create points with time dimension: (time, y, x)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: add_tracks() got an unexpected keyword argument 'color'"
     ]
    }
   ],
   "source": [
    "# Create head direction overlay (angles in radians)\n",
    "head_direction_overlay = HeadDirectionOverlay(\n",
    "    data=head_angles,\n",
    "    color=\"yellow\",\n",
    "    length=15.0,  # Arrow length in cm\n",
    ")\n",
    "\n",
    "print(\"Example 3: Head Direction Visualization\")\n",
    "print(f\"  Head angles: {head_angles.shape[0]} frames\")\n",
    "print(\"  Arrow color: yellow, Length: 15.0 cm\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "\n",
    "    # Combine position + head direction overlays\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay, head_direction_overlay],\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Position + Head Direction\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch the yellow arrow show heading direction\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4",
   "metadata": {},
   "source": [
    "## Example 4: Multi-Animal Tracking\n",
    "\n",
    "Track multiple animals simultaneously by providing multiple overlay instances.\n",
    "\n",
    "**Key features**:\n",
    "- Pass a list of overlays for each animal\n",
    "- Each overlay automatically gets a suffix (e.g., \"Position_1\", \"Position_2\")\n",
    "- All animals rendered in the same animation with different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ex4_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4: Multi-Animal Tracking\n",
      "\n",
      "Simulating second animal...\n",
      "  Animal 1: red\n",
      "  Animal 2: blue (offset trajectory)\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:547: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_tracks() got an unexpected keyword argument 'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_ipython\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLaunching Napari viewer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m viewer = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43manimate_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43manimal1_overlay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manimal2_overlay\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Multiple overlays\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnapari\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMulti-Animal Tracking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Napari viewer opened\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Watch both animals explore simultaneously\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/decorators.py:148\u001b[39m, in \u001b[36mcheck_fitted.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_fitted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EnvironmentNotFittedError(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, method.\u001b[34m__name__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/visualization.py:822\u001b[39m, in \u001b[36mEnvironmentVisualization.animate_fields\u001b[39m\u001b[34m(self, fields, backend, save_path, fps, cmap, vmin, vmax, frame_labels, overlay_trajectory, title, dpi, codec, bitrate, n_workers, dry_run, image_format, max_html_frames, contrast_limits, show_colorbar, colorbar_label, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Animate spatial fields over time with multiple backend options.\u001b[39;00m\n\u001b[32m    489\u001b[39m \n\u001b[32m    490\u001b[39m \u001b[33;03mCreates animations of spatial field data across different time points,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    818\u001b[39m \n\u001b[32m    819\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animate_fields \u001b[38;5;28;01mas\u001b[39;00m _animate\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_animate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Forward backend-specific parameters (e.g., layout, layer_names for napari)\u001b[39;49;00m\n\u001b[32m    849\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/core.py:206\u001b[39m, in \u001b[36manimate_fields\u001b[39m\u001b[34m(env, fields, backend, save_path, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mnapari\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnapari_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_napari\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrender_napari\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]  # Backend signatures updated in future milestone\u001b[39;49;00m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvideo_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_video\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:934\u001b[39m, in \u001b[36mrender_napari\u001b[39m\u001b[34m(env, fields, fps, cmap, vmin, vmax, frame_labels, title, cache_size, chunk_size, max_chunks, layout, layer_names, overlay_data, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, pos_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.positions):\n\u001b[32m    933\u001b[39m     suffix = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(overlay_data.positions) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m     \u001b[43m_render_position_overlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[38;5;66;03m# Render bodypart overlays (points + skeleton)\u001b[39;00m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, bodypart_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.bodypart_sets):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:92\u001b[39m, in \u001b[36m_render_position_overlay\u001b[39m\u001b[34m(viewer, position_data, name_suffix)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_data.trail_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Create track data: (track_id, time, y, x)\u001b[39;00m\n\u001b[32m     83\u001b[39m     track_data = np.column_stack(\n\u001b[32m     84\u001b[39m         [\n\u001b[32m     85\u001b[39m             np.zeros(n_frames),  \u001b[38;5;66;03m# Single track\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m         ]\n\u001b[32m     90\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     layer = \u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_tracks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrack_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPosition Trail\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname_suffix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtail_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrail_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     layers.append(layer)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Add points layer for current position marker\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Create points with time dimension: (time, y, x)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: add_tracks() got an unexpected keyword argument 'color'"
     ]
    }
   ],
   "source": [
    "print(\"Example 4: Multi-Animal Tracking\")\n",
    "print(\"\\nSimulating second animal...\")\n",
    "\n",
    "# Second animal with offset trajectory\n",
    "trajectory_2 = trajectory + np.array([10, -10])  # Offset spatially\n",
    "trajectory_2 = np.clip(trajectory_2, 5, 95)  # Keep in bounds\n",
    "\n",
    "# Create overlays for both animals\n",
    "animal1_overlay = PositionOverlay(\n",
    "    data=trajectory, color=\"red\", size=12.0, trail_length=8\n",
    ")\n",
    "\n",
    "animal2_overlay = PositionOverlay(\n",
    "    data=trajectory_2, color=\"blue\", size=12.0, trail_length=8\n",
    ")\n",
    "\n",
    "print(\"  Animal 1: red\")\n",
    "print(\"  Animal 2: blue (offset trajectory)\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[animal1_overlay, animal2_overlay],  # Multiple overlays\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Multi-Animal Tracking\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch both animals explore simultaneously\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex5",
   "metadata": {},
   "source": [
    "## Example 5: Regions Overlay with Spatial Fields\n",
    "\n",
    "Highlight spatial regions of interest (e.g., reward zones) alongside overlays.\n",
    "\n",
    "**Key features**:\n",
    "- `show_regions=True` displays all defined regions\n",
    "- `show_regions=[\"reward\"]` displays specific regions only\n",
    "- `region_alpha=0.3` controls transparency\n",
    "- Regions rendered as colored polygons/points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ex5_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 5: Regions Overlay\n",
      "  Showing region: ['reward']\n",
      "  Region alpha: 0.3 (30% transparent)\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:547: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_tracks() got an unexpected keyword argument 'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_ipython\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLaunching Napari viewer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m viewer = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43manimate_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition_overlay\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Show all regions\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 30% transparent\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnapari\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPosition + Reward Region\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Napari viewer opened\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Watch the animal approach the reward region\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/decorators.py:148\u001b[39m, in \u001b[36mcheck_fitted.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_fitted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EnvironmentNotFittedError(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, method.\u001b[34m__name__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/visualization.py:822\u001b[39m, in \u001b[36mEnvironmentVisualization.animate_fields\u001b[39m\u001b[34m(self, fields, backend, save_path, fps, cmap, vmin, vmax, frame_labels, overlay_trajectory, title, dpi, codec, bitrate, n_workers, dry_run, image_format, max_html_frames, contrast_limits, show_colorbar, colorbar_label, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Animate spatial fields over time with multiple backend options.\u001b[39;00m\n\u001b[32m    489\u001b[39m \n\u001b[32m    490\u001b[39m \u001b[33;03mCreates animations of spatial field data across different time points,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    818\u001b[39m \n\u001b[32m    819\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animate_fields \u001b[38;5;28;01mas\u001b[39;00m _animate\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_animate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Forward backend-specific parameters (e.g., layout, layer_names for napari)\u001b[39;49;00m\n\u001b[32m    849\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/core.py:206\u001b[39m, in \u001b[36manimate_fields\u001b[39m\u001b[34m(env, fields, backend, save_path, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mnapari\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnapari_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_napari\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrender_napari\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]  # Backend signatures updated in future milestone\u001b[39;49;00m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvideo_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_video\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:934\u001b[39m, in \u001b[36mrender_napari\u001b[39m\u001b[34m(env, fields, fps, cmap, vmin, vmax, frame_labels, title, cache_size, chunk_size, max_chunks, layout, layer_names, overlay_data, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, pos_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.positions):\n\u001b[32m    933\u001b[39m     suffix = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(overlay_data.positions) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m     \u001b[43m_render_position_overlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[38;5;66;03m# Render bodypart overlays (points + skeleton)\u001b[39;00m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, bodypart_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.bodypart_sets):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:92\u001b[39m, in \u001b[36m_render_position_overlay\u001b[39m\u001b[34m(viewer, position_data, name_suffix)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_data.trail_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Create track data: (track_id, time, y, x)\u001b[39;00m\n\u001b[32m     83\u001b[39m     track_data = np.column_stack(\n\u001b[32m     84\u001b[39m         [\n\u001b[32m     85\u001b[39m             np.zeros(n_frames),  \u001b[38;5;66;03m# Single track\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m         ]\n\u001b[32m     90\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     layer = \u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_tracks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrack_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPosition Trail\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname_suffix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtail_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrail_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     layers.append(layer)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Add points layer for current position marker\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Create points with time dimension: (time, y, x)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: add_tracks() got an unexpected keyword argument 'color'"
     ]
    }
   ],
   "source": [
    "print(\"Example 5: Regions Overlay\")\n",
    "print(f\"  Showing region: {list(env.regions.keys())}\")\n",
    "print(\"  Region alpha: 0.3 (30% transparent)\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay],\n",
    "        show_regions=True,  # Show all regions\n",
    "        region_alpha=0.3,  # 30% transparent\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Position + Reward Region\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Watch the animal approach the reward region\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex6",
   "metadata": {},
   "source": [
    "## Example 6: Mixed-Rate Temporal Alignment\n",
    "\n",
    "Align overlays sampled at different rates using temporal timestamps.\n",
    "\n",
    "**Key features**:\n",
    "- Overlay `times` parameter specifies timestamps for each frame\n",
    "- `frame_times` parameter specifies field frame timestamps\n",
    "- Linear interpolation automatically aligns overlay to field frames\n",
    "- Works even when overlay and fields have different sampling rates\n",
    "\n",
    "**Example**: Position tracked at 120 Hz, fields computed at 10 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ex6_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 6: Mixed-Rate Temporal Alignment\n",
      "\n",
      "Simulating high-frequency position tracking...\n",
      "  Position tracking: 600 samples at 120 Hz\n",
      "  Field computation: 50 frames at 10 Hz\n",
      "\n",
      "✓ Overlay will be interpolated to match field frame times\n",
      "  (Linear interpolation: 120 Hz → 10 Hz)\n",
      "\n",
      "Launching Napari viewer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:547: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_tracks() got an unexpected keyword argument 'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_ipython\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLaunching Napari viewer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m viewer = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43manimate_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields_low_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition_overlay_timed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Explicit field timestamps\u001b[39;49;00m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnapari\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMixed-Rate Alignment (120 Hz → 10 Hz)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Napari viewer opened\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Position automatically aligned to field frames\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/decorators.py:148\u001b[39m, in \u001b[36mcheck_fitted.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_fitted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EnvironmentNotFittedError(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, method.\u001b[34m__name__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/visualization.py:822\u001b[39m, in \u001b[36mEnvironmentVisualization.animate_fields\u001b[39m\u001b[34m(self, fields, backend, save_path, fps, cmap, vmin, vmax, frame_labels, overlay_trajectory, title, dpi, codec, bitrate, n_workers, dry_run, image_format, max_html_frames, contrast_limits, show_colorbar, colorbar_label, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Animate spatial fields over time with multiple backend options.\u001b[39;00m\n\u001b[32m    489\u001b[39m \n\u001b[32m    490\u001b[39m \u001b[33;03mCreates animations of spatial field data across different time points,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    818\u001b[39m \n\u001b[32m    819\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animate_fields \u001b[38;5;28;01mas\u001b[39;00m _animate\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_animate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Forward backend-specific parameters (e.g., layout, layer_names for napari)\u001b[39;49;00m\n\u001b[32m    849\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/core.py:206\u001b[39m, in \u001b[36manimate_fields\u001b[39m\u001b[34m(env, fields, backend, save_path, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mnapari\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnapari_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_napari\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrender_napari\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]  # Backend signatures updated in future milestone\u001b[39;49;00m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvideo_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_video\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:934\u001b[39m, in \u001b[36mrender_napari\u001b[39m\u001b[34m(env, fields, fps, cmap, vmin, vmax, frame_labels, title, cache_size, chunk_size, max_chunks, layout, layer_names, overlay_data, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, pos_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.positions):\n\u001b[32m    933\u001b[39m     suffix = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(overlay_data.positions) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m     \u001b[43m_render_position_overlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[38;5;66;03m# Render bodypart overlays (points + skeleton)\u001b[39;00m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, bodypart_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.bodypart_sets):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:92\u001b[39m, in \u001b[36m_render_position_overlay\u001b[39m\u001b[34m(viewer, position_data, name_suffix)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_data.trail_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Create track data: (track_id, time, y, x)\u001b[39;00m\n\u001b[32m     83\u001b[39m     track_data = np.column_stack(\n\u001b[32m     84\u001b[39m         [\n\u001b[32m     85\u001b[39m             np.zeros(n_frames),  \u001b[38;5;66;03m# Single track\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m         ]\n\u001b[32m     90\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     layer = \u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_tracks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrack_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPosition Trail\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname_suffix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtail_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrail_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     layers.append(layer)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Add points layer for current position marker\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Create points with time dimension: (time, y, x)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: add_tracks() got an unexpected keyword argument 'color'"
     ]
    }
   ],
   "source": [
    "print(\"Example 6: Mixed-Rate Temporal Alignment\")\n",
    "print(\"\\nSimulating high-frequency position tracking...\")\n",
    "\n",
    "# High-frequency position tracking (120 Hz)\n",
    "duration = 5.0  # seconds\n",
    "fps_high = 120  # Hz\n",
    "n_samples_high = int(duration * fps_high)  # 600 samples\n",
    "\n",
    "# Generate high-frequency trajectory\n",
    "t_high = np.linspace(0, duration, n_samples_high)\n",
    "theta_high = t_high * 2 * np.pi + np.random.randn(n_samples_high) * 0.05\n",
    "r_high = 20 + 15 * np.sin(t_high * 3)\n",
    "\n",
    "trajectory_high_freq = np.column_stack(\n",
    "    [50 + r_high * np.cos(theta_high), 50 + r_high * np.sin(theta_high)]\n",
    ")\n",
    "timestamps_high = t_high\n",
    "\n",
    "print(f\"  Position tracking: {n_samples_high} samples at {fps_high} Hz\")\n",
    "\n",
    "# Low-frequency fields (10 Hz)\n",
    "fps_low = 10  # Hz\n",
    "n_frames_low = int(duration * fps_low)  # 50 frames\n",
    "frame_times = np.linspace(0, duration, n_frames_low)\n",
    "\n",
    "print(f\"  Field computation: {n_frames_low} frames at {fps_low} Hz\")\n",
    "\n",
    "# Compute fields at low frequency\n",
    "fields_low_freq = []\n",
    "for t in frame_times:\n",
    "    # Find closest high-freq position\n",
    "    idx = np.argmin(np.abs(timestamps_high - t))\n",
    "    pos = trajectory_high_freq[idx : idx + 1]\n",
    "    center_bin = env.bin_at(pos)[0]\n",
    "\n",
    "    distances = env.distance_to([center_bin])\n",
    "    field = np.exp(-(distances**2) / (2 * 12.0**2))\n",
    "    field = field + np.random.randn(env.n_bins) * 0.1\n",
    "    field = np.maximum(field, 0)\n",
    "    fields_low_freq.append(field)\n",
    "\n",
    "fields_low_freq = np.array(fields_low_freq)\n",
    "\n",
    "# Create overlay with timestamps\n",
    "position_overlay_timed = PositionOverlay(\n",
    "    data=trajectory_high_freq,\n",
    "    times=timestamps_high,  # 120 Hz timestamps\n",
    "    color=\"red\",\n",
    "    size=10.0,\n",
    "    trail_length=15,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Overlay will be interpolated to match field frame times\")\n",
    "print(\"  (Linear interpolation: 120 Hz → 10 Hz)\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"\\nLaunching Napari viewer...\")\n",
    "    viewer = env.animate_fields(\n",
    "        fields_low_freq,\n",
    "        overlays=[position_overlay_timed],\n",
    "        frame_times=frame_times,  # Explicit field timestamps\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Mixed-Rate Alignment (120 Hz → 10 Hz)\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari viewer opened\")\n",
    "    print(\"  Position automatically aligned to field frames\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex7",
   "metadata": {},
   "source": [
    "## Example 7: Backend Comparison\n",
    "\n",
    "Compare overlay rendering across all backends with the same data.\n",
    "\n",
    "**Backend capabilities**:\n",
    "\n",
    "| Backend | Position | Bodypart | HeadDirection | Regions |\n",
    "|---------|----------|----------|---------------|--------|\n",
    "| Napari  | ✓ | ✓ | ✓ | ✓ |\n",
    "| Video   | ✓ | ✓ | ✓ | ✓ |\n",
    "| HTML    | ✓ | ✗ | ✗ | ✓ |\n",
    "| Widget  | ✓ | ✓ | ✓ | ✓ |\n",
    "\n",
    "**Note**: HTML backend warns when given unsupported overlay types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ex7_napari",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7a: Napari Backend (Full Support)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edeno/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:547: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  is_playing = viewer.window.qt_viewer.dims.is_playing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_tracks() got an unexpected keyword argument 'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_ipython\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# All overlay types supported\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m viewer = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43manimate_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition_overlay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbodypart_overlay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_direction_overlay\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnapari\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNapari: All Overlays\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Napari: Position + Pose + Head Direction + Regions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_ipython() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/decorators.py:148\u001b[39m, in \u001b[36mcheck_fitted.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_fitted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EnvironmentNotFittedError(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, method.\u001b[34m__name__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/environment/visualization.py:822\u001b[39m, in \u001b[36mEnvironmentVisualization.animate_fields\u001b[39m\u001b[34m(self, fields, backend, save_path, fps, cmap, vmin, vmax, frame_labels, overlay_trajectory, title, dpi, codec, bitrate, n_workers, dry_run, image_format, max_html_frames, contrast_limits, show_colorbar, colorbar_label, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Animate spatial fields over time with multiple backend options.\u001b[39;00m\n\u001b[32m    489\u001b[39m \n\u001b[32m    490\u001b[39m \u001b[33;03mCreates animations of spatial field data across different time points,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    818\u001b[39m \n\u001b[32m    819\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animate_fields \u001b[38;5;28;01mas\u001b[39;00m _animate\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_animate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_trajectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_html_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrast_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_colorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorbar_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Forward backend-specific parameters (e.g., layout, layer_names for napari)\u001b[39;49;00m\n\u001b[32m    849\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/core.py:206\u001b[39m, in \u001b[36manimate_fields\u001b[39m\u001b[34m(env, fields, backend, save_path, overlays, frame_times, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mnapari\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnapari_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_napari\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrender_napari\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]  # Backend signatures updated in future milestone\u001b[39;49;00m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlay_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneurospatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvideo_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_video\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:934\u001b[39m, in \u001b[36mrender_napari\u001b[39m\u001b[34m(env, fields, fps, cmap, vmin, vmax, frame_labels, title, cache_size, chunk_size, max_chunks, layout, layer_names, overlay_data, show_regions, region_alpha, **kwargs)\u001b[39m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, pos_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.positions):\n\u001b[32m    933\u001b[39m     suffix = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(overlay_data.positions) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m     \u001b[43m_render_position_overlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[38;5;66;03m# Render bodypart overlays (points + skeleton)\u001b[39;00m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, bodypart_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overlay_data.bodypart_sets):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/neurospatial/src/neurospatial/animation/backends/napari_backend.py:92\u001b[39m, in \u001b[36m_render_position_overlay\u001b[39m\u001b[34m(viewer, position_data, name_suffix)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_data.trail_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Create track data: (track_id, time, y, x)\u001b[39;00m\n\u001b[32m     83\u001b[39m     track_data = np.column_stack(\n\u001b[32m     84\u001b[39m         [\n\u001b[32m     85\u001b[39m             np.zeros(n_frames),  \u001b[38;5;66;03m# Single track\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m         ]\n\u001b[32m     90\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     layer = \u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_tracks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrack_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPosition Trail\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname_suffix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtail_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrail_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     layers.append(layer)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Add points layer for current position marker\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Create points with time dimension: (time, y, x)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: add_tracks() got an unexpected keyword argument 'color'"
     ]
    }
   ],
   "source": [
    "print(\"Example 7a: Napari Backend (Full Support)\")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    # All overlay types supported\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay, bodypart_overlay, head_direction_overlay],\n",
    "        show_regions=True,\n",
    "        backend=\"napari\",\n",
    "        fps=10,\n",
    "        title=\"Napari: All Overlays\",\n",
    "    )\n",
    "\n",
    "    print(\"✓ Napari: Position + Pose + Head Direction + Regions\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ Napari not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ex7_video",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7b: Video Backend (Full Support)\n",
      "Rendering 50 frames using 4 workers...\n",
      "Estimated time: ~6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Workers: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding video...\n",
      "✓ Video saved to /Users/edeno/Documents/GitHub/neurospatial/examples/17_all_overlays.mp4\n",
      "✓ Video: Saved to /Users/edeno/Documents/GitHub/neurospatial/examples/17_all_overlays.mp4\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 7b: Video Backend (Full Support)\")\n",
    "\n",
    "if check_ffmpeg_available():\n",
    "    # All overlay types supported\n",
    "    output_path = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[position_overlay, bodypart_overlay, head_direction_overlay],\n",
    "        show_regions=True,\n",
    "        backend=\"video\",\n",
    "        save_path=output_dir / \"17_all_overlays.mp4\",\n",
    "        fps=10,\n",
    "        n_workers=4,\n",
    "    )\n",
    "    print(f\"✓ Video: Saved to {output_path}\")\n",
    "else:\n",
    "    print(\"⊗ ffmpeg not available for video export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ex7_html",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7c: HTML Backend (Position + Regions Only)\n",
      "  WARNING: HTML backend does NOT support bodypart or head direction overlays\n",
      "  (Warnings will be emitted if provided)\n",
      "\n",
      "Rendering 50 frames to PNG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding frames: 100%|██████████| 50/50 [00:01<00:00, 41.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HTML saved to /Users/edeno/Documents/GitHub/neurospatial/examples/17_position_only.html (1.3 MB)\n",
      "✓ HTML: Saved to /Users/edeno/Documents/GitHub/neurospatial/examples/17_position_only.html\n",
      "  (Position + Regions rendered; pose/head direction not supported)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 7c: HTML Backend (Position + Regions Only)\")\n",
    "print(\"  WARNING: HTML backend does NOT support bodypart or head direction overlays\")\n",
    "print(\"  (Warnings will be emitted if provided)\\n\")\n",
    "\n",
    "# HTML: Only position and regions supported\n",
    "html_path = env.animate_fields(\n",
    "    fields,\n",
    "    overlays=[position_overlay],  # Only position overlay\n",
    "    show_regions=True,\n",
    "    backend=\"html\",\n",
    "    save_path=output_dir / \"17_position_only.html\",\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "print(f\"✓ HTML: Saved to {html_path}\")\n",
    "print(\"  (Position + Regions rendered; pose/head direction not supported)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ex7_widget",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7d: Widget Backend (Full Support)\n",
      "Pre-rendering 50 frames for widget...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edc244c97f54331bde98713d40ccd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Play(value=0, max=49), IntSlider(value=0, description='Frame:', max=49))), HTML(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Widget: Position + Pose + Head Direction + Regions\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 7d: Widget Backend (Full Support)\")\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    if get_ipython() is not None:\n",
    "        # All overlay types supported\n",
    "        widget = env.animate_fields(\n",
    "            fields,\n",
    "            overlays=[position_overlay, bodypart_overlay, head_direction_overlay],\n",
    "            show_regions=True,\n",
    "            backend=\"widget\",\n",
    "            fps=10,\n",
    "        )\n",
    "        print(\"✓ Widget: Position + Pose + Head Direction + Regions\")\n",
    "    else:\n",
    "        print(\"⊗ Not in Jupyter notebook\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⊗ IPython/ipywidgets not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Overlay Types\n",
    "\n",
    "1. **PositionOverlay**: Trajectories with decaying trails\n",
    "   - `data`: (n_frames, n_dims) array\n",
    "   - `trail_length`: Number of past frames to show\n",
    "   - `color`, `size`: Marker appearance\n",
    "\n",
    "2. **BodypartOverlay**: Pose tracking with skeletons\n",
    "   - `data`: Dict mapping bodypart names to (n_frames, n_dims) arrays\n",
    "   - `skeleton`: List of (bodypart1, bodypart2) edge tuples\n",
    "   - `colors`: Per-bodypart colors\n",
    "   - `skeleton_color`, `skeleton_width`: Skeleton appearance\n",
    "\n",
    "3. **HeadDirectionOverlay**: Orientation arrows\n",
    "   - `data`: (n_frames,) angles in radians OR (n_frames, n_dims) unit vectors\n",
    "   - `color`, `length`: Arrow appearance\n",
    "\n",
    "### Temporal Alignment\n",
    "\n",
    "- Add `times` parameter to overlay for timestamps\n",
    "- Add `frame_times` parameter to `animate_fields()` for field timestamps\n",
    "- Linear interpolation automatically aligns overlay to field frames\n",
    "- Works even when overlay and fields have different sampling rates\n",
    "\n",
    "### Backend Capabilities\n",
    "\n",
    "- **Napari**: Full support (all overlay types + regions)\n",
    "- **Video**: Full support (all overlay types + regions)\n",
    "- **HTML**: Partial support (position + regions only, warns for others)\n",
    "- **Widget**: Full support (all overlay types + regions)\n",
    "\n",
    "### Multi-Animal Support\n",
    "\n",
    "- Pass multiple overlay instances in a list\n",
    "- Each overlay automatically gets a suffix (e.g., \"Position_1\", \"Position_2\")\n",
    "- Use different colors to distinguish animals\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Simple trajectory overlay\n",
    "from neurospatial import PositionOverlay\n",
    "overlay = PositionOverlay(data=trajectory, color=\"red\", trail_length=10)\n",
    "env.animate_fields(fields, overlays=[overlay], backend=\"napari\")\n",
    "\n",
    "# Pose with skeleton\n",
    "from neurospatial import BodypartOverlay\n",
    "overlay = BodypartOverlay(\n",
    "    data={\"nose\": nose_traj, \"body\": body_traj, \"tail\": tail_traj},\n",
    "    skeleton=[(\"tail\", \"body\"), (\"body\", \"nose\")],\n",
    "    skeleton_color=\"white\"\n",
    ")\n",
    "env.animate_fields(fields, overlays=[overlay], backend=\"napari\")\n",
    "\n",
    "# Mixed-rate alignment\n",
    "overlay = PositionOverlay(data=trajectory_120hz, times=times_120hz)\n",
    "env.animate_fields(\n",
    "    fields_10hz,\n",
    "    overlays=[overlay],\n",
    "    frame_times=times_10hz,  # Automatic interpolation\n",
    "    backend=\"napari\"\n",
    ")\n",
    "\n",
    "# Multi-animal\n",
    "env.animate_fields(\n",
    "    fields,\n",
    "    overlays=[overlay_animal1, overlay_animal2],\n",
    "    backend=\"napari\"\n",
    ")\n",
    "\n",
    "# Show regions\n",
    "env.animate_fields(\n",
    "    fields,\n",
    "    overlays=[overlay],\n",
    "    show_regions=True,\n",
    "    region_alpha=0.3,\n",
    "    backend=\"napari\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Video export**: Use `n_workers > 1` for parallel rendering\n",
    "- **Large datasets**: Use Napari for exploration, subsample for video\n",
    "- **HTML file size**: Limit frames (default max 500) or use video backend\n",
    "- **Parallel rendering**: Call `env.clear_cache()` before video export with `n_workers > 1`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Apply overlays to your own behavioral tracking data\n",
    "- Combine multiple overlay types for rich visualizations\n",
    "- Export publication-quality videos with overlays\n",
    "- Use temporal alignment for multi-modal data (tracking + neural recordings)\n",
    "\n",
    "For more details, see:\n",
    "- `docs/animation_overlays.md` - Complete overlay documentation\n",
    "- `examples/16_field_animation.ipynb` - Animation backends without overlays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
