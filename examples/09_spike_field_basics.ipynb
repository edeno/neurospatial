{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1",
   "metadata": {},
   "source": [
    "# Spike Field and Reward Primitives\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Convert spike trains to occupancy-normalized firing rate fields\n",
    "- Use `spikes_to_field()` and `compute_place_field()` functions\n",
    "- Understand min occupancy thresholds and their impact\n",
    "- Generate reward fields for reinforcement learning\n",
    "- Compare different reward decay profiles\n",
    "- Apply reward shaping strategies to spatial navigation tasks\n",
    "\n",
    "**Estimated time: 20-25 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from neurospatial import (\n",
    "    Environment,\n",
    "    compute_place_field,\n",
    "    goal_reward_field,\n",
    "    region_reward_field,\n",
    "    spikes_to_field,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for clear, readable figures\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 5)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"axes.labelsize\"] = 13\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"xtick.labelsize\"] = 11\n",
    "plt.rcParams[\"ytick.labelsize\"] = 11\n",
    "plt.rcParams[\"legend.fontsize\"] = 11\n",
    "plt.rcParams[\"figure.titlesize\"] = 15\n",
    "\n",
    "# Use colorblind-friendly colors (Wong palette)\n",
    "WONG_COLORS = {\n",
    "    \"blue\": \"#0173B2\",\n",
    "    \"orange\": \"#DE8F05\",\n",
    "    \"green\": \"#029E73\",\n",
    "    \"yellow\": \"#FBCA00\",\n",
    "    \"purple\": \"#7C3C88\",\n",
    "    \"cyan\": \"#56B4E9\",\n",
    "    \"red\": \"#CC78BC\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Converting Spike Trains to Firing Rate Maps\n",
    "\n",
    "### Why Occupancy Normalization?\n",
    "\n",
    "In neuroscience, we need to normalize spike counts by the time spent in each spatial location (occupancy) to get meaningful firing rate estimates. Without normalization, frequently-visited locations would appear to have higher firing rates simply due to more samples, not actual spatial preference.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\\text{firing rate}_i = \\frac{\\text{spike count}_i}{\\text{occupancy}_i \\text{ (seconds)}}$$\n",
    "\n",
    "This is the **standard approach** in place field analysis (O'Keefe & Dostrovsky, 1971)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data\n",
    "\n",
    "Let's create a simulated trajectory and spike train for a place cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate circular trajectory (60 seconds at 30 Hz)\n",
    "times = np.linspace(0, 60, 1800)\n",
    "radius = 40\n",
    "positions = np.column_stack(\n",
    "    [\n",
    "        radius * np.cos(0.5 * times),  # X coordinate\n",
    "        radius * np.sin(0.5 * times),  # Y coordinate\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Simulate place cell with Gaussian tuning\n",
    "preferred_location = np.array([20.0, 20.0])  # Top-right quadrant\n",
    "tuning_width = 10.0  # cm\n",
    "\n",
    "# Compute distance to preferred location\n",
    "distances = np.linalg.norm(positions - preferred_location, axis=1)\n",
    "\n",
    "# Gaussian spatial tuning (peak 15 Hz)\n",
    "peak_rate = 15.0  # Hz\n",
    "instantaneous_rate = peak_rate * np.exp(-(distances**2) / (2 * tuning_width**2))\n",
    "\n",
    "# Generate spikes using Poisson process\n",
    "dt = np.mean(np.diff(times))  # ~0.033 seconds\n",
    "spike_prob = instantaneous_rate * dt\n",
    "spike_mask = np.random.rand(len(times)) < spike_prob\n",
    "spike_times = times[spike_mask]\n",
    "\n",
    "print(f\"Generated trajectory: {len(times)} samples over {times[-1]:.1f} seconds\")\n",
    "print(\n",
    "    f\"Generated spikes: {len(spike_times)} spikes (mean rate: {len(spike_times) / times[-1]:.2f} Hz)\"\n",
    ")\n",
    "print(f\"Preferred location: {preferred_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7",
   "metadata": {},
   "source": [
    "### Create Environment\n",
    "\n",
    "Discretize the continuous space into spatial bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment from trajectory\n",
    "env = Environment.from_samples(positions, bin_size=5.0)\n",
    "env.units = \"cm\"\n",
    "\n",
    "print(f\"Environment: {env.n_bins} bins\")\n",
    "print(\n",
    "    f\"Spatial extent: X=[{env.dimension_ranges[0][0]:.1f}, {env.dimension_ranges[0][1]:.1f}] cm\"\n",
    ")\n",
    "print(\n",
    "    f\"                Y=[{env.dimension_ranges[1][0]:.1f}, {env.dimension_ranges[1][1]:.1f}] cm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9",
   "metadata": {},
   "source": [
    "### Compute Firing Rate Field\n",
    "\n",
    "Now let's convert the spike train to a firing rate map using `spikes_to_field()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute firing rate field (default: no occupancy filtering)\n",
    "firing_rate_raw = spikes_to_field(\n",
    "    env,\n",
    "    spike_times,\n",
    "    times,\n",
    "    positions,\n",
    "    min_occupancy_seconds=0.0,  # Include all bins\n",
    ")\n",
    "\n",
    "# Compute with occupancy threshold (standard practice)\n",
    "firing_rate_filtered = spikes_to_field(\n",
    "    env,\n",
    "    spike_times,\n",
    "    times,\n",
    "    positions,\n",
    "    min_occupancy_seconds=0.5,  # Exclude bins with < 0.5 seconds\n",
    ")\n",
    "\n",
    "# Also compute occupancy for visualization\n",
    "occupancy = env.occupancy(times, positions, return_seconds=True)\n",
    "\n",
    "print(\n",
    "    f\"Firing rate range (raw): {np.nanmin(firing_rate_raw):.2f} - {np.nanmax(firing_rate_raw):.2f} Hz\"\n",
    ")\n",
    "print(\n",
    "    f\"Firing rate range (filtered): {np.nanmin(firing_rate_filtered):.2f} - {np.nanmax(firing_rate_filtered):.2f} Hz\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of NaN bins (filtered): {np.sum(np.isnan(firing_rate_filtered))} / {env.n_bins}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11",
   "metadata": {},
   "source": [
    "### Visualize: Occupancy vs Firing Rate\n",
    "\n",
    "Let's compare occupancy, raw firing rate, and filtered firing rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), constrained_layout=True)\n",
    "\n",
    "# Plot occupancy\n",
    "env.plot(occupancy, ax=axes[0], cmap=\"viridis\")\n",
    "axes[0].set_title(\"Occupancy (seconds)\", fontsize=14, fontweight=\"bold\", pad=10)\n",
    "axes[0].set_xlabel(\"X (cm)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Y (cm)\", fontsize=12)\n",
    "\n",
    "# Plot raw firing rate\n",
    "env.plot(firing_rate_raw, ax=axes[1], cmap=\"hot\", vmin=0)\n",
    "axes[1].set_title(\"Firing Rate (raw)\", fontsize=14, fontweight=\"bold\", pad=10)\n",
    "axes[1].set_xlabel(\"X (cm)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Y (cm)\", fontsize=12)\n",
    "\n",
    "# Plot filtered firing rate\n",
    "env.plot(firing_rate_filtered, ax=axes[2], cmap=\"hot\", vmin=0)\n",
    "axes[2].set_title(\n",
    "    \"Firing Rate (filtered, min_occ=0.5s)\", fontsize=14, fontweight=\"bold\", pad=10\n",
    ")\n",
    "axes[2].set_xlabel(\"X (cm)\", fontsize=12)\n",
    "axes[2].set_ylabel(\"Y (cm)\", fontsize=12)\n",
    "\n",
    "# Mark preferred location\n",
    "for ax in axes[1:]:\n",
    "    ax.plot(\n",
    "        preferred_location[0],\n",
    "        preferred_location[1],\n",
    "        \"*\",\n",
    "        color=WONG_COLORS[\"cyan\"],\n",
    "        markersize=18,\n",
    "        markeredgecolor=\"white\",\n",
    "        markeredgewidth=2,\n",
    "        label=\"True location\",\n",
    "    )\n",
    "    ax.legend(loc=\"upper right\", fontsize=11, framealpha=0.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- **Occupancy**: Shows the circular trajectory (uniform along the circle)\n",
    "- **Raw firing rate**: Includes all bins, even those with very brief visits (can be noisy)\n",
    "- **Filtered firing rate**: Excludes unreliable bins (< 0.5 seconds), cleaner place field\n",
    "\n",
    "The filtered version is **standard practice** in place field analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14",
   "metadata": {},
   "source": [
    "### Smoothed Place Fields\n",
    "\n",
    "For typical place field analysis, we also apply Gaussian smoothing using `compute_place_field()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner: spike conversion + smoothing\n",
    "place_field = compute_place_field(\n",
    "    env,\n",
    "    spike_times,\n",
    "    times,\n",
    "    positions,\n",
    "    min_occupancy_seconds=0.5,\n",
    "    smoothing_bandwidth=8.0,  # Gaussian kernel bandwidth (cm)\n",
    ")\n",
    "\n",
    "# Compare with two-step approach (equivalent)\n",
    "firing_rate_manual = spikes_to_field(\n",
    "    env, spike_times, times, positions, min_occupancy_seconds=0.5\n",
    ")\n",
    "place_field_manual = env.smooth(firing_rate_manual, bandwidth=8.0)\n",
    "\n",
    "print(f\"Place field peak: {np.nanmax(place_field):.2f} Hz\")\n",
    "print(\n",
    "    f\"Place field matches manual: {np.allclose(place_field, place_field_manual, equal_nan=True)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: raw vs smoothed\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), constrained_layout=True)\n",
    "\n",
    "env.plot(firing_rate_filtered, ax=axes[0], cmap=\"hot\", vmin=0)\n",
    "axes[0].set_title(\"Raw Firing Rate\", fontsize=14, fontweight=\"bold\", pad=10)\n",
    "axes[0].plot(\n",
    "    preferred_location[0],\n",
    "    preferred_location[1],\n",
    "    \"*\",\n",
    "    color=WONG_COLORS[\"cyan\"],\n",
    "    markersize=18,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=2,\n",
    ")\n",
    "\n",
    "env.plot(place_field, ax=axes[1], cmap=\"hot\", vmin=0)\n",
    "axes[1].set_title(\n",
    "    \"Smoothed Place Field (bandwidth=8 cm)\", fontsize=14, fontweight=\"bold\", pad=10\n",
    ")\n",
    "axes[1].plot(\n",
    "    preferred_location[0],\n",
    "    preferred_location[1],\n",
    "    \"*\",\n",
    "    color=WONG_COLORS[\"cyan\"],\n",
    "    markersize=18,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=2,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Reward Fields for Reinforcement Learning\n",
    "\n",
    "Reward shaping provides gradient information to help RL agents learn faster. However, it must be used carefully to avoid biasing policies toward suboptimal solutions.\n",
    "\n",
    "### Region-Based Rewards\n",
    "\n",
    "Let's create a goal region and explore different decay profiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define goal region at top-right\n",
    "goal_location = np.array([25.0, 25.0])\n",
    "env.regions.add(\"goal\", point=goal_location)\n",
    "\n",
    "print(f\"Goal region added at {goal_location}\")\n",
    "print(f\"Available regions: {list(env.regions.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reward fields with different decay types\n",
    "reward_constant = region_reward_field(env, \"goal\", reward_value=10.0, decay=\"constant\")\n",
    "\n",
    "reward_linear = region_reward_field(env, \"goal\", reward_value=10.0, decay=\"linear\")\n",
    "\n",
    "reward_gaussian = region_reward_field(\n",
    "    env, \"goal\", reward_value=10.0, decay=\"gaussian\", bandwidth=12.0\n",
    ")\n",
    "\n",
    "print(\"Region reward fields generated:\")\n",
    "print(\n",
    "    f\"  Constant: max={np.max(reward_constant):.2f}, non-zero bins={np.sum(reward_constant > 0)}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Linear: max={np.max(reward_linear):.2f}, non-zero bins={np.sum(reward_linear > 0)}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Gaussian: max={np.max(reward_gaussian):.2f}, non-zero bins={np.sum(reward_gaussian > 0)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize region-based rewards\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "env.plot(reward_constant, ax=axes[0], cmap=\"viridis\", vmin=0, vmax=10)\n",
    "axes[0].set_title(\"Constant (Sparse RL)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].plot(\n",
    "    goal_location[0],\n",
    "    goal_location[1],\n",
    "    \"r*\",\n",
    "    markersize=15,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=1.5,\n",
    "    label=\"Goal\",\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "env.plot(reward_linear, ax=axes[1], cmap=\"viridis\", vmin=0, vmax=10)\n",
    "axes[1].set_title(\"Linear Decay\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].plot(\n",
    "    goal_location[0],\n",
    "    goal_location[1],\n",
    "    \"r*\",\n",
    "    markersize=15,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=1.5,\n",
    "    label=\"Goal\",\n",
    ")\n",
    "axes[1].legend()\n",
    "\n",
    "env.plot(reward_gaussian, ax=axes[2], cmap=\"viridis\", vmin=0, vmax=10)\n",
    "axes[2].set_title(\"Gaussian Falloff (bandwidth=12 cm)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].plot(\n",
    "    goal_location[0],\n",
    "    goal_location[1],\n",
    "    \"r*\",\n",
    "    markersize=15,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=1.5,\n",
    "    label=\"Goal\",\n",
    ")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21",
   "metadata": {},
   "source": [
    "**Key Differences:**\n",
    "\n",
    "- **Constant**: Binary reward (10 inside goal, 0 elsewhere). No gradient guidance.\n",
    "- **Linear**: Gradual decay from boundary. Provides gradient but normalizes by global max distance.\n",
    "- **Gaussian**: Smooth falloff controlled by bandwidth. Best for gradient-based RL but most likely to bias policies.\n",
    "\n",
    "**Important:** The Gaussian version rescales by the max **within the region** to preserve the intended reward magnitude (10.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22",
   "metadata": {},
   "source": [
    "### Goal-Based Distance Rewards\n",
    "\n",
    "Now let's explore distance-based rewards from a goal bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select goal bin\n",
    "goal_bin = env.bin_at(np.array([goal_location]))[0]\n",
    "print(f\"Goal bin index: {goal_bin}\")\n",
    "\n",
    "# Generate distance-based rewards\n",
    "reward_exponential = goal_reward_field(env, goal_bin, decay=\"exponential\", scale=15.0)\n",
    "\n",
    "reward_linear_cutoff = goal_reward_field(\n",
    "    env, goal_bin, decay=\"linear\", scale=10.0, max_distance=50.0\n",
    ")\n",
    "\n",
    "reward_inverse = goal_reward_field(env, goal_bin, decay=\"inverse\", scale=10.0)\n",
    "\n",
    "print(\"\\nGoal-based reward fields generated:\")\n",
    "print(\n",
    "    f\"  Exponential: max={np.max(reward_exponential):.2f}, min={np.min(reward_exponential):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Linear: max={np.max(reward_linear_cutoff):.2f}, min={np.min(reward_linear_cutoff):.4f}\"\n",
    ")\n",
    "print(f\"  Inverse: max={np.max(reward_inverse):.2f}, min={np.min(reward_inverse):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize goal-based rewards\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "env.plot(reward_exponential, ax=axes[0], cmap=\"plasma\")\n",
    "axes[0].set_title(\"Exponential Decay (scale=15)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].plot(\n",
    "    goal_location[0],\n",
    "    goal_location[1],\n",
    "    \"r*\",\n",
    "    markersize=15,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=1.5,\n",
    "    label=\"Goal\",\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "env.plot(reward_linear_cutoff, ax=axes[1], cmap=\"plasma\")\n",
    "axes[1].set_title(\"Linear Decay (cutoff=50 cm)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].plot(\n",
    "    goal_location[0],\n",
    "    goal_location[1],\n",
    "    \"r*\",\n",
    "    markersize=15,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=1.5,\n",
    "    label=\"Goal\",\n",
    ")\n",
    "axes[1].legend()\n",
    "\n",
    "env.plot(reward_inverse, ax=axes[2], cmap=\"plasma\")\n",
    "axes[2].set_title(\"Inverse Distance\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].plot(\n",
    "    goal_location[0],\n",
    "    goal_location[1],\n",
    "    \"r*\",\n",
    "    markersize=15,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=1.5,\n",
    "    label=\"Goal\",\n",
    ")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25",
   "metadata": {},
   "source": [
    "**Comparison:**\n",
    "\n",
    "- **Exponential** (most common): Smooth decay, `reward = scale * exp(-d/scale)`. At distance=scale, reward ≈ 0.37*scale.\n",
    "- **Linear**: Reaches exactly zero at cutoff distance. Constant gradient within range.\n",
    "- **Inverse**: Never reaches zero, provides global gradients. Rarely used (can bias policies).\n",
    "\n",
    "**Recommendation:** Use exponential decay as default. It's well-studied and most commonly used in RL literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26",
   "metadata": {},
   "source": [
    "### Multiple Goals\n",
    "\n",
    "The goal-based rewards support multiple goal locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two goals (top-right and bottom-left)\n",
    "goal_loc_1 = np.array([25.0, 25.0])\n",
    "goal_loc_2 = np.array([-25.0, -25.0])\n",
    "\n",
    "goal_bin_1 = env.bin_at(np.array([goal_loc_1]))[0]\n",
    "goal_bin_2 = env.bin_at(np.array([goal_loc_2]))[0]\n",
    "\n",
    "# Create multi-goal reward field\n",
    "multi_goal_reward = goal_reward_field(\n",
    "    env, goal_bins=[goal_bin_1, goal_bin_2], decay=\"exponential\", scale=15.0\n",
    ")\n",
    "\n",
    "print(\"Multi-goal reward field created\")\n",
    "print(f\"Goal 1 bin: {goal_bin_1}, Goal 2 bin: {goal_bin_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-goal reward\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "env.plot(multi_goal_reward, ax=ax, cmap=\"plasma\")\n",
    "ax.set_title(\"Multi-Goal Reward Field (Exponential)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.plot(\n",
    "    goal_loc_1[0],\n",
    "    goal_loc_1[1],\n",
    "    \"r*\",\n",
    "    markersize=20,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=2,\n",
    "    label=\"Goal 1\",\n",
    ")\n",
    "ax.plot(\n",
    "    goal_loc_2[0],\n",
    "    goal_loc_2[1],\n",
    "    \"c*\",\n",
    "    markersize=20,\n",
    "    markeredgecolor=\"white\",\n",
    "    markeredgewidth=2,\n",
    "    label=\"Goal 2\",\n",
    ")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29",
   "metadata": {},
   "source": [
    "**Multi-goal behavior:** Each bin is influenced by its **nearest** goal, creating a Voronoi-like partition. Useful for multi-goal tasks or hierarchical RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Reward Shaping Best Practices\n",
    "\n",
    "### Caution: When Shaping Can Hurt\n",
    "\n",
    "From Ng et al. (1999):\n",
    "\n",
    "> \"Poorly designed reward shaping can cause agents to learn suboptimal policies that are difficult to correct.\"\n",
    "\n",
    "**Key recommendations:**\n",
    "\n",
    "1. **Start sparse**: Begin with constant (binary) rewards\n",
    "2. **Add shaping cautiously**: Only if learning is too slow\n",
    "3. **Validate policies**: Always compare shaped policy against sparse baseline\n",
    "4. **Prefer exponential**: Well-studied, most commonly used in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31",
   "metadata": {},
   "source": [
    "### Combining Reward Sources\n",
    "\n",
    "You can combine multiple reward fields for complex tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary goal (high reward)\n",
    "primary_reward = region_reward_field(env, \"goal\", reward_value=100.0, decay=\"constant\")\n",
    "\n",
    "# Distance-based shaping (low weight)\n",
    "shaping_reward = goal_reward_field(env, goal_bin, decay=\"exponential\", scale=10.0)\n",
    "\n",
    "# Combined reward (weight shaping less than primary)\n",
    "combined_reward = primary_reward + 0.1 * shaping_reward\n",
    "\n",
    "print(f\"Primary reward max: {np.max(primary_reward):.1f}\")\n",
    "print(f\"Shaping reward max: {np.max(shaping_reward):.1f}\")\n",
    "print(f\"Combined reward max: {np.max(combined_reward):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize combined rewards\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "env.plot(primary_reward, ax=axes[0], cmap=\"viridis\")\n",
    "axes[0].set_title(\"Primary (Sparse)\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "env.plot(shaping_reward * 0.1, ax=axes[1], cmap=\"viridis\")\n",
    "axes[1].set_title(\"Shaping (Weighted 0.1x)\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "env.plot(combined_reward, ax=axes[2], cmap=\"viridis\")\n",
    "axes[2].set_title(\"Combined Reward\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(\n",
    "        goal_location[0],\n",
    "        goal_location[1],\n",
    "        \"r*\",\n",
    "        markersize=15,\n",
    "        markeredgecolor=\"white\",\n",
    "        markeredgewidth=1.5,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34",
   "metadata": {},
   "source": [
    "**Strategy:** Keep the sparse reward dominant (100) and add a small shaping component (0.1 * 10 = 1). This provides gradient guidance while maintaining the correct optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "### Spike Field Conversion\n",
    "- ✅ Convert spike trains to firing rate fields using `spikes_to_field()`\n",
    "- ✅ Apply occupancy thresholds (`min_occupancy_seconds=0.5` is standard)\n",
    "- ✅ Use `compute_place_field()` for one-liner workflows with smoothing\n",
    "- ✅ Visualize occupancy, raw, and filtered firing rates\n",
    "\n",
    "### Reward Field Generation\n",
    "- ✅ Create region-based rewards (constant, linear, gaussian)\n",
    "- ✅ Create goal-based distance rewards (exponential, linear, inverse)\n",
    "- ✅ Handle multiple goals (Voronoi partitioning)\n",
    "- ✅ Combine reward sources for complex tasks\n",
    "\n",
    "### Best Practices\n",
    "- ✅ Always validate shaped policies against sparse baselines\n",
    "- ✅ Prefer exponential decay (well-studied, most common)\n",
    "- ✅ Weight shaping components appropriately (keep sparse dominant)\n",
    "- ✅ Be cautious: poorly designed shaping can bias policies!\n",
    "\n",
    "### Next Steps\n",
    "- Explore other layout engines (hexagonal, graph-based)\n",
    "- Learn about differential operators for computing reward gradients\n",
    "- Apply these techniques to real neural data\n",
    "- Implement full RL algorithms using these reward primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. **O'Keefe, J., & Dostrovsky, J. (1971)**. \"The hippocampus as a spatial map.\" *Brain Research*.\n",
    "\n",
    "2. **Muller, R. U., Kubie, J. L., & Ranck, J. B. (1987)**. \"Spatial firing patterns of hippocampal complex-spike cells in a fixed environment.\" *Journal of Neuroscience*.\n",
    "\n",
    "3. **Ng, A. Y., Harada, D., & Russell, S. (1999)**. \"Policy invariance under reward transformations: Theory and application to reward shaping.\" *ICML*.\n",
    "\n",
    "4. **Sutton, R. S., & Barto, A. G. (2018)**. *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
