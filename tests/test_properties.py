"""
Property-based tests using Hypothesis to verify mathematical invariants.

These tests use randomized inputs to verify that key mathematical properties
hold across a wide range of scenarios. Property-based testing is particularly
valuable for scientific code where mathematical constraints must be maintained.

Categories of properties tested:
1. Environment mathematical invariants (bin centers, connectivity, normalization)
2. Transform mathematical properties (rotation composition, isometry, inverse)
3. Neuroscience metrics (sparsity, selectivity, skaggs_information, coherence)
4. Circular statistics (wrap_angle, circular_mean, mean_resultant_length)
5. Egocentric transforms (allocentric<->egocentric roundtrips, bearing properties)
6. Smoothing operations (non-negativity, mass preservation)
7. Place field detection (valid indices, uniform rate behavior)

Performance Notes:
- Tests use Hypothesis profiles defined in conftest.py for consistent settings
- Set HYPOTHESIS_PROFILE=ci for faster CI runs (fewer examples)
- Set HYPOTHESIS_PROFILE=thorough for comprehensive pre-release testing
- Default "dev" profile balances speed and coverage
"""

import networkx as nx
import numpy as np
import pytest
from hypothesis import given, settings
from hypothesis import strategies as st
from hypothesis.extra import numpy as hnp
from numpy.typing import NDArray

from neurospatial import Environment
from neurospatial.encoding.place import (
    rate_map_coherence,
    selectivity,
    skaggs_information,
    sparsity,
)
from neurospatial.ops import normalize_field
from neurospatial.ops.alignment import get_2d_rotation_matrix
from neurospatial.ops.transforms import AffineND, from_rotation_matrix
from neurospatial.stats.circular import (
    circular_mean,
    circular_variance,
    mean_resultant_length,
    wrap_angle,
)


def rotate_2d(angle_radians: float) -> AffineND:
    """
    Helper to create a 2D rotation transform from angle in radians.

    Parameters
    ----------
    angle_radians : float
        Rotation angle in radians

    Returns
    -------
    Affine2D
        2D rotation transformation
    """
    angle_degrees = np.degrees(angle_radians)
    rot_matrix = get_2d_rotation_matrix(angle_degrees)
    return from_rotation_matrix(rot_matrix)


@pytest.mark.slow
class TestEnvironmentProperties:
    """Property-based tests for Environment mathematical invariants."""

    @given(
        data=hnp.arrays(
            dtype=np.float64,
            shape=hnp.array_shapes(min_dims=2, max_dims=2, min_side=100, max_side=500),
            elements=st.floats(
                min_value=-1000.0,
                max_value=1000.0,
                allow_nan=False,
                allow_infinity=False,
            ),
        ),
        bin_size=st.floats(min_value=1.0, max_value=100.0),
    )
    @settings(
        deadline=10000
    )  # Use profile's max_examples, extend deadline for expensive test
    def test_bin_centers_within_data_range(
        self, data: NDArray[np.float64], bin_size: float
    ):
        """
        Property: Bin centers should always be within the input data bounds.

        This is a fundamental spatial discretization invariant - the bins we create
        should cover the data, and bin centers should fall within the data extent.

        Parameters
        ----------
        data : NDArray, shape (n_samples, 2)
            Random 2D point data generated by hypothesis
        bin_size : float
            Random bin size in range [1.0, 100.0]
        """
        # Ensure we have 2D data as required by from_samples
        if data.shape[1] != 2:
            data = data[:, :2]

        # Skip if data is too small or degenerate
        data_range = np.ptp(data, axis=0)
        if np.any(data_range < bin_size) or np.any(data_range < 1e-6):
            pytest.skip("Data range too small for given bin size")

        try:
            env = Environment.from_samples(
                data, bin_size=bin_size, bin_count_threshold=1
            )
        except (ValueError, RuntimeError) as e:
            # Valid to fail if no active bins created
            if "No active bins" in str(e) or "active_mask is all False" in str(e):
                pytest.skip("No active bins created - expected for some random data")
            raise

        # Property: All bin centers should be within data bounds
        # Allow small tolerance (half bin_size) for edge effects
        data_min = np.min(data, axis=0)
        data_max = np.max(data, axis=0)
        tolerance = bin_size / 2.0

        for bin_center in env.bin_centers:
            assert np.all(bin_center >= data_min - tolerance), (
                f"Bin center {bin_center} below data min {data_min}"
            )
            assert np.all(bin_center <= data_max + tolerance), (
                f"Bin center {bin_center} above data max {data_max}"
            )

    @given(
        angle1=st.floats(min_value=-2 * np.pi, max_value=2 * np.pi),
        angle2=st.floats(min_value=-2 * np.pi, max_value=2 * np.pi),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_rotation_composition_property(self, angle1: float, angle2: float):
        """
        Property: Composing two rotations should equal a single rotation by the sum.

        Mathematically: R(θ₁) ∘ R(θ₂) = R(θ₁ + θ₂)

        This tests the fundamental group property of rotations and verifies
        our transform composition is mathematically sound.

        Parameters
        ----------
        angle1 : float
            First rotation angle in radians
        angle2 : float
            Second rotation angle in radians
        """
        # Create two rotation transforms
        T1 = rotate_2d(angle1)
        T2 = rotate_2d(angle2)

        # Compose them using @ operator
        T_composed = T1 @ T2

        # Expected: single rotation by sum of angles
        T_expected = rotate_2d(angle1 + angle2)

        # Property: Composed transform should match expected transform
        # Compare the transformation matrices (allowing numerical tolerance)
        np.testing.assert_allclose(
            T_composed.A,
            T_expected.A,
            rtol=1e-10,
            atol=1e-10,
            err_msg=f"Rotation composition failed for angles {angle1}, {angle2}",
        )

    @given(
        n_nodes=st.integers(min_value=5, max_value=30),
        seed=st.integers(min_value=0, max_value=10000),
    )
    @settings(
        deadline=10000
    )  # Use profile's max_examples, extend deadline for graph operations
    def test_distance_triangle_inequality(self, n_nodes: int, seed: int):
        """
        Property: Graph distances must satisfy triangle inequality.

        For any three nodes i, j, k: d(i,k) ≤ d(i,j) + d(j,k)

        This is a fundamental property of metric spaces and must hold for
        all shortest path distances in our connectivity graphs.

        Parameters
        ----------
        n_nodes : int
            Number of nodes in the graph
        seed : int
            Random seed for reproducibility
        """
        rng = np.random.default_rng(seed)

        # Create random 2D grid environment
        data = rng.uniform(-50, 50, size=(n_nodes * 5, 2))

        try:
            env = Environment.from_samples(
                data,
                bin_size=10.0,
                bin_count_threshold=1,
                connect_diagonal_neighbors=True,
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        if env.n_bins < 5:
            pytest.skip("Not enough bins for meaningful triangle inequality test")

        # Verify graph is connected (otherwise distances might be infinite)
        if not nx.is_connected(env.connectivity):
            pytest.skip("Graph is not connected")

        # Sample 3 random nodes to test triangle inequality
        # Use at least 3 trials to test different node combinations
        for _ in range(min(3, env.n_bins // 2)):
            # Sample 3 distinct nodes
            node_indices = rng.choice(env.n_bins, size=3, replace=False)
            i, j, k = node_indices

            # Get pairwise distances
            try:
                d_ij = nx.shortest_path_length(
                    env.connectivity, source=i, target=j, weight="distance"
                )
                d_jk = nx.shortest_path_length(
                    env.connectivity, source=j, target=k, weight="distance"
                )
                d_ik = nx.shortest_path_length(
                    env.connectivity, source=i, target=k, weight="distance"
                )
            except nx.NetworkXNoPath:
                # Nodes not connected - skip this combination
                continue

            # Property: Triangle inequality must hold
            # Allow small numerical tolerance for floating point arithmetic
            tolerance = 1e-10
            assert d_ik <= d_ij + d_jk + tolerance, (
                f"Triangle inequality violated: d({i},{k})={d_ik} > d({i},{j})+d({j},{k})={d_ij + d_jk}"
            )

    @given(
        n_bins=st.integers(min_value=20, max_value=100),
        seed=st.integers(min_value=0, max_value=10000),
    )
    @settings(deadline=10000)  # Use profile's max_examples
    def test_connectivity_graph_is_undirected(self, n_bins: int, seed: int):
        """
        Property: Connectivity graph must be undirected (symmetric edges).

        For spatial environments, if bin A is a neighbor of bin B,
        then bin B must be a neighbor of bin A. This is a fundamental
        property of spatial adjacency.

        Mathematically: (i, j) ∈ E ⟺ (j, i) ∈ E

        Parameters
        ----------
        n_bins : int
            Target number of bins to generate
        seed : int
            Random seed for reproducibility
        """
        rng = np.random.default_rng(seed)

        # Create environment with known number of bins
        n_samples = n_bins * 10
        data = rng.uniform(-50, 50, size=(n_samples, 2))

        try:
            env = Environment.from_samples(
                data,
                bin_size=5.0,
                bin_count_threshold=1,
                connect_diagonal_neighbors=True,
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        if env.n_bins < 5:
            pytest.skip("Not enough bins for meaningful symmetry test")

        # Property: Graph must be undirected
        # For every edge (i, j), there must exist edge (j, i)
        for edge in env.connectivity.edges():
            i, j = edge
            # Check reverse edge exists
            assert env.connectivity.has_edge(j, i), (
                f"Graph not undirected: edge ({i},{j}) exists but ({j},{i}) doesn't"
            )

            # Check edge attributes match (distance, etc.)
            dist_ij = env.connectivity.edges[i, j]["distance"]
            dist_ji = env.connectivity.edges[j, i]["distance"]
            np.testing.assert_allclose(
                dist_ij,
                dist_ji,
                rtol=1e-10,
                atol=1e-10,
                err_msg=f"Edge distances not symmetric: d({i},{j})={dist_ij} ≠ d({j},{i})={dist_ji}",
            )

    @given(
        field_size=st.integers(min_value=10, max_value=100),
        seed=st.integers(min_value=0, max_value=10000),
    )
    @settings(deadline=5000)  # Use profile's max_examples
    def test_normalized_field_sums_to_one(self, field_size: int, seed: int):
        """
        Property: Normalized spatial fields should sum to 1 (probability mass).

        When a spatial field is normalized, it represents a probability distribution
        over space. The total probability must sum to 1.

        This tests both the normalize_field method and the underlying mathematical
        correctness of probability normalization.

        Parameters
        ----------
        field_size : int
            Number of bins in the environment
        seed : int
            Random seed for reproducibility
        """
        rng = np.random.default_rng(seed)

        # Create environment with known number of bins
        n_samples = field_size * 5
        data = rng.uniform(-50, 50, size=(n_samples, 2))

        try:
            env = Environment.from_samples(
                data,
                bin_size=10.0,
                bin_count_threshold=1,
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        if env.n_bins < 5:
            pytest.skip("Not enough bins for meaningful normalization test")

        # Generate random non-negative field (like a firing rate map)
        # Use exponential to ensure non-negative values
        field = rng.exponential(scale=2.0, size=env.n_bins)

        # Normalize the field
        try:
            normalized = normalize_field(field)
        except (ValueError, ZeroDivisionError):
            # Valid to fail if field is all zeros
            if np.all(field == 0):
                pytest.skip("All-zero field cannot be normalized")
            raise

        # Property: Normalized field should sum to 1 (probability mass)
        field_sum = np.sum(normalized)
        np.testing.assert_allclose(
            field_sum,
            1.0,
            rtol=1e-10,
            atol=1e-10,
            err_msg=f"Normalized field sums to {field_sum}, expected 1.0",
        )

        # Additional property: All values should be non-negative (valid probability)
        assert np.all(normalized >= 0), "Normalized field has negative values"


class TestTransformProperties:
    """Property-based tests for transformation mathematical properties."""

    @given(
        angle=st.floats(min_value=-2 * np.pi, max_value=2 * np.pi),
        point_x=st.floats(min_value=-100.0, max_value=100.0),
        point_y=st.floats(min_value=-100.0, max_value=100.0),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_rotation_preserves_distance_from_origin(
        self, angle: float, point_x: float, point_y: float
    ):
        """
        Property: Rotation preserves distance from origin.

        Rotations are isometries - they preserve distances. Specifically,
        rotating a point should not change its distance from the origin.

        Mathematically: ||R(p)|| = ||p|| for any rotation R and point p.

        Parameters
        ----------
        angle : float
            Rotation angle in radians
        point_x, point_y : float
            2D point coordinates
        """
        point = np.array([point_x, point_y])

        # Distance before rotation
        dist_before = np.linalg.norm(point)

        # Apply rotation using __call__ (Affine2D is callable)
        T = rotate_2d(angle)
        rotated_point = T(point.reshape(1, 2))[0]

        # Distance after rotation
        dist_after = np.linalg.norm(rotated_point)

        # Property: Distance should be preserved (isometry)
        np.testing.assert_allclose(
            dist_after,
            dist_before,
            rtol=1e-10,
            atol=1e-10,
            err_msg=f"Rotation changed distance: {dist_before} -> {dist_after}",
        )

    @given(
        angle=st.floats(min_value=-2 * np.pi, max_value=2 * np.pi),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_rotation_inverse_property(self, angle: float):
        """
        Property: Applying a rotation and its inverse should return to identity.

        Mathematically: R(θ) ∘ R(-θ) = I (identity)

        This tests that our rotation composition and inversion are correct.

        Parameters
        ----------
        angle : float
            Rotation angle in radians
        """
        # Create rotation and its inverse
        T = rotate_2d(angle)
        T_inv = rotate_2d(-angle)

        # Compose them using @ operator
        T_composed = T @ T_inv

        # Expected: Identity transformation (identity has matrix [[1,0,0],[0,1,0],[0,0,1]])
        identity_matrix = np.eye(3)

        # Property: Composition should equal identity
        np.testing.assert_allclose(
            T_composed.A,
            identity_matrix,
            rtol=1e-10,
            atol=1e-10,
            err_msg=f"R({angle}) ∘ R({-angle}) ≠ I",
        )


# =============================================================================
# Hypothesis Strategies for Neuroscience Metrics
# =============================================================================


@st.composite
def valid_firing_rate_and_occupancy(
    draw: st.DrawFn,
    min_bins: int = 10,
    max_bins: int = 100,
) -> tuple[NDArray[np.float64], NDArray[np.float64]]:
    """Generate matching firing rate and occupancy arrays.

    Parameters
    ----------
    draw : st.DrawFn
        Hypothesis draw function.
    min_bins : int, default=10
        Minimum number of bins.
    max_bins : int, default=100
        Maximum number of bins.

    Returns
    -------
    tuple[NDArray[np.float64], NDArray[np.float64]]
        (firing_rate, occupancy) with same shape and valid values.
    """
    n_bins = draw(st.integers(min_value=min_bins, max_value=max_bins))

    # Generate firing rate (non-negative)
    firing_rate = draw(
        hnp.arrays(
            dtype=np.float64,
            shape=n_bins,
            elements=st.floats(
                min_value=0.0,
                max_value=100.0,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )

    # Generate occupancy (positive, will be normalized)
    occupancy = draw(
        hnp.arrays(
            dtype=np.float64,
            shape=n_bins,
            elements=st.floats(
                min_value=0.01,
                max_value=100.0,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )

    # Sanity check: arrays must have matching shapes
    # This is guaranteed by using n_bins for both, but defensive programming
    assert firing_rate.shape == occupancy.shape == (n_bins,), (
        f"Shape mismatch: firing_rate {firing_rate.shape}, "
        f"occupancy {occupancy.shape}, expected ({n_bins},)"
    )

    return firing_rate, occupancy


# =============================================================================
# Property Tests for Neuroscience Metrics
# =============================================================================


class TestSparsityProperties:
    """Property-based tests for sparsity metric.

    Sparsity measures what fraction of the environment elicits significant firing.
    Formula: sparsity = (sum p_i * r_i)^2 / (sum p_i * r_i^2)

    Mathematical properties:
    - Range: [0, 1]
    - Uniform firing → sparsity ≈ 1.0
    - Single-bin firing → sparsity ≈ 1/n_bins
    """

    @given(valid_firing_rate_and_occupancy())
    @settings(deadline=None)  # Use profile's max_examples
    def test_sparsity_range_property(
        self, data: tuple[NDArray[np.float64], NDArray[np.float64]]
    ):
        """Property: sparsity is always in [0, 1] for valid inputs."""
        firing_rate, occupancy = data

        sp = sparsity(firing_rate, occupancy)

        # Property: 0 <= sparsity <= 1 (strict, guaranteed by implementation)
        assert 0.0 <= sp <= 1.0, f"Sparsity {sp} outside valid range [0, 1]"

    @given(
        valid_firing_rate_and_occupancy(min_bins=10, max_bins=100),
        st.floats(min_value=0.1, max_value=100.0),
    )
    @settings(deadline=None)  # Use profile's max_examples
    def test_uniform_firing_high_sparsity(
        self,
        data: tuple[NDArray[np.float64], NDArray[np.float64]],
        uniform_rate: float,
    ):
        """Property: uniform firing produces high sparsity (close to 1.0)."""
        _, occupancy = data
        n_bins = len(occupancy)

        # Create uniform firing at the given rate
        firing_rate = np.ones(n_bins, dtype=np.float64) * uniform_rate

        sp = sparsity(firing_rate, occupancy)

        # Property: uniform firing → sparsity very close to 1.0
        # Allow small numerical tolerance
        assert sp >= 0.95, f"Uniform firing sparsity {sp} should be close to 1.0"

    @given(valid_firing_rate_and_occupancy(min_bins=20, max_bins=100))
    @settings(deadline=None)  # Use profile's max_examples
    def test_single_peak_low_sparsity(
        self, data: tuple[NDArray[np.float64], NDArray[np.float64]]
    ):
        """Property: firing in single bin produces sparsity equal to occupancy fraction.

        For single-bin firing, sparsity = p_k where p_k is the occupancy probability
        in the firing bin. Sparsity is "low" only when occupancy in that bin is low.
        """
        _, occupancy = data
        n_bins = len(occupancy)

        # Create firing only in one bin
        firing_rate = np.zeros(n_bins, dtype=np.float64)
        peak_idx = n_bins // 2
        firing_rate[peak_idx] = 100.0

        sp = sparsity(firing_rate, occupancy)

        # Property: single-bin firing → sparsity ≈ occupancy fraction in that bin
        # (not always "low" - depends on occupancy distribution)
        expected_sparsity = occupancy[peak_idx] / occupancy.sum()
        assert np.isclose(sp, expected_sparsity, rtol=0.01), (
            f"Single-peak sparsity {sp} should equal occupancy fraction {expected_sparsity}"
        )


class TestSelectivityProperties:
    """Property-based tests for selectivity metric.

    Selectivity measures how spatially selective firing is.
    Formula: selectivity = peak_rate / mean_rate

    Mathematical properties:
    - Range: [1.0, ∞) (always >= 1.0)
    - Uniform firing → selectivity = 1.0
    - Sparse firing → selectivity >> 1.0
    """

    @given(valid_firing_rate_and_occupancy())
    @settings(deadline=None)  # Use profile's max_examples
    def test_selectivity_minimum_value(
        self, data: tuple[NDArray[np.float64], NDArray[np.float64]]
    ):
        """Property: selectivity is always >= 1.0."""
        firing_rate, occupancy = data

        # Skip all-zero firing rate (undefined selectivity)
        if np.all(firing_rate == 0):
            pytest.skip("All-zero firing rate")

        sel = selectivity(firing_rate, occupancy)

        # Property: selectivity >= 1.0 always
        # (peak rate is always >= mean rate by definition)
        if not np.isnan(sel) and not np.isinf(sel):
            assert sel >= 1.0, f"Selectivity {sel} should be >= 1.0"

    @given(
        valid_firing_rate_and_occupancy(min_bins=10, max_bins=100),
        st.floats(min_value=0.1, max_value=100.0),
    )
    @settings(deadline=None)  # Use profile's max_examples
    def test_uniform_firing_selectivity_one(
        self,
        data: tuple[NDArray[np.float64], NDArray[np.float64]],
        uniform_rate: float,
    ):
        """Property: uniform firing produces selectivity = 1.0."""
        _, occupancy = data
        n_bins = len(occupancy)

        # Create uniform firing
        firing_rate = np.ones(n_bins, dtype=np.float64) * uniform_rate

        sel = selectivity(firing_rate, occupancy)

        # Property: uniform firing → selectivity = 1.0
        # Allow small numerical tolerance
        assert np.abs(sel - 1.0) < 0.01, (
            f"Uniform firing selectivity {sel} should be 1.0"
        )

    @given(valid_firing_rate_and_occupancy(min_bins=20, max_bins=100))
    @settings(deadline=None)  # Use profile's max_examples
    def test_sparse_firing_high_selectivity(
        self, data: tuple[NDArray[np.float64], NDArray[np.float64]]
    ):
        """Property: firing in few bins produces high selectivity."""
        _, occupancy = data
        n_bins = len(occupancy)

        # Create firing in only 10% of bins
        firing_rate = np.zeros(n_bins, dtype=np.float64)
        n_active = max(2, n_bins // 10)  # At least 2 bins
        firing_rate[:n_active] = 100.0

        sel = selectivity(firing_rate, occupancy)

        # Property: sparse firing → selectivity > 1.0 (higher than uniform)
        # Note: Exact value depends on occupancy distribution
        # Hypothesis found cases where uneven occupancy reduces selectivity
        assert sel > 1.0, f"Sparse firing selectivity {sel} should be > 1.0"


class TestSkaggsInformationProperties:
    """Property-based tests for Skaggs spatial information metric.

    Skaggs information quantifies how much each spike conveys about location.
    Formula: I = sum p_i * (r_i / mean_r) * log2(r_i / mean_r)

    Mathematical properties:
    - Range: [0, ∞) (always non-negative)
    - Uniform firing → information ≈ 0.0
    - Selective firing → information > 0
    """

    @given(valid_firing_rate_and_occupancy())
    @settings(deadline=None)  # Use profile's max_examples
    def test_information_non_negative(
        self, data: tuple[NDArray[np.float64], NDArray[np.float64]]
    ):
        """Property: Skaggs information is always non-negative."""
        firing_rate, occupancy = data

        info = skaggs_information(firing_rate, occupancy)

        # Property: information >= 0.0 always (strict, guaranteed by implementation)
        assert info >= 0.0, f"Information {info} should be >= 0.0"

    @given(
        valid_firing_rate_and_occupancy(min_bins=10, max_bins=100),
        st.floats(min_value=0.1, max_value=100.0),
    )
    @settings(deadline=None)  # Use profile's max_examples
    def test_uniform_firing_zero_information(
        self,
        data: tuple[NDArray[np.float64], NDArray[np.float64]],
        uniform_rate: float,
    ):
        """Property: uniform firing produces zero spatial information."""
        _, occupancy = data
        n_bins = len(occupancy)

        # Create uniform firing
        firing_rate = np.ones(n_bins, dtype=np.float64) * uniform_rate

        info = skaggs_information(firing_rate, occupancy)

        # Property: uniform firing → information ≈ 0.0
        assert info < 0.01, f"Uniform firing information {info} should be close to 0.0"

    @given(valid_firing_rate_and_occupancy(min_bins=20, max_bins=100))
    @settings(deadline=None)  # Use profile's max_examples
    def test_selective_firing_positive_information(
        self, data: tuple[NDArray[np.float64], NDArray[np.float64]]
    ):
        """Property: selective firing produces positive information."""
        _, occupancy = data
        n_bins = len(occupancy)

        # Create selective firing (Gaussian-like)
        firing_rate = np.zeros(n_bins, dtype=np.float64)
        peak_idx = n_bins // 2
        # Add firing in center bin and neighbors
        firing_rate[max(0, peak_idx - 2) : min(n_bins, peak_idx + 3)] = 10.0
        firing_rate[peak_idx] = 50.0  # Peak

        info = skaggs_information(firing_rate, occupancy)

        # Property: selective firing → information > 0
        # Note: For highly non-uniform occupancy (which Hypothesis can generate),
        # the information can be very small but should be positive
        assert info > 0.0, (
            f"Selective firing information {info} should be positive (> 0)"
        )


class TestRateMapCoherenceProperties:
    """Property-based tests for rate map coherence metric.

    Coherence measures spatial smoothness of firing.
    Formula: correlation between each bin and mean of its neighbors

    Mathematical properties:
    - Range: [-1, 1] (correlation coefficient)
    - Smooth fields → high coherence (> 0.5)
    - Random noise → low coherence (~ 0)
    """

    @given(
        st.integers(min_value=50, max_value=200),
        st.integers(min_value=0, max_value=10000),
    )
    @settings(deadline=None)  # Use profile's max_examples
    def test_coherence_range_property(self, n_positions: int, seed: int):
        """Property: coherence is in [-1, 1] for all valid inputs."""
        # Create simple environment
        rng = np.random.default_rng(seed)
        positions = rng.standard_normal((n_positions, 2)) * 10

        try:
            env = Environment.from_samples(
                positions, bin_size=2.0, connect_diagonal_neighbors=False
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        # Create random firing rate
        firing_rate = rng.uniform(0, 10, size=env.n_bins)

        coherence = rate_map_coherence(firing_rate, env)

        # Property: -1 <= coherence <= 1 (correlation coefficient range)
        if not np.isnan(coherence):
            assert -1.0 <= coherence <= 1.0, (
                f"Coherence {coherence} outside valid range [-1, 1]"
            )

    @given(
        st.integers(min_value=100, max_value=300),
        st.integers(min_value=0, max_value=10000),
    )
    @settings(deadline=None)  # Use profile's max_examples
    def test_smooth_field_high_coherence(self, n_positions: int, seed: int):
        """Property: smooth Gaussian field produces high coherence."""
        # Create environment
        rng = np.random.default_rng(seed)
        positions = rng.standard_normal((n_positions, 2)) * 20

        try:
            env = Environment.from_samples(
                positions, bin_size=2.0, connect_diagonal_neighbors=False
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        if env.n_bins < 10:
            pytest.skip("Not enough bins for coherence test")

        # Create smooth Gaussian field
        firing_rate = np.zeros(env.n_bins, dtype=np.float64)
        for i in range(env.n_bins):
            dist = np.linalg.norm(env.bin_centers[i])
            firing_rate[i] = 10.0 * np.exp(-(dist**2) / (2 * 5.0**2))

        coherence = rate_map_coherence(firing_rate, env)

        # Property: smooth field → high coherence (> 0.4)
        # Threshold is conservative because random environment topology can reduce
        # correlation between bins and neighbors even for smooth Gaussian fields.
        # Empirically observed range for random 2D environments: [0.4, 0.8]
        if not np.isnan(coherence):
            assert coherence > 0.4, (
                f"Smooth field coherence {coherence} should be > 0.4"
            )


class TestCrossMetricProperties:
    """Property-based tests for relationships between metrics.

    Tests mathematical relationships that should hold between different metrics.
    """

    @given(valid_firing_rate_and_occupancy(min_bins=20, max_bins=100))
    @settings(deadline=None)  # Use profile's max_examples
    def test_sparsity_selectivity_inverse_relationship(
        self, data: tuple[NDArray[np.float64], NDArray[np.float64]]
    ):
        """Property: high sparsity (uniform firing) implies low selectivity.

        For uniform-like firing patterns (sparsity > 0.9), selectivity
        should be close to 1.0 since peak rate ≈ mean rate. This tests
        the mathematical relationship between spatial spread (sparsity)
        and peak-to-mean ratio (selectivity).
        """
        firing_rate, occupancy = data

        # Skip all-zero case
        if np.all(firing_rate == 0):
            pytest.skip("All-zero firing rate")

        sp = sparsity(firing_rate, occupancy)
        sel = selectivity(firing_rate, occupancy)

        # For uniform firing (high sparsity ~ 1.0):
        # - sparsity should be high
        # - selectivity should be low (~ 1.0)
        if (
            sp > 0.9 and not np.isnan(sel) and not np.isinf(sel)
        ):  # High sparsity (uniform-like)
            assert sel < 2.0, (
                f"High sparsity ({sp}) should imply low selectivity, got {sel}"
            )

    @given(valid_firing_rate_and_occupancy(min_bins=20, max_bins=100))
    @settings(deadline=None)  # Use profile's max_examples
    def test_uniform_firing_implies_zero_information(
        self, data: tuple[NDArray[np.float64], NDArray[np.float64]]
    ):
        """Property: uniform firing → zero information (not vice versa).

        Note: The reverse is NOT true - zero information doesn't imply
        uniform firing (e.g., one high-firing bin with low occupancy).
        """
        _, occupancy = data
        n_bins = len(occupancy)

        # Create uniform firing
        firing_rate = np.ones(n_bins, dtype=np.float64) * 10.0

        info = skaggs_information(firing_rate, occupancy)

        # Property: uniform firing → information ≈ 0.0
        assert info < 0.01, (
            f"Uniform firing should produce zero information, got {info}"
        )


# =============================================================================
# Property Tests for Smoothing Operations
# =============================================================================


@pytest.mark.slow
class TestSmoothingProperties:
    """Property-based tests for spatial field smoothing operations."""

    @given(
        st.integers(min_value=100, max_value=500),
        st.integers(min_value=0, max_value=10000),
        st.floats(min_value=1.0, max_value=10.0),
    )
    @settings(
        deadline=20000
    )  # Use profile's max_examples, extend deadline for smoothing
    def test_smoothing_preserves_non_negativity(
        self, n_positions: int, seed: int, bandwidth: float
    ):
        """Property: Smoothing non-negative field produces non-negative result."""
        rng = np.random.default_rng(seed)

        # Create simple environment
        positions = rng.standard_normal((n_positions, 2)) * 20

        try:
            env = Environment.from_samples(
                positions, bin_size=2.0, connect_diagonal_neighbors=True
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        if env.n_bins < 10:
            pytest.skip("Not enough bins")

        # Create non-negative field
        field = rng.exponential(scale=5.0, size=env.n_bins)

        # Smooth the field
        smoothed = env.smooth(field, bandwidth=bandwidth)

        # Property: smoothing non-negative field should remain non-negative
        assert np.all(smoothed >= -1e-10), (
            f"Smoothing produced negative values: min={smoothed.min()}"
        )

    @given(
        st.integers(min_value=100, max_value=500),
        st.integers(min_value=0, max_value=10000),
        st.floats(min_value=1.0, max_value=10.0),
    )
    @settings(
        deadline=20000
    )  # Use profile's max_examples, extend deadline for smoothing
    def test_transition_smoothing_preserves_mass(
        self, n_positions: int, seed: int, bandwidth: float
    ):
        """Property: Transition-mode smoothing preserves total mass."""
        rng = np.random.default_rng(seed)

        # Create simple environment
        positions = rng.standard_normal((n_positions, 2)) * 20

        try:
            env = Environment.from_samples(
                positions, bin_size=2.0, connect_diagonal_neighbors=True
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        if env.n_bins < 10:
            pytest.skip("Not enough bins")

        # Create field with known mass
        field = rng.exponential(scale=5.0, size=env.n_bins)
        original_mass = np.sum(field)

        # Smooth with transition mode (row-normalized kernel)
        smoothed = env.smooth(field, bandwidth=bandwidth, mode="transition")

        # Property: transition smoothing should preserve total mass
        smoothed_mass = np.sum(smoothed)
        np.testing.assert_allclose(
            smoothed_mass,
            original_mass,
            rtol=0.01,
            err_msg=f"Transition smoothing changed mass: {original_mass} -> {smoothed_mass}",
        )

    @given(
        st.integers(min_value=100, max_value=500),
        st.integers(min_value=0, max_value=10000),
    )
    @settings(
        deadline=20000
    )  # Use profile's max_examples, extend deadline for smoothing
    def test_large_bandwidth_smooths_towards_mean(self, n_positions: int, seed: int):
        """Property: Large bandwidth smoothing produces values closer to mean."""
        rng = np.random.default_rng(seed)

        # Create simple environment
        positions = rng.standard_normal((n_positions, 2)) * 20

        try:
            env = Environment.from_samples(
                positions, bin_size=2.0, connect_diagonal_neighbors=True
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        if env.n_bins < 10:
            pytest.skip("Not enough bins")

        # Create random field with variation
        field = rng.exponential(scale=5.0, size=env.n_bins)

        # Smooth with large bandwidth
        smoothed = env.smooth(field, bandwidth=50.0, mode="transition")

        # Property: large bandwidth should reduce variance (smooth towards mean)
        original_std = np.std(field)
        smoothed_std = np.std(smoothed)

        # Smoothing should reduce variance
        assert smoothed_std < original_std, (
            f"Large bandwidth should reduce std: {original_std:.3f} -> {smoothed_std:.3f}"
        )


class TestFieldNormalizationProperties:
    """Property-based tests for field normalization."""

    @given(
        st.integers(min_value=10, max_value=100),
        st.integers(min_value=0, max_value=10000),
    )
    @settings(deadline=5000)  # Use profile's max_examples
    def test_normalize_field_sum_one(self, n_bins: int, seed: int):
        """Property: Normalized field sums to 1."""
        rng = np.random.default_rng(seed)

        # Create non-negative field
        field = rng.exponential(scale=5.0, size=n_bins)

        # Skip if all zeros
        if np.all(field == 0):
            pytest.skip("All-zero field")

        normalized = normalize_field(field)

        np.testing.assert_allclose(
            np.sum(normalized),
            1.0,
            rtol=1e-10,
            err_msg="Normalized field should sum to 1",
        )

    @given(
        st.integers(min_value=10, max_value=100),
        st.integers(min_value=0, max_value=10000),
        st.floats(min_value=0.1, max_value=100.0),
    )
    @settings(deadline=5000)  # Use profile's max_examples
    def test_normalize_scaling_invariant(self, n_bins: int, seed: int, scale: float):
        """Property: Normalization is scale-invariant."""
        rng = np.random.default_rng(seed)

        # Create non-negative field
        field = rng.exponential(scale=5.0, size=n_bins)

        # Skip if all zeros
        if np.all(field == 0):
            pytest.skip("All-zero field")

        # Normalize original and scaled field
        normalized = normalize_field(field)
        scaled_normalized = normalize_field(field * scale)

        # Property: normalization should be scale-invariant
        np.testing.assert_allclose(
            scaled_normalized,
            normalized,
            rtol=1e-10,
            err_msg="Normalization should be scale-invariant",
        )


# =============================================================================
# Property Tests for Place Field Detection
# =============================================================================


@pytest.mark.slow
class TestPlaceFieldDetectionProperties:
    """Property-based tests for place field detection invariants."""

    @given(
        st.integers(min_value=100, max_value=300),
        st.integers(min_value=0, max_value=10000),
    )
    @settings(
        deadline=30000
    )  # Use profile's max_examples, extend deadline for detection
    def test_detected_fields_have_valid_indices(self, n_positions: int, seed: int):
        """Property: Detected field bin indices are valid."""
        from neurospatial.encoding.place import detect_place_fields

        rng = np.random.default_rng(seed)

        # Create environment
        positions = rng.standard_normal((n_positions, 2)) * 30

        try:
            env = Environment.from_samples(
                positions, bin_size=3.0, connect_diagonal_neighbors=True
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        if env.n_bins < 20:
            pytest.skip("Not enough bins")

        # Create firing rate with Gaussian peak
        firing_rate = np.zeros(env.n_bins, dtype=np.float64)
        peak_idx = env.n_bins // 2
        for i in range(env.n_bins):
            dist = np.linalg.norm(env.bin_centers[i] - env.bin_centers[peak_idx])
            firing_rate[i] = 10.0 * np.exp(-(dist**2) / (2 * 10.0**2))

        # Detect fields
        try:
            fields = detect_place_fields(firing_rate, env)

            # Property: all field bin indices should be valid
            for field in fields:
                for bin_idx in field:
                    assert 0 <= bin_idx < env.n_bins, (
                        f"Invalid bin index {bin_idx} in detected field"
                    )
        except (ValueError, RuntimeError):
            pass  # Some configurations may not detect fields

    @given(
        st.integers(min_value=100, max_value=300),
        st.integers(min_value=0, max_value=10000),
    )
    @settings(
        deadline=30000
    )  # Use profile's max_examples, extend deadline for detection
    def test_uniform_rate_produces_no_fields(self, n_positions: int, seed: int):
        """Property: Uniform firing rate should produce no detected fields."""
        from neurospatial.encoding.place import detect_place_fields

        rng = np.random.default_rng(seed)

        # Create environment
        positions = rng.standard_normal((n_positions, 2)) * 30

        try:
            env = Environment.from_samples(
                positions, bin_size=3.0, connect_diagonal_neighbors=True
            )
        except (ValueError, RuntimeError):
            pytest.skip("Could not create valid environment")

        if env.n_bins < 20:
            pytest.skip("Not enough bins")

        # Create uniform firing rate
        firing_rate = np.ones(env.n_bins, dtype=np.float64) * 5.0

        # Detect fields
        try:
            fields = detect_place_fields(
                firing_rate,
                env,
                threshold=0.5,  # Threshold for field detection
            )

            # Property: uniform rate should produce either no fields or one big field
            # (depending on threshold relative to the uniform level)
            # Most importantly, it shouldn't crash or produce invalid results
            for field in fields:
                for bin_idx in field:
                    assert 0 <= bin_idx < env.n_bins
        except (ValueError, RuntimeError):
            pass


# =============================================================================
# Property Tests for Circular Statistics
# =============================================================================


class TestWrapAngleProperties:
    """Property-based tests for wrap_angle function.

    wrap_angle maps angles to (-pi, pi] range while preserving
    trigonometric values. Key properties:
    - Output always in (-pi, pi]
    - Idempotent: wrap(wrap(x)) == wrap(x)
    - Preserves sin/cos values
    - Invariant to 2*pi shifts
    """

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1, max_value=100),
            elements=st.floats(
                min_value=-10 * np.pi,
                max_value=10 * np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_output_always_in_range(self, angles: NDArray[np.float64]) -> None:
        """Property: wrap_angle output is always in (-pi, pi]."""
        wrapped = wrap_angle(angles)

        # All values should be in (-pi, pi]
        assert np.all(wrapped > -np.pi), "Some angles <= -pi"
        assert np.all(wrapped <= np.pi), "Some angles > pi"

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1, max_value=100),
            elements=st.floats(
                min_value=-10 * np.pi,
                max_value=10 * np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_idempotent(self, angles: NDArray[np.float64]) -> None:
        """Property: wrapping twice gives same result as wrapping once."""
        wrapped_once = wrap_angle(angles)
        wrapped_twice = wrap_angle(wrapped_once)

        np.testing.assert_allclose(
            wrapped_once,
            wrapped_twice,
            rtol=1e-10,
            err_msg="wrap_angle is not idempotent",
        )

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1, max_value=100),
            elements=st.floats(
                min_value=-10 * np.pi,
                max_value=10 * np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_preserves_trigonometric_values(self, angles: NDArray[np.float64]) -> None:
        """Property: wrapping preserves sin and cos values."""
        wrapped = wrap_angle(angles)

        # Use generous atol for near-zero values where floating-point
        # precision limits matter (e.g., sin(10*pi) ≈ 0 but with fp error)
        np.testing.assert_allclose(
            np.sin(angles),
            np.sin(wrapped),
            rtol=1e-10,
            atol=1e-14,
            err_msg="wrap_angle changed sin values",
        )
        np.testing.assert_allclose(
            np.cos(angles),
            np.cos(wrapped),
            rtol=1e-10,
            atol=1e-14,
            err_msg="wrap_angle changed cos values",
        )

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1, max_value=50),
            elements=st.floats(
                min_value=-5 * np.pi,
                max_value=5 * np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        ),
        st.integers(min_value=-10, max_value=10),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_shift_by_2pi_invariant(self, angles: NDArray[np.float64], k: int) -> None:
        """Property: adding k*2*pi then wrapping gives same result.

        Note: -pi and pi are equivalent angles, so we compare via
        trigonometric functions which handle the discontinuity.
        """
        shifted = angles + k * 2 * np.pi
        wrapped_original = wrap_angle(angles)
        wrapped_shifted = wrap_angle(shifted)

        # Compare via sin/cos to handle -pi vs pi equivalence
        np.testing.assert_allclose(
            np.sin(wrapped_original),
            np.sin(wrapped_shifted),
            rtol=1e-10,
            atol=1e-14,
            err_msg="wrap_angle not invariant to 2*pi shifts (sin)",
        )
        np.testing.assert_allclose(
            np.cos(wrapped_original),
            np.cos(wrapped_shifted),
            rtol=1e-10,
            atol=1e-14,
            err_msg="wrap_angle not invariant to 2*pi shifts (cos)",
        )


class TestCircularMeanProperties:
    """Property-based tests for circular_mean function.

    Circular mean computes the mean direction of angular data.
    Key properties:
    - Output always in [-pi, pi]
    - Identical angles give that angle as mean
    - Rotation equivariance: rotate inputs -> rotate output
    """

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=3, max_value=50),
            elements=st.floats(
                min_value=-10 * np.pi,
                max_value=10 * np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_output_in_valid_range(self, angles: NDArray[np.float64]) -> None:
        """Property: circular mean is in [-pi, pi]."""
        mean = circular_mean(angles)

        assert -np.pi <= mean <= np.pi, f"Mean {mean} outside [-pi, pi]"

    @given(
        st.floats(min_value=-np.pi, max_value=np.pi, allow_nan=False),
        st.integers(min_value=3, max_value=50),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_identical_angles_give_that_angle(self, angle: float, n: int) -> None:
        """Property: circular mean of identical angles is that angle."""
        angles = np.full(n, angle, dtype=np.float64)
        mean = circular_mean(angles)

        np.testing.assert_allclose(
            mean,
            angle,
            rtol=1e-10,
            atol=1e-10,
            err_msg="Mean of identical angles should be that angle",
        )

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=3, max_value=50),
            elements=st.floats(
                min_value=-np.pi,
                max_value=np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_rotation_equivariance(self, angles: NDArray[np.float64]) -> None:
        """Property: rotating all angles rotates the mean by same amount."""
        rotation = np.pi / 4  # Arbitrary rotation
        rotated_angles = angles + rotation

        mean_original = circular_mean(angles)
        mean_rotated = circular_mean(rotated_angles)

        # The means should differ by the rotation (mod 2*pi)
        expected_rotated_mean = wrap_angle(np.array([mean_original + rotation]))[0]
        actual_rotated_mean = wrap_angle(np.array([mean_rotated]))[0]

        np.testing.assert_allclose(
            actual_rotated_mean,
            expected_rotated_mean,
            rtol=1e-10,
            atol=1e-10,
            err_msg="Circular mean not equivariant to rotation",
        )


class TestMeanResultantLengthProperties:
    """Property-based tests for mean_resultant_length function.

    Mean resultant length R measures concentration of circular data.
    Key properties:
    - Range: [0, 1]
    - Identical angles give R=1
    - Rotation invariant
    """

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=3, max_value=50),
            elements=st.floats(
                min_value=-10 * np.pi,
                max_value=10 * np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_output_in_valid_range(self, angles: NDArray[np.float64]) -> None:
        """Property: mean resultant length is in [0, 1]."""
        r = mean_resultant_length(angles)

        # Allow small numerical tolerance for floating-point arithmetic
        assert -1e-10 <= r <= 1.0 + 1e-10, f"R={r} outside [0, 1]"

    @given(
        st.floats(min_value=-np.pi, max_value=np.pi, allow_nan=False),
        st.integers(min_value=3, max_value=50),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_identical_angles_give_one(self, angle: float, n: int) -> None:
        """Property: identical angles give R=1."""
        angles = np.full(n, angle, dtype=np.float64)
        r = mean_resultant_length(angles)

        np.testing.assert_allclose(
            r,
            1.0,
            rtol=1e-10,
            err_msg="Identical angles should give R=1",
        )

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=3, max_value=50),
            elements=st.floats(
                min_value=-np.pi,
                max_value=np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_rotation_invariant(self, angles: NDArray[np.float64]) -> None:
        """Property: R is invariant to rotation of all angles."""
        rotation = np.pi / 3  # Arbitrary rotation
        rotated_angles = angles + rotation

        r_original = mean_resultant_length(angles)
        r_rotated = mean_resultant_length(rotated_angles)

        np.testing.assert_allclose(
            r_original,
            r_rotated,
            rtol=1e-10,
            err_msg="R should be rotation invariant",
        )


class TestCircularVarianceProperties:
    """Property-based tests for circular_variance function.

    Circular variance V = 1 - R measures dispersion of circular data.
    Key properties:
    - Range: [0, 1]
    - V + R = 1 (by definition)
    - Identical angles give V=0
    """

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=3, max_value=50),
            elements=st.floats(
                min_value=-10 * np.pi,
                max_value=10 * np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_variance_plus_r_equals_one(self, angles: NDArray[np.float64]) -> None:
        """Property: variance = 1 - R (definition check)."""
        r = mean_resultant_length(angles)
        v = circular_variance(angles)

        np.testing.assert_allclose(
            v + r,
            1.0,
            rtol=1e-10,
            err_msg="V + R should equal 1",
        )

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.integers(min_value=3, max_value=50),
            elements=st.floats(
                min_value=-10 * np.pi,
                max_value=10 * np.pi,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_output_in_valid_range(self, angles: NDArray[np.float64]) -> None:
        """Property: circular variance is in [0, 1]."""
        v = circular_variance(angles)

        # Allow small numerical tolerance for floating-point arithmetic
        assert -1e-10 <= v <= 1.0 + 1e-10, f"Variance {v} outside [0, 1]"

    @given(
        st.floats(min_value=-np.pi, max_value=np.pi, allow_nan=False),
        st.integers(min_value=3, max_value=50),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_identical_angles_give_zero_variance(self, angle: float, n: int) -> None:
        """Property: identical angles give variance=0."""
        angles = np.full(n, angle, dtype=np.float64)
        v = circular_variance(angles)

        np.testing.assert_allclose(
            v,
            0.0,
            atol=1e-10,
            err_msg="Identical angles should give variance=0",
        )


# =============================================================================
# Property Tests for Egocentric Transforms
# =============================================================================


class TestEgocentricTransformProperties:
    """Property-based tests for egocentric coordinate transforms.

    Allocentric <-> egocentric transforms convert between world coordinates
    and animal-centered coordinates. Key properties:
    - Roundtrip: allo -> ego -> allo is identity
    - Self position maps to origin in egocentric frame
    - Point ahead has positive x in egocentric frame

    Note: API is allocentric_to_egocentric(points, positions, headings)
    Output shape: (n_time, n_points, 2)
    """

    @given(
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-np.pi, max_value=np.pi, allow_nan=False),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_allocentric_to_egocentric_roundtrip(
        self, x: float, y: float, heading: float
    ) -> None:
        """Property: allocentric -> egocentric -> allocentric is identity."""
        from neurospatial.ops.egocentric import (
            allocentric_to_egocentric,
            egocentric_to_allocentric,
        )

        position = np.array([[x, y]])
        point = np.array([[x + 10, y + 5]])  # Some other point (n_points=1, 2)
        headings = np.array([heading])

        # Transform to egocentric: (points, positions, headings)
        # Output: (n_time=1, n_points=1, 2)
        ego_point = allocentric_to_egocentric(point, position, headings)

        # Transform back to allocentric: (points, positions, headings)
        # Input shape: (n_time=1, n_points=1, 2)
        allo_point = egocentric_to_allocentric(ego_point, position, headings)

        # Output: (n_time=1, n_points=1, 2), compare to original (n_points=1, 2)
        np.testing.assert_allclose(
            allo_point[0],  # Take first time point
            point,
            rtol=1e-10,
            atol=1e-10,
            err_msg="Roundtrip should recover original point",
        )

    @given(
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-np.pi, max_value=np.pi, allow_nan=False),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_self_position_maps_to_origin(
        self, x: float, y: float, heading: float
    ) -> None:
        """Property: animal's position maps to origin in egocentric frame."""
        from neurospatial.ops.egocentric import allocentric_to_egocentric

        position = np.array([[x, y]])  # shape (n_time=1, 2)
        headings = np.array([heading])

        # Transform position to egocentric: (points, positions, headings)
        # Output: (n_time=1, n_points=1, 2)
        ego_self = allocentric_to_egocentric(position, position, headings)

        np.testing.assert_allclose(
            ego_self[0, 0],  # First time, first point
            [0.0, 0.0],
            atol=1e-10,
            err_msg="Self position should map to origin",
        )

    @given(
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-np.pi, max_value=np.pi, allow_nan=False),
        st.floats(min_value=1, max_value=50, allow_nan=False),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_point_ahead_has_positive_x(
        self, x: float, y: float, heading: float, distance: float
    ) -> None:
        """Property: point directly ahead maps to positive x-axis."""
        from neurospatial.ops.egocentric import allocentric_to_egocentric

        position = np.array([[x, y]])  # shape (n_time=1, 2)
        headings = np.array([heading])

        # Point directly ahead in allocentric coordinates
        point_ahead = np.array(
            [[x + distance * np.cos(heading), y + distance * np.sin(heading)]]
        )

        # Transform: (points, positions, headings)
        # Output: (n_time=1, n_points=1, 2)
        ego_ahead = allocentric_to_egocentric(point_ahead, position, headings)

        # In egocentric frame, "ahead" should be positive x (or close to it)
        np.testing.assert_allclose(
            ego_ahead[0, 0, 0],  # First time, first point, x-coordinate
            distance,
            rtol=1e-10,
            atol=1e-10,
            err_msg="Point ahead should have x=distance",
        )
        np.testing.assert_allclose(
            ego_ahead[0, 0, 1],  # First time, first point, y-coordinate
            0.0,
            atol=1e-10,
            err_msg="Point ahead should have y=0",
        )


class TestBearingProperties:
    """Property-based tests for egocentric bearing computations.

    Egocentric bearing measures the angle from the animal's heading
    to an object. Key properties:
    - Bearing ahead is 0
    - Bearing left is +pi/2
    - Bearing always in [-pi, pi]

    Note: compute_egocentric_bearing takes (targets, positions, headings)
    """

    @given(
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-np.pi, max_value=np.pi, allow_nan=False),
        st.floats(min_value=1, max_value=50, allow_nan=False),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_bearing_ahead_is_zero(
        self, x: float, y: float, heading: float, distance: float
    ) -> None:
        """Property: bearing to point directly ahead is 0."""
        from neurospatial.ops.egocentric import compute_egocentric_bearing

        position = np.array([[x, y]])
        headings = np.array([heading])

        # Point directly ahead
        point_ahead = np.array(
            [[x + distance * np.cos(heading), y + distance * np.sin(heading)]]
        )

        # API: compute_egocentric_bearing(targets, positions, headings)
        bearing = compute_egocentric_bearing(point_ahead, position, headings)

        np.testing.assert_allclose(
            bearing[0, 0],
            0.0,
            atol=1e-10,
            err_msg="Bearing to point ahead should be 0",
        )

    @given(
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-np.pi, max_value=np.pi, allow_nan=False),
        st.floats(min_value=1, max_value=50, allow_nan=False),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_bearing_left_is_positive(
        self, x: float, y: float, heading: float, distance: float
    ) -> None:
        """Property: bearing to point directly left is +pi/2."""
        from neurospatial.ops.egocentric import compute_egocentric_bearing

        position = np.array([[x, y]])
        headings = np.array([heading])

        # Point directly to the left (90 degrees counterclockwise from heading)
        left_heading = heading + np.pi / 2
        point_left = np.array(
            [[x + distance * np.cos(left_heading), y + distance * np.sin(left_heading)]]
        )

        # API: compute_egocentric_bearing(targets, positions, headings)
        bearing = compute_egocentric_bearing(point_left, position, headings)

        np.testing.assert_allclose(
            bearing[0, 0],
            np.pi / 2,
            atol=1e-10,
            err_msg="Bearing to point left should be +pi/2",
        )

    @given(
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-100, max_value=100, allow_nan=False),
        st.floats(min_value=-np.pi, max_value=np.pi, allow_nan=False),
        st.integers(min_value=0, max_value=10000),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_bearing_always_in_range(
        self, x: float, y: float, heading: float, seed: int
    ) -> None:
        """Property: bearing is always in [-pi, pi]."""
        from neurospatial.ops.egocentric import compute_egocentric_bearing

        position = np.array([[x, y]])
        headings = np.array([heading])

        # Random object positions
        rng = np.random.default_rng(seed)
        objects = rng.uniform(-100, 100, size=(10, 2))

        # API: compute_egocentric_bearing(targets, positions, headings)
        bearings = compute_egocentric_bearing(objects, position, headings)

        assert np.all(bearings >= -np.pi), "Found bearing < -pi"
        assert np.all(bearings <= np.pi), "Found bearing > pi"


# =============================================================================
# Property Tests for Distance Operations
# =============================================================================


class TestEuclideanDistanceMatrixProperties:
    """Property-based tests for Euclidean distance matrix operations.

    Distance matrices must satisfy metric space axioms:
    - Symmetry: d(a,b) = d(b,a)
    - Non-negativity: d(a,b) >= 0
    - Identity: d(a,a) = 0
    - Triangle inequality: d(a,c) <= d(a,b) + d(b,c)
    """

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.tuples(
                st.integers(min_value=2, max_value=20),
                st.just(2),
            ),
            elements=st.floats(
                min_value=0.0,
                max_value=100.0,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_distance_matrix_symmetry(self, positions: NDArray[np.float64]) -> None:
        """Property: distance matrix is symmetric."""
        from neurospatial.ops.distance import euclidean_distance_matrix

        dist_matrix = euclidean_distance_matrix(positions)

        np.testing.assert_allclose(
            dist_matrix,
            dist_matrix.T,
            rtol=1e-10,
            err_msg="Distance matrix should be symmetric",
        )

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.tuples(
                st.integers(min_value=2, max_value=20),
                st.just(2),
            ),
            elements=st.floats(
                min_value=0.0,
                max_value=100.0,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_distance_matrix_non_negative(self, positions: NDArray[np.float64]) -> None:
        """Property: all distances are non-negative."""
        from neurospatial.ops.distance import euclidean_distance_matrix

        dist_matrix = euclidean_distance_matrix(positions)

        assert np.all(dist_matrix >= 0), "Found negative distances"

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.tuples(
                st.integers(min_value=2, max_value=20),
                st.just(2),
            ),
            elements=st.floats(
                min_value=0.0,
                max_value=100.0,
                allow_nan=False,
                allow_infinity=False,
            ),
        )
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_distance_matrix_zero_diagonal(
        self, positions: NDArray[np.float64]
    ) -> None:
        """Property: diagonal elements are zero (self-distance)."""
        from neurospatial.ops.distance import euclidean_distance_matrix

        dist_matrix = euclidean_distance_matrix(positions)

        np.testing.assert_allclose(
            np.diag(dist_matrix),
            0.0,
            atol=1e-10,
            err_msg="Self-distance should be zero",
        )

    @given(
        hnp.arrays(
            dtype=np.float64,
            shape=st.tuples(
                st.integers(min_value=3, max_value=15),
                st.just(2),
            ),
            elements=st.floats(
                min_value=0.0,
                max_value=100.0,
                allow_nan=False,
                allow_infinity=False,
            ),
        ),
        st.integers(min_value=0, max_value=10000),
    )
    @settings(deadline=1000)  # Use profile's max_examples
    def test_triangle_inequality(
        self, positions: NDArray[np.float64], seed: int
    ) -> None:
        """Property: d(a,c) <= d(a,b) + d(b,c) for all triplets."""
        from neurospatial.ops.distance import euclidean_distance_matrix

        dist_matrix = euclidean_distance_matrix(positions)
        n = len(positions)

        # Test triangle inequality for random triplets (testing all is O(n^3))
        rng = np.random.default_rng(seed)
        n_tests = min(100, n * (n - 1) * (n - 2) // 6)

        for _ in range(n_tests):
            i, j, k = rng.choice(n, size=3, replace=False)
            d_ij = dist_matrix[i, j]
            d_jk = dist_matrix[j, k]
            d_ik = dist_matrix[i, k]

            assert d_ik <= d_ij + d_jk + 1e-10, (
                f"Triangle inequality violated: d({i},{k})={d_ik} > "
                f"d({i},{j})={d_ij} + d({j},{k})={d_jk}"
            )
