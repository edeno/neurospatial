{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Bayesian Position Decoding\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Build encoding models (place fields) for a population of neurons\n",
    "- Decode spatial position from population spike counts using Bayesian methods\n",
    "- Access and interpret `DecodingResult` properties (posterior, MAP, mean, uncertainty)\n",
    "- Evaluate decoding accuracy with error metrics\n",
    "- Detect trajectory structure using isotonic/linear regression and Radon transform\n",
    "- Test significance of decoded sequences using shuffle-based methods\n",
    "\n",
    "**Estimated time: 25-30 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from neurospatial import (\n",
    "    Environment,\n",
    "    compute_place_field,\n",
    "    decode_position,\n",
    "    decoding_error,\n",
    "    median_decoding_error,\n",
    ")\n",
    "from neurospatial.decoding import (\n",
    "    compute_shuffle_pvalue,\n",
    "    confusion_matrix,\n",
    "    decoding_correlation,\n",
    "    fit_isotonic_trajectory,\n",
    "    fit_linear_trajectory,\n",
    "    shuffle_time_bins,\n",
    ")\n",
    "from neurospatial.simulation import (\n",
    "    PlaceCellModel,\n",
    "    generate_poisson_spikes,\n",
    "    simulate_trajectory_ou,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for clear figures\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"axes.labelsize\"] = 13\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "\n",
    "# Colorblind-friendly palette\n",
    "COLORS = {\n",
    "    \"blue\": \"#0173B2\",\n",
    "    \"orange\": \"#DE8F05\",\n",
    "    \"green\": \"#029E73\",\n",
    "    \"red\": \"#CC78BC\",\n",
    "    \"cyan\": \"#56B4E9\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Generate Synthetic Data\n",
    "\n",
    "We'll create a 1D linear track environment with a population of place cells. This simplified setup makes it easy to visualize and understand decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D linear track (100 cm)\n",
    "track_length = 100.0  # cm\n",
    "bin_size = 2.0  # cm per bin\n",
    "\n",
    "# Create 1D positions along track\n",
    "positions_1d = np.linspace(0, track_length, 51).reshape(-1, 1)\n",
    "env = Environment.from_samples(positions_1d, bin_size=bin_size)\n",
    "env.units = \"cm\"\n",
    "\n",
    "print(f\"Track length: {track_length} cm\")\n",
    "print(f\"Number of spatial bins: {env.n_bins}\")\n",
    "print(f\"Bin size: {bin_size} cm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trajectory-header",
   "metadata": {},
   "source": [
    "### Generate Animal Trajectory\n",
    "\n",
    "Simulate a rat running back and forth on the linear track:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate smooth trajectory on track using OU process\n",
    "duration = 600.0  # 10 minutes\n",
    "positions, times = simulate_trajectory_ou(\n",
    "    env,\n",
    "    duration=duration,\n",
    "    dt=0.01,  # 10ms timestep\n",
    "    speed_mean=15.0,  # cm/s\n",
    "    speed_std=2.0,\n",
    "    coherence_time=0.5,\n",
    "    boundary_mode=\"reflect\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Trajectory duration: {times[-1]:.1f} seconds\")\n",
    "print(f\"Number of samples: {len(times)}\")\n",
    "print(f\"Sampling rate: {1 / (times[1] - times[0]):.0f} Hz\")\n",
    "print(f\"Position range: [{positions.min():.1f}, {positions.max():.1f}] cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trajectory\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.plot(times[:5000], positions[:5000, 0], color=COLORS[\"blue\"], linewidth=0.5)\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Position (cm)\")\n",
    "ax.set_title(\"First 50 seconds of trajectory\", fontweight=\"bold\")\n",
    "ax.set_xlim(0, 50)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "place-cells-header",
   "metadata": {},
   "source": [
    "### Create Place Cell Population\n",
    "\n",
    "Generate a population of place cells with fields distributed along the track:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-place-cells",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create place cells with fields distributed along the track\n",
    "n_neurons = 30\n",
    "field_centers = np.linspace(5, track_length - 5, n_neurons)  # Spread across track\n",
    "field_widths = np.random.uniform(8, 15, n_neurons)  # Variable field widths\n",
    "peak_rates = np.random.uniform(10, 30, n_neurons)  # Variable peak firing rates\n",
    "\n",
    "# Create PlaceCellModel for each neuron and generate spikes\n",
    "spike_times_list = []\n",
    "place_cells = []\n",
    "\n",
    "for i in range(n_neurons):\n",
    "    cell = PlaceCellModel(\n",
    "        env,\n",
    "        center=np.array([field_centers[i]]),\n",
    "        width=field_widths[i],\n",
    "        max_rate=peak_rates[i],\n",
    "        baseline_rate=0.1,\n",
    "    )\n",
    "    place_cells.append(cell)\n",
    "\n",
    "    # Generate spike train\n",
    "    rates = cell.firing_rate(positions, times)\n",
    "    spikes = generate_poisson_spikes(rates, times, refractory_period=0.002, seed=42 + i)\n",
    "    spike_times_list.append(spikes)\n",
    "\n",
    "print(f\"Created {n_neurons} place cells\")\n",
    "print(f\"Total spikes: {sum(len(s) for s in spike_times_list)}\")\n",
    "print(f\"Mean spikes per neuron: {np.mean([len(s) for s in spike_times_list]):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-place-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few place fields\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    cell_idx = idx * (n_neurons // 3)\n",
    "    field = compute_place_field(\n",
    "        env,\n",
    "        spike_times_list[cell_idx],\n",
    "        times,\n",
    "        positions,\n",
    "        method=\"diffusion_kde\",\n",
    "        bandwidth=5.0,\n",
    "    )\n",
    "\n",
    "    # Plot field\n",
    "    x_positions = env.bin_centers[:, 0]\n",
    "    ax.bar(x_positions, field, width=bin_size * 0.8, color=COLORS[\"blue\"], alpha=0.7)\n",
    "    ax.axvline(\n",
    "        field_centers[cell_idx],\n",
    "        color=COLORS[\"red\"],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=\"True center\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Position (cm)\")\n",
    "    ax.set_ylabel(\"Firing rate (Hz)\")\n",
    "    ax.set_title(f\"Neuron {cell_idx + 1}\", fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Example Place Fields\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Build Encoding Models\n",
    "\n",
    "Encoding models are place fields that describe how each neuron's firing rate varies with spatial position. These are the \"tuning curves\" we use for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-encoding-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute place fields for all neurons (encoding models)\n",
    "encoding_models = np.array(\n",
    "    [\n",
    "        compute_place_field(\n",
    "            env,\n",
    "            spike_times_list[i],\n",
    "            times,\n",
    "            positions,\n",
    "            method=\"diffusion_kde\",\n",
    "            bandwidth=5.0,\n",
    "        )\n",
    "        for i in range(n_neurons)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Encoding models shape: {encoding_models.shape}\")\n",
    "print(f\"  (n_neurons, n_bins) = ({n_neurons}, {env.n_bins})\")\n",
    "print(f\"Max firing rate: {np.nanmax(encoding_models):.2f} Hz\")\n",
    "print(f\"NaN values: {np.isnan(encoding_models).sum()}\")\n",
    "\n",
    "# Replace NaN with small baseline (bins without enough occupancy)\n",
    "encoding_models = np.nan_to_num(encoding_models, nan=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-encoding-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all encoding models as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Sort neurons by peak position for better visualization\n",
    "peak_positions = np.argmax(encoding_models, axis=1)\n",
    "sorted_idx = np.argsort(peak_positions)\n",
    "sorted_models = encoding_models[sorted_idx]\n",
    "\n",
    "im = ax.imshow(\n",
    "    sorted_models,\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"hot\",\n",
    "    extent=[0, track_length, n_neurons, 0],\n",
    "    vmin=0,\n",
    "    vmax=np.percentile(sorted_models, 95),\n",
    ")\n",
    "ax.set_xlabel(\"Position (cm)\")\n",
    "ax.set_ylabel(\"Neuron (sorted by peak position)\")\n",
    "ax.set_title(\"Population Encoding Models (Place Fields)\", fontweight=\"bold\")\n",
    "plt.colorbar(im, label=\"Firing rate (Hz)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Bin Spikes for Decoding\n",
    "\n",
    "Bayesian decoding works on spike counts in discrete time bins. We need to convert spike times to spike counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bin-spikes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time bin parameters\n",
    "dt = 0.1  # 100 ms time bins (typical for spatial decoding)\n",
    "time_bins = np.arange(0, times[-1], dt)\n",
    "n_time_bins = len(time_bins) - 1\n",
    "\n",
    "# Bin spikes for each neuron\n",
    "spike_counts = np.zeros((n_time_bins, n_neurons), dtype=np.int64)\n",
    "for i, spikes in enumerate(spike_times_list):\n",
    "    spike_counts[:, i], _ = np.histogram(spikes, bins=time_bins)\n",
    "\n",
    "# Get actual positions at each time bin center\n",
    "time_bin_centers = time_bins[:-1] + dt / 2\n",
    "actual_bin_indices = np.searchsorted(times, time_bin_centers) - 1\n",
    "actual_bin_indices = np.clip(actual_bin_indices, 0, len(positions) - 1)\n",
    "actual_positions = positions[actual_bin_indices]\n",
    "\n",
    "print(f\"Time bin width: {dt * 1000:.0f} ms\")\n",
    "print(f\"Number of time bins: {n_time_bins}\")\n",
    "print(f\"Spike counts shape: {spike_counts.shape}\")\n",
    "print(f\"Total spikes in binned data: {spike_counts.sum()}\")\n",
    "print(f\"Mean spikes per time bin: {spike_counts.sum(axis=1).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Decode Position\n",
    "\n",
    "Now we can decode position using Bayesian inference. The `decode_position()` function computes the posterior probability distribution over positions for each time bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decode-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode position from spike counts\n",
    "result = decode_position(\n",
    "    env,\n",
    "    spike_counts,\n",
    "    encoding_models,\n",
    "    dt,\n",
    "    prior=None,  # Uniform prior\n",
    "    times=time_bin_centers,\n",
    ")\n",
    "\n",
    "print(\"Decoding complete!\")\n",
    "print(f\"Result type: {type(result).__name__}\")\n",
    "print(f\"Posterior shape: {result.posterior.shape}\")\n",
    "print(f\"  (n_time_bins, n_bins) = ({result.n_time_bins}, {env.n_bins})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "result-properties-header",
   "metadata": {},
   "source": [
    "### DecodingResult Properties\n",
    "\n",
    "The `DecodingResult` container provides several useful properties (computed lazily):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "result-properties",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access result properties\n",
    "print(\"DecodingResult Properties:\")\n",
    "print(f\"  posterior shape: {result.posterior.shape}\")\n",
    "print(f\"  map_estimate shape: {result.map_estimate.shape} (bin indices)\")\n",
    "print(f\"  map_position shape: {result.map_position.shape} (coordinates)\")\n",
    "print(f\"  mean_position shape: {result.mean_position.shape} (coordinates)\")\n",
    "print(f\"  uncertainty shape: {result.uncertainty.shape} (entropy in bits)\")\n",
    "print(f\"  times shape: {result.times.shape if result.times is not None else 'None'}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nPosterior sum check (should be ~1.0): {result.posterior.sum(axis=1).mean():.6f}\"\n",
    ")\n",
    "print(f\"Mean uncertainty: {result.uncertainty.mean():.2f} bits\")\n",
    "print(f\"Max uncertainty (uniform): {np.log2(env.n_bins):.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "### Visualize Decoding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-posterior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior probability as heatmap (first 100 time bins)\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "n_show = 500  # Number of time bins to show\n",
    "result.plot(ax=ax, show_map=True, colorbar=True)\n",
    "ax.set_xlim(0, n_show * dt)\n",
    "\n",
    "# Overlay actual position\n",
    "ax.plot(\n",
    "    time_bin_centers[:n_show],\n",
    "    actual_positions[:n_show, 0],\n",
    "    color=COLORS[\"cyan\"],\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    "    label=\"Actual position\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Position (cm)\")\n",
    "ax.set_title(\n",
    "    \"Decoded Posterior with MAP Estimate (white) and Actual Position (cyan)\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-decoded-vs-actual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare decoded vs actual position\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time series comparison\n",
    "ax = axes[0]\n",
    "ax.plot(\n",
    "    time_bin_centers[:n_show],\n",
    "    actual_positions[:n_show, 0],\n",
    "    color=COLORS[\"blue\"],\n",
    "    linewidth=1,\n",
    "    alpha=0.7,\n",
    "    label=\"Actual\",\n",
    ")\n",
    "ax.plot(\n",
    "    time_bin_centers[:n_show],\n",
    "    result.map_position[:n_show, 0],\n",
    "    color=COLORS[\"orange\"],\n",
    "    linewidth=1,\n",
    "    alpha=0.7,\n",
    "    label=\"Decoded (MAP)\",\n",
    ")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Position (cm)\")\n",
    "ax.set_title(\"Decoded vs Actual Position Over Time\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "\n",
    "# Scatter plot\n",
    "ax = axes[1]\n",
    "ax.scatter(\n",
    "    actual_positions[:, 0], result.map_position[:, 0], alpha=0.3, s=3, c=COLORS[\"blue\"]\n",
    ")\n",
    "ax.plot(\n",
    "    [0, track_length], [0, track_length], \"k--\", linewidth=2, label=\"Perfect decoding\"\n",
    ")\n",
    "ax.set_xlabel(\"Actual position (cm)\")\n",
    "ax.set_ylabel(\"Decoded position (cm)\")\n",
    "ax.set_title(\"Decoded vs Actual Position\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Evaluate Decoding Accuracy\n",
    "\n",
    "Let's quantify how well the decoder performs using error metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-errors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute decoding error\n",
    "errors = decoding_error(result.map_position, actual_positions)\n",
    "median_err = median_decoding_error(result.map_position, actual_positions)\n",
    "\n",
    "# Also compute error for mean position estimate\n",
    "mean_errors = decoding_error(result.mean_position, actual_positions)\n",
    "median_mean_err = median_decoding_error(result.mean_position, actual_positions)\n",
    "\n",
    "print(\"Decoding Error Summary:\")\n",
    "print(\"\\nMAP estimate:\")\n",
    "print(f\"  Median error: {median_err:.2f} cm\")\n",
    "print(f\"  Mean error: {np.nanmean(errors):.2f} cm\")\n",
    "print(f\"  Std error: {np.nanstd(errors):.2f} cm\")\n",
    "\n",
    "print(\"\\nMean position estimate:\")\n",
    "print(f\"  Median error: {median_mean_err:.2f} cm\")\n",
    "print(f\"  Mean error: {np.nanmean(mean_errors):.2f} cm\")\n",
    "print(f\"  Std error: {np.nanstd(mean_errors):.2f} cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-error-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram of errors\n",
    "ax = axes[0]\n",
    "ax.hist(errors, bins=50, color=COLORS[\"blue\"], alpha=0.7, edgecolor=\"white\")\n",
    "ax.axvline(\n",
    "    median_err,\n",
    "    color=COLORS[\"red\"],\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Median = {median_err:.2f} cm\",\n",
    ")\n",
    "ax.set_xlabel(\"Decoding error (cm)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Error Distribution (MAP estimate)\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "\n",
    "# Error vs uncertainty\n",
    "ax = axes[1]\n",
    "ax.scatter(result.uncertainty, errors, alpha=0.3, s=5, c=COLORS[\"blue\"])\n",
    "ax.set_xlabel(\"Uncertainty (bits)\")\n",
    "ax.set_ylabel(\"Decoding error (cm)\")\n",
    "ax.set_title(\"Error vs Posterior Uncertainty\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation\n",
    "corr = decoding_correlation(result.map_position, actual_positions)\n",
    "print(f\"\\nDecoding correlation: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-header",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix shows which positions are confused with each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "actual_bins = env.bin_at(actual_positions)\n",
    "cm = confusion_matrix(env, result.posterior, actual_bins, method=\"map\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "im = ax.imshow(cm, cmap=\"Blues\", aspect=\"auto\")\n",
    "ax.set_xlabel(\"Decoded bin\")\n",
    "ax.set_ylabel(\"Actual bin\")\n",
    "ax.set_title(\"Confusion Matrix\", fontweight=\"bold\")\n",
    "plt.colorbar(im, label=\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Diagonal (correct) proportion: {np.diag(cm).sum() / cm.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Trajectory Analysis (for Replay Detection)\n",
    "\n",
    "For replay detection, we often want to detect whether decoded positions follow a coherent trajectory. Let's analyze a short segment that might resemble replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a short segment (simulating a replay event)\n",
    "start_idx = 100\n",
    "end_idx = 150  # 50 time bins = 5 seconds\n",
    "\n",
    "segment_posterior = result.posterior[start_idx:end_idx]\n",
    "segment_times = time_bin_centers[start_idx:end_idx]\n",
    "segment_actual = actual_positions[start_idx:end_idx, 0]\n",
    "\n",
    "print(f\"Segment: {segment_times[0]:.1f}s to {segment_times[-1]:.1f}s\")\n",
    "print(f\"Duration: {segment_times[-1] - segment_times[0]:.1f}s\")\n",
    "print(f\"Number of time bins: {len(segment_times)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isotonic-header",
   "metadata": {},
   "source": [
    "### Isotonic Regression\n",
    "\n",
    "Fit a monotonic (increasing or decreasing) trajectory to the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isotonic-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit isotonic trajectory\n",
    "iso_result = fit_isotonic_trajectory(\n",
    "    segment_posterior,\n",
    "    segment_times,\n",
    "    method=\"expected\",  # Use posterior mean\n",
    "    increasing=None,  # Auto-detect direction\n",
    ")\n",
    "\n",
    "print(\"Isotonic Regression Results:\")\n",
    "print(f\"  Direction: {iso_result.direction}\")\n",
    "print(f\"  R-squared: {iso_result.r_squared:.4f}\")\n",
    "print(\n",
    "    f\"  Fitted positions range: [{iso_result.fitted_positions.min():.1f}, {iso_result.fitted_positions.max():.1f}] bins\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-isotonic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize isotonic fit\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Posterior heatmap with isotonic fit\n",
    "ax = axes[0]\n",
    "extent = [segment_times[0], segment_times[-1], 0, env.n_bins]\n",
    "ax.imshow(\n",
    "    segment_posterior.T, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", extent=extent\n",
    ")\n",
    "ax.plot(\n",
    "    segment_times,\n",
    "    iso_result.fitted_positions,\n",
    "    color=\"white\",\n",
    "    linewidth=2,\n",
    "    label=\"Isotonic fit\",\n",
    ")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Position (bin index)\")\n",
    "ax.set_title(f\"Isotonic Fit (R² = {iso_result.r_squared:.4f})\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "\n",
    "# Residuals\n",
    "ax = axes[1]\n",
    "ax.bar(\n",
    "    range(len(iso_result.residuals)),\n",
    "    iso_result.residuals,\n",
    "    color=COLORS[\"blue\"],\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.axhline(0, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "ax.set_xlabel(\"Time bin\")\n",
    "ax.set_ylabel(\"Residual (bins)\")\n",
    "ax.set_title(\"Isotonic Regression Residuals\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-header",
   "metadata": {},
   "source": [
    "### Linear Regression with Uncertainty\n",
    "\n",
    "Fit a linear trajectory using Monte Carlo sampling to account for posterior uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear trajectory with sampling\n",
    "lin_result = fit_linear_trajectory(\n",
    "    env,\n",
    "    segment_posterior,\n",
    "    segment_times,\n",
    "    method=\"sample\",  # Monte Carlo sampling\n",
    "    n_samples=1000,\n",
    "    rng=42,\n",
    ")\n",
    "\n",
    "print(\"Linear Regression Results (sampling method):\")\n",
    "print(f\"  Slope: {lin_result.slope:.2f} bins/s\")\n",
    "print(f\"  Slope std: {lin_result.slope_std:.2f} bins/s\")\n",
    "print(f\"  Intercept: {lin_result.intercept:.2f} bins\")\n",
    "print(f\"  R-squared: {lin_result.r_squared:.4f}\")\n",
    "\n",
    "# Convert slope to cm/s\n",
    "slope_cm_per_s = lin_result.slope * bin_size\n",
    "print(f\"\\n  Speed: {slope_cm_per_s:.1f} cm/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Shuffle-Based Significance Testing\n",
    "\n",
    "To determine if a decoded sequence is significant, we compare it to a null distribution generated by shuffling. This is essential for replay detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shuffle-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the segment of spike counts for shuffling\n",
    "segment_spikes = spike_counts[start_idx:end_idx]\n",
    "\n",
    "# Use R² from isotonic fit as our sequence score\n",
    "observed_score = iso_result.r_squared\n",
    "\n",
    "# Generate null distribution by shuffling time bins\n",
    "n_shuffles = 500  # Use fewer for demo (typically 1000+)\n",
    "null_scores = []\n",
    "\n",
    "print(f\"Running {n_shuffles} shuffles...\")\n",
    "for shuffled_spikes in shuffle_time_bins(segment_spikes, n_shuffles=n_shuffles, rng=42):\n",
    "    # Decode shuffled spikes\n",
    "    shuffled_result = decode_position(env, shuffled_spikes, encoding_models, dt)\n",
    "    # Fit isotonic trajectory\n",
    "    shuffled_fit = fit_isotonic_trajectory(\n",
    "        shuffled_result.posterior, segment_times, method=\"expected\"\n",
    "    )\n",
    "    null_scores.append(shuffled_fit.r_squared)\n",
    "\n",
    "null_scores = np.array(null_scores)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-pvalue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute p-value\n",
    "p_value = compute_shuffle_pvalue(observed_score, null_scores, tail=\"greater\")\n",
    "\n",
    "print(\"Significance Test Results:\")\n",
    "print(f\"  Observed R²: {observed_score:.4f}\")\n",
    "print(f\"  Null mean: {null_scores.mean():.4f}\")\n",
    "print(f\"  Null std: {null_scores.std():.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant (p < 0.05): {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-null-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize null distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(\n",
    "    null_scores,\n",
    "    bins=50,\n",
    "    color=COLORS[\"blue\"],\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"white\",\n",
    "    label=\"Null distribution\",\n",
    ")\n",
    "ax.axvline(\n",
    "    observed_score,\n",
    "    color=COLORS[\"red\"],\n",
    "    linewidth=3,\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Observed (R² = {observed_score:.4f})\",\n",
    ")\n",
    "ax.axvline(\n",
    "    np.percentile(null_scores, 95),\n",
    "    color=COLORS[\"orange\"],\n",
    "    linewidth=2,\n",
    "    linestyle=\":\",\n",
    "    label=\"95th percentile\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Isotonic R²\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"Shuffle Test: p = {p_value:.4f}\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Export Results\n",
    "\n",
    "The `DecodingResult` can be exported to a pandas DataFrame for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "to-dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to DataFrame\n",
    "df = result.to_dataframe()\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "### Building Encoding Models\n",
    "- Compute place fields for each neuron using `compute_place_field()`\n",
    "- Stack into encoding models array: shape `(n_neurons, n_bins)`\n",
    "\n",
    "### Bayesian Decoding\n",
    "- Bin spikes into spike counts: shape `(n_time_bins, n_neurons)`\n",
    "- Decode with `decode_position()` to get posterior distribution\n",
    "- Access `DecodingResult` properties: `posterior`, `map_position`, `mean_position`, `uncertainty`\n",
    "\n",
    "### Error Metrics\n",
    "- `decoding_error()` - Per-time-bin position error\n",
    "- `median_decoding_error()` - Summary statistic\n",
    "- `decoding_correlation()` - Weighted Pearson correlation\n",
    "- `confusion_matrix()` - Spatial confusion analysis\n",
    "\n",
    "### Trajectory Analysis\n",
    "- `fit_isotonic_trajectory()` - Monotonic trajectory fitting\n",
    "- `fit_linear_trajectory()` - Linear fit with uncertainty estimation\n",
    "- R² measures trajectory coherence\n",
    "\n",
    "### Significance Testing\n",
    "- `shuffle_time_bins()` - Generate null distribution\n",
    "- `compute_shuffle_pvalue()` - Monte Carlo p-value\n",
    "- Essential for determining if decoded sequences are significant\n",
    "\n",
    "### Next Steps\n",
    "- Try different shuffle methods (`shuffle_cell_identity`, `shuffle_place_fields_circular`)\n",
    "- Explore the Radon transform for trajectory detection (`detect_trajectory_radon`)\n",
    "- Apply to real neural data\n",
    "- Experiment with custom priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. **Zhang, K., Ginzburg, I., McNaughton, B. L., & Sejnowski, T. J. (1998)**. \"Interpreting neuronal population activity by reconstruction: Unified framework with application to hippocampal place cells.\" *Journal of Neurophysiology*.\n",
    "\n",
    "2. **Davidson, T. J., Kloosterman, F., & Wilson, M. A. (2009)**. \"Hippocampal replay of extended experience.\" *Neuron*.\n",
    "\n",
    "3. **Karlsson, M. P., & Frank, L. M. (2009)**. \"Awake replay of remote experiences in the hippocampus.\" *Nature Neuroscience*."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_as_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
