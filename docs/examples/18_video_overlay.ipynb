{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Video Overlay\n",
    "\n",
    "This notebook demonstrates the VideoOverlay feature for compositing raw behavioral video frames with spatial field animations:\n",
    "\n",
    "1. **Loading Video Metadata** - Inspect video dimensions, frame rate, and duration\n",
    "2. **Scale Bar Calibration** - Map pixels to cm using a known-length scale bar\n",
    "3. **Landmark Calibration** - Map pixels to cm using corresponding arena corners\n",
    "4. **VideoOverlay Options** - Control alpha, z-order, crop, and downsample\n",
    "5. **Animation with Video Background** - Composite video beneath spatial fields\n",
    "6. **Exporting Synchronized Video** - Export video with overlays\n",
    "7. **Performance Tips** - Handle large videos efficiently\n",
    "\n",
    "**Estimated time**: 15-20 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Create video calibrations using scale bars or landmark correspondences\n",
    "- Overlay behavioral video on animated spatial fields\n",
    "- Control video appearance with alpha blending and z-order\n",
    "- Export synchronized video files\n",
    "- Handle memory-efficient streaming for large videos\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Required dependencies**:\n",
    "\n",
    "```bash\n",
    "# OpenCV for video reading\n",
    "pip install opencv-python>=4.11.0\n",
    "\n",
    "# imageio for video creation in examples\n",
    "pip install imageio>=2.35.0 imageio-ffmpeg>=0.5.1\n",
    "```\n",
    "\n",
    "**Optional dependencies**:\n",
    "\n",
    "```bash\n",
    "# For Napari backend (recommended for interactive viewing)\n",
    "pip install 'napari[all]>=0.4.18'\n",
    "\n",
    "# For video export\n",
    "# macOS: brew install ffmpeg\n",
    "# Ubuntu: sudo apt install ffmpeg\n",
    "```\n",
    "\n",
    "**Note**: VideoOverlay requires 2D environments (not supported on 1D linearized tracks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from neurospatial import Environment\n",
    "from neurospatial.animation import PositionOverlay, VideoOverlay, calibrate_video\n",
    "from neurospatial.animation.backends.video_backend import check_ffmpeg_available\n",
    "from neurospatial.ops.transforms import (\n",
    "    VideoCalibration,\n",
    "    calibrate_from_landmarks,\n",
    "    calibrate_from_scale_bar,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path.cwd()\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup: Create Environment and Synthetic Video\n",
    "\n",
    "We'll create:\n",
    "1. A square arena environment (100 x 100 cm)\n",
    "2. A synthetic video with a moving gradient pattern\n",
    "3. A simulated place field and trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_env",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating square arena environment...\")\n",
    "\n",
    "# Square arena (100 x 100 cm)\n",
    "positions = np.array(\n",
    "    [\n",
    "        [5, 5],\n",
    "        [95, 5],\n",
    "        [95, 95],\n",
    "        [5, 95],  # Corners\n",
    "        [50, 50],  # Center\n",
    "    ]\n",
    ")\n",
    "\n",
    "env = Environment.from_samples(\n",
    "    positions=np.random.uniform(0, 100, (1000, 2)),  # Fill arena\n",
    "    bin_size=2.5,\n",
    "    name=\"SquareArena\",\n",
    ")\n",
    "env.units = \"cm\"\n",
    "env.frame = \"behavioral_box\"\n",
    "\n",
    "print(f\"Environment: {env.n_bins} bins, {env.n_dims}D\")\n",
    "print(f\"Spatial extent: {env.dimension_ranges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_video",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating synthetic video...\")\n",
    "\n",
    "try:\n",
    "    import imageio.v3 as iio\n",
    "except ImportError:\n",
    "    import imageio as iio\n",
    "\n",
    "# Video parameters\n",
    "video_width, video_height = 320, 240  # pixels\n",
    "n_video_frames = 100\n",
    "video_fps = 30.0\n",
    "video_path = output_dir / \"synthetic_video.mp4\"\n",
    "\n",
    "# Create frames with moving gradient pattern\n",
    "frames = []\n",
    "for i in range(n_video_frames):\n",
    "    # Create gradient that shifts over time\n",
    "    x = np.linspace(0, 1, video_width)\n",
    "    y = np.linspace(0, 1, video_height)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # Phase shifts create moving pattern\n",
    "    phase = 2 * np.pi * i / n_video_frames\n",
    "\n",
    "    # RGB channels with different patterns\n",
    "    R = (128 + 127 * np.sin(4 * np.pi * X + phase)).astype(np.uint8)\n",
    "    G = (128 + 127 * np.sin(4 * np.pi * Y + phase)).astype(np.uint8)\n",
    "    B = (128 + 127 * np.cos(4 * np.pi * (X + Y) + phase)).astype(np.uint8)\n",
    "\n",
    "    frame = np.stack([R, G, B], axis=-1)\n",
    "    frames.append(frame)\n",
    "\n",
    "# Write video file\n",
    "frames_array = np.array(frames)\n",
    "iio.imwrite(video_path, frames_array, fps=video_fps)\n",
    "\n",
    "print(f\"Video created: {video_path}\")\n",
    "print(f\"  Size: {video_width}x{video_height} pixels\")\n",
    "print(f\"  Frames: {n_video_frames} at {video_fps} fps\")\n",
    "print(f\"  Duration: {n_video_frames / video_fps:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simulate_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSimulating trajectory and place field...\")\n",
    "\n",
    "# Number of animation frames (match video)\n",
    "n_frames = n_video_frames\n",
    "\n",
    "# Trajectory: spiral through arena (ensure within bounds)\n",
    "t = np.linspace(0, 4 * np.pi, n_frames)\n",
    "r = np.linspace(10, 40, n_frames)  # Radius increases (stay within 100x100 arena)\n",
    "trajectory = np.column_stack(\n",
    "    [\n",
    "        50 + r * np.cos(t),  # Center at (50, 50), max radius 40 -> range [10, 90]\n",
    "        50 + r * np.sin(t),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Clip trajectory to environment bounds (with small margin)\n",
    "trajectory = np.clip(trajectory, 5, 95)\n",
    "\n",
    "# Place field following trajectory\n",
    "fields = []\n",
    "for i in range(n_frames):\n",
    "    pos = trajectory[i : i + 1]\n",
    "    center_bin = env.bin_at(pos)[0]\n",
    "    if center_bin >= 0:  # Valid bin\n",
    "        distances = env.distance_to([center_bin])\n",
    "        field = np.exp(-(distances**2) / (2 * 10.0**2))\n",
    "    else:\n",
    "        # Position outside environment - use uniform low field\n",
    "        field = np.ones(env.n_bins) * 0.1\n",
    "    field = np.clip(field + np.random.randn(env.n_bins) * 0.05, 0, 1)\n",
    "    fields.append(field)\n",
    "\n",
    "fields = np.array(fields)\n",
    "\n",
    "# Create frame_times (required for animate_fields)\n",
    "frame_times = np.arange(n_frames) / video_fps  # seconds\n",
    "\n",
    "print(f\"Trajectory: {n_frames} frames\")\n",
    "print(\n",
    "    f\"Trajectory range: x=[{trajectory[:, 0].min():.1f}, {trajectory[:, 0].max():.1f}], y=[{trajectory[:, 1].min():.1f}, {trajectory[:, 1].max():.1f}]\"\n",
    ")\n",
    "print(f\"Fields: {fields.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata_header",
   "metadata": {},
   "source": [
    "## Example 1: Loading and Inspecting Video Metadata\n",
    "\n",
    "Before calibration, inspect the video properties using the `VideoReader` class.\n",
    "\n",
    "**Key properties**:\n",
    "- `n_frames`: Total frame count\n",
    "- `fps`: Frame rate\n",
    "- `frame_size_px`: (width, height) in pixels\n",
    "- `duration`: Video length in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_video",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurospatial.animation._video_io import VideoReader\n",
    "\n",
    "print(\"Example 1: Inspecting Video Metadata\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create reader to inspect video\n",
    "reader = VideoReader(video_path)\n",
    "\n",
    "print(f\"Video file: {video_path.name}\")\n",
    "print(f\"  Frame size: {reader.frame_size_px} (width, height)\")\n",
    "print(f\"  Frame count: {reader.n_frames}\")\n",
    "print(f\"  Frame rate: {reader.fps} fps\")\n",
    "print(f\"  Duration: {reader.duration:.2f} seconds\")\n",
    "\n",
    "# Access a single frame\n",
    "frame_0 = reader[0]\n",
    "print(f\"\\nFrame 0 shape: {frame_0.shape} (height, width, channels)\")\n",
    "print(f\"Frame 0 dtype: {frame_0.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scalebar_header",
   "metadata": {},
   "source": [
    "## Example 2: Calibrating with Scale Bar Method\n",
    "\n",
    "The scale bar method uses two known points in the video and their real-world distance.\n",
    "\n",
    "**Use case**: When you have a ruler or known-length object visible in the video.\n",
    "\n",
    "**Parameters**:\n",
    "- `p1_px`, `p2_px`: Endpoints of scale bar in pixels (x, y)\n",
    "- `known_length_cm`: Real-world length in cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scalebar_calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example 2: Scale Bar Calibration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Assume we have a scale bar spanning the video width\n",
    "# Video is 320 pixels wide, representing 100 cm\n",
    "p1_px = (10, 120)  # Left end of scale bar (x, y) in pixels\n",
    "p2_px = (310, 120)  # Right end of scale bar\n",
    "known_length_cm = 100.0  # 300 pixels = 100 cm\n",
    "\n",
    "# Method 1: Using calibrate_from_scale_bar directly\n",
    "transform = calibrate_from_scale_bar(\n",
    "    p1_px=p1_px,\n",
    "    p2_px=p2_px,\n",
    "    known_length_cm=known_length_cm,\n",
    "    frame_size_px=(video_width, video_height),\n",
    ")\n",
    "\n",
    "calibration_scalebar = VideoCalibration(\n",
    "    transform_px_to_cm=transform,\n",
    "    frame_size_px=(video_width, video_height),\n",
    ")\n",
    "\n",
    "print(f\"Scale bar: {p1_px} to {p2_px}\")\n",
    "print(f\"Known length: {known_length_cm} cm\")\n",
    "print(f\"Computed scale: {calibration_scalebar.cm_per_px:.4f} cm/px\")\n",
    "\n",
    "# Verify calibration by transforming test points\n",
    "test_px = np.array([[160, 120]])  # Center of video\n",
    "test_cm = calibration_scalebar.transform_px_to_cm(test_px)\n",
    "print(f\"\\nCenter pixel {test_px[0]} maps to {test_cm[0]} cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scalebar_convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMethod 2: Using calibrate_video() convenience function\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# The convenience function combines calibration and validation\n",
    "calibration_easy = calibrate_video(\n",
    "    video_path,\n",
    "    env,\n",
    "    scale_bar=(p1_px, p2_px, known_length_cm),\n",
    ")\n",
    "\n",
    "print(\"Calibration created successfully!\")\n",
    "print(f\"  cm_per_px: {calibration_easy.cm_per_px:.4f}\")\n",
    "print(f\"  frame_size_px: {calibration_easy.frame_size_px}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "landmarks_header",
   "metadata": {},
   "source": [
    "## Example 3: Calibrating with Landmark Correspondences\n",
    "\n",
    "The landmark method uses multiple corresponding points between video and environment.\n",
    "\n",
    "**Use case**: When you know the pixel locations of arena corners or markers.\n",
    "\n",
    "**Parameters**:\n",
    "- `landmarks_px`: Points in video pixels (n_points, 2)\n",
    "- `landmarks_env`: Corresponding points in environment cm (n_points, 2)\n",
    "\n",
    "**Note**: Use at least 3 non-collinear points for reliable calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "landmarks_calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example 3: Landmark Calibration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Arena corners in video pixels\n",
    "# Assuming video shows arena with some margin\n",
    "landmarks_px = np.array(\n",
    "    [\n",
    "        [10, 10],  # Top-left corner (pixel coords, origin top-left)\n",
    "        [310, 10],  # Top-right corner\n",
    "        [310, 230],  # Bottom-right corner\n",
    "        [10, 230],  # Bottom-left corner\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Corresponding arena corners in environment coordinates\n",
    "# Environment origin is bottom-left, Y increases upward\n",
    "landmarks_env = np.array(\n",
    "    [\n",
    "        [0, 100],  # Top-left (x=0, y=max)\n",
    "        [100, 100],  # Top-right (x=max, y=max)\n",
    "        [100, 0],  # Bottom-right (x=max, y=0)\n",
    "        [0, 0],  # Bottom-left (x=0, y=0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Landmark correspondences:\")\n",
    "for i, (px, cm) in enumerate(zip(landmarks_px, landmarks_env, strict=True)):\n",
    "    print(f\"  Point {i + 1}: pixel {px} -> env {cm} cm\")\n",
    "\n",
    "# Method 1: Using calibrate_from_landmarks directly\n",
    "transform_lm = calibrate_from_landmarks(\n",
    "    landmarks_px=landmarks_px,\n",
    "    landmarks_cm=landmarks_env,\n",
    "    frame_size_px=(video_width, video_height),\n",
    "    kind=\"similarity\",  # or \"rigid\", \"affine\"\n",
    ")\n",
    "\n",
    "calibration_landmarks = VideoCalibration(\n",
    "    transform_px_to_cm=transform_lm,\n",
    "    frame_size_px=(video_width, video_height),\n",
    ")\n",
    "\n",
    "print(\"\\nCalibration created (similarity transform)\")\n",
    "print(f\"  cm_per_px: {calibration_landmarks.cm_per_px:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "landmarks_convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMethod 2: Using calibrate_video() convenience function\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "calibration_lm_easy = calibrate_video(\n",
    "    video_path,\n",
    "    env,\n",
    "    landmarks_px=landmarks_px,\n",
    "    landmarks_env=landmarks_env,\n",
    ")\n",
    "\n",
    "print(\"Calibration created successfully!\")\n",
    "print(f\"  cm_per_px: {calibration_lm_easy.cm_per_px:.4f}\")\n",
    "\n",
    "# Verify round-trip accuracy\n",
    "transformed = calibration_lm_easy.transform_px_to_cm(landmarks_px)\n",
    "error = np.abs(transformed - landmarks_env).max()\n",
    "print(f\"\\nRound-trip error: {error:.6f} cm (should be ~0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "options_header",
   "metadata": {},
   "source": [
    "## Example 4: Creating VideoOverlay with Various Options\n",
    "\n",
    "VideoOverlay supports several options for controlling appearance:\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|----------|\n",
    "| `alpha` | Opacity (0.0-1.0) | 0.5 |\n",
    "| `z_order` | \"above\" or \"below\" field | \"above\" |\n",
    "| `crop` | (x, y, width, height) | None |\n",
    "| `downsample` | Spatial downsampling factor | 1 |\n",
    "| `times` | Video frame timestamps | Auto from fps |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "videooverlay_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example 4a: Basic VideoOverlay\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic video overlay with calibration\n",
    "video_overlay = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    ")\n",
    "\n",
    "print(\"VideoOverlay created:\")\n",
    "print(f\"  source: {video_path.name}\")\n",
    "print(f\"  alpha: {video_overlay.alpha} (default)\")\n",
    "print(f\"  z_order: {video_overlay.z_order} (default)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "videooverlay_alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExample 4b: Alpha Blending Options\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Field dominant (low alpha)\n",
    "overlay_field_dominant = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    "    alpha=0.3,  # 30% video, 70% field\n",
    ")\n",
    "\n",
    "# Balanced (default)\n",
    "overlay_balanced = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    "    alpha=0.5,  # 50% video, 50% field\n",
    ")\n",
    "\n",
    "# Video dominant (high alpha)\n",
    "overlay_video_dominant = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    "    alpha=0.7,  # 70% video, 30% field\n",
    ")\n",
    "\n",
    "print(\"Alpha options:\")\n",
    "print(\"  alpha=0.3: Field shows through video (field dominant)\")\n",
    "print(\"  alpha=0.5: Equal visibility (balanced, default)\")\n",
    "print(\"  alpha=0.7: Video shows through field (video dominant)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "videooverlay_zorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExample 4c: Z-Order Options\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Video above field (default)\n",
    "overlay_above = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    "    z_order=\"above\",  # Video on top of field\n",
    ")\n",
    "\n",
    "# Video below field (only visible if field has transparency)\n",
    "overlay_below = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    "    z_order=\"below\",  # Video behind field\n",
    ")\n",
    "\n",
    "print(\"Z-order options:\")\n",
    "print(\"  z_order='above': Video on top (default, works with opaque fields)\")\n",
    "print(\"  z_order='below': Video behind (only visible if field has NaN/transparent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "videooverlay_advanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExample 4d: Crop and Downsample\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Crop to region of interest\n",
    "overlay_cropped = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    "    crop=(50, 30, 200, 150),  # (x, y, width, height) in pixels\n",
    ")\n",
    "\n",
    "# Downsample for faster rendering\n",
    "overlay_downsampled = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    "    downsample=2,  # Half resolution (160x120)\n",
    ")\n",
    "\n",
    "print(\"Advanced options:\")\n",
    "print(\"  crop=(50, 30, 200, 150): Crop to 200x150 region starting at (50, 30)\")\n",
    "print(\"  downsample=2: Reduce resolution by factor of 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animate_header",
   "metadata": {},
   "source": [
    "## Example 5: Animating Fields with Video Background\n",
    "\n",
    "Combine the spatial field animation with the video overlay using `animate_fields()`.\n",
    "\n",
    "**Supported backends**:\n",
    "- `napari`: Best for interactive exploration\n",
    "- `video`: Best for exporting MP4 files\n",
    "- `widget`: For Jupyter notebook playback\n",
    "- `html`: NOT supported for VideoOverlay (warning emitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animate_napari",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example 5a: Napari Backend (Interactive)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create video overlay with position tracking\n",
    "video_overlay = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "position_overlay = PositionOverlay(\n",
    "    data=trajectory,\n",
    "    color=\"red\",\n",
    "    size=10.0,\n",
    "    trail_length=10,\n",
    ")\n",
    "\n",
    "try:\n",
    "    import napari\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    print(\"Launching Napari viewer...\")\n",
    "\n",
    "    # IMPORTANT: Clear cache before parallel rendering\n",
    "    env.clear_cache()\n",
    "\n",
    "    viewer = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[video_overlay, position_overlay],\n",
    "        frame_times=frame_times,\n",
    "        backend=\"napari\",\n",
    "        fps=video_fps,\n",
    "        title=\"Video + Field Animation\",\n",
    "    )\n",
    "\n",
    "    print(\"\\nNapari viewer opened:\")\n",
    "    print(\"  - Video layer with field overlay\")\n",
    "    print(\"  - Position tracking with trail\")\n",
    "    print(\"  - Use slider or play button to animate\")\n",
    "\n",
    "    if get_ipython() is None:\n",
    "        napari.run()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Napari not available. Install with: pip install 'napari[all]>=0.4.18'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_header",
   "metadata": {},
   "source": [
    "## Example 6: Exporting Synchronized Video\n",
    "\n",
    "Export the animation with video overlay to an MP4 file.\n",
    "\n",
    "**Key parameters**:\n",
    "- `n_workers`: Number of parallel workers (speeds up rendering)\n",
    "- `fps`: Output frame rate\n",
    "- `dpi`: Output resolution\n",
    "\n",
    "**Important**: Call `env.clear_cache()` before parallel rendering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_video",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example 6: Exporting Video with Overlay\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if check_ffmpeg_available():\n",
    "    output_path = output_dir / \"18_video_overlay_export.mp4\"\n",
    "\n",
    "    # IMPORTANT: Clear cache before parallel rendering\n",
    "    env.clear_cache()\n",
    "\n",
    "    result = env.animate_fields(\n",
    "        fields,\n",
    "        overlays=[video_overlay, position_overlay],\n",
    "        frame_times=frame_times,\n",
    "        backend=\"video\",\n",
    "        save_path=output_path,\n",
    "        fps=video_fps,\n",
    "        n_workers=4,  # Parallel rendering\n",
    "        dpi=100,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nVideo exported to: {result}\")\n",
    "    print(f\"  Frames: {n_frames}\")\n",
    "    print(f\"  FPS: {video_fps}\")\n",
    "    print(\"  Workers: 4 (parallel)\")\n",
    "else:\n",
    "    print(\"ffmpeg not available for video export.\")\n",
    "    print(\"Install with: brew install ffmpeg (macOS) or apt install ffmpeg (Linux)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example 6b: Widget Backend (Jupyter Playback)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    from IPython.display import display\n",
    "\n",
    "    if get_ipython() is not None:\n",
    "        # Clear cache before rendering\n",
    "        env.clear_cache()\n",
    "\n",
    "        # Use only first 30 frames for quick demo\n",
    "        n_demo = 30\n",
    "\n",
    "        # Create video timestamps for the full video\n",
    "        video_times = np.arange(n_video_frames) / video_fps\n",
    "        # Create field timestamps for the demo subset\n",
    "        field_times = video_times[:n_demo]\n",
    "\n",
    "        widget = env.animate_fields(\n",
    "            fields[:n_demo],\n",
    "            overlays=[\n",
    "                VideoOverlay(\n",
    "                    source=video_path,\n",
    "                    calibration=calibration_landmarks,\n",
    "                    times=video_times,  # Full video timestamps\n",
    "                    alpha=0.5,\n",
    "                ),\n",
    "                PositionOverlay(\n",
    "                    data=trajectory[:n_demo],\n",
    "                    color=\"red\",\n",
    "                    trail_length=10,\n",
    "                ),\n",
    "            ],\n",
    "            frame_times=field_times,  # Field timestamps for alignment\n",
    "            backend=\"widget\",\n",
    "            fps=10,\n",
    "        )\n",
    "        print(\"Widget created - use slider to navigate\")\n",
    "        display(widget)\n",
    "    else:\n",
    "        print(\"Not in Jupyter notebook environment\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"IPython/ipywidgets not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_header",
   "metadata": {},
   "source": [
    "## Example 7: Performance Tips for Large Videos\n",
    "\n",
    "Behavioral videos can be 30+ minutes at 30fps = 50,000+ frames. Here's how to handle them efficiently:\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "1. **LRU Caching**: VideoReader caches recently accessed frames (default: 100 frames)\n",
    "2. **Streaming**: Frames are loaded on-demand, never all at once\n",
    "3. **Downsampling**: Reduce resolution for faster rendering\n",
    "\n",
    "### Rendering Optimization\n",
    "\n",
    "1. **Parallel Export**: Use `n_workers > 1` for video export\n",
    "2. **Subsampling**: Reduce frame count for preview\n",
    "3. **Clear Cache**: Always call `env.clear_cache()` before parallel rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_tips",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurospatial.animation import subsample_frames\n",
    "\n",
    "print(\"Example 7: Performance Optimization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Tip 1: Adjust cache size for your workflow\n",
    "reader_small_cache = VideoReader(video_path, cache_size=50)\n",
    "reader_large_cache = VideoReader(video_path, cache_size=200)\n",
    "\n",
    "print(\"Tip 1: Adjust cache size\")\n",
    "print(\"  cache_size=50: Low memory, good for random access\")\n",
    "print(\"  cache_size=200: Higher memory, better for sequential playback\")\n",
    "\n",
    "# Tip 2: Downsample for faster rendering\n",
    "print(\"\\nTip 2: Downsample for speed\")\n",
    "overlay_fast = VideoOverlay(\n",
    "    source=video_path,\n",
    "    calibration=calibration_landmarks,\n",
    "    downsample=2,  # Half resolution\n",
    ")\n",
    "print(f\"  Original: {video_width}x{video_height} pixels\")\n",
    "print(f\"  Downsampled: {video_width // 2}x{video_height // 2} pixels\")\n",
    "\n",
    "# Tip 3: Subsample frames for preview\n",
    "print(\"\\nTip 3: Subsample frames for quick preview\")\n",
    "# Subsample from 30fps to 10fps\n",
    "fields_subsampled = subsample_frames(fields, source_fps=30, target_fps=10)\n",
    "print(f\"  Original: {len(fields)} frames at 30fps\")\n",
    "print(f\"  Subsampled: {len(fields_subsampled)} frames at 10fps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cache_warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTip 4: CRITICAL - Clear cache before parallel rendering\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# This is REQUIRED for parallel video export\n",
    "env.clear_cache()\n",
    "\n",
    "print(\"Always call env.clear_cache() before animate_fields() with n_workers > 1\")\n",
    "print(\"\")\n",
    "print(\"Why? The Environment object must be pickle-able for multiprocessing.\")\n",
    "print(\"Cached KDTree and kernel matrices cannot be pickled.\")\n",
    "print(\"\")\n",
    "print(\"Example:\")\n",
    "print(\"  env.clear_cache()  # Make environment pickle-able\")\n",
    "print(\"  env.animate_fields(fields, overlays=[video], n_workers=4, ...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove the synthetic video file created for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup synthetic video\n",
    "if video_path.exists():\n",
    "    video_path.unlink()\n",
    "    print(f\"Removed: {video_path}\")\n",
    "\n",
    "# Cleanup exported video\n",
    "export_path = output_dir / \"18_video_overlay_export.mp4\"\n",
    "if export_path.exists():\n",
    "    export_path.unlink()\n",
    "    print(f\"Removed: {export_path}\")\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Calibration Methods\n",
    "\n",
    "1. **Scale Bar**: Use when you have a known-length reference in the video\n",
    "   ```python\n",
    "   calibration = calibrate_video(\n",
    "       \"video.mp4\", env,\n",
    "       scale_bar=((x1, y1), (x2, y2), length_cm)\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Landmarks**: Use when you know pixel locations of arena corners\n",
    "   ```python\n",
    "   calibration = calibrate_video(\n",
    "       \"video.mp4\", env,\n",
    "       landmarks_px=corners_px,\n",
    "       landmarks_env=corners_cm\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Direct Scale**: Use when you know the exact cm/pixel ratio\n",
    "   ```python\n",
    "   calibration = calibrate_video(\n",
    "       \"video.mp4\", env,\n",
    "       cm_per_px=0.25\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### VideoOverlay Best Practices\n",
    "\n",
    "| Goal | Settings |\n",
    "|------|----------|\n",
    "| Balanced view | `alpha=0.5, z_order=\"above\"` (default) |\n",
    "| Field dominant | `alpha=0.3, z_order=\"above\"` |\n",
    "| Video dominant | `alpha=0.7, z_order=\"above\"` |\n",
    "| Video background | `z_order=\"below\"` (needs transparent field) |\n",
    "\n",
    "### Backend Support\n",
    "\n",
    "| Backend | VideoOverlay Support |\n",
    "|---------|---------------------|\n",
    "| Napari | Full support |\n",
    "| Video | Full support |\n",
    "| Widget | Full support |\n",
    "| HTML | NOT supported (warning emitted) |\n",
    "\n",
    "### Performance Checklist\n",
    "\n",
    "- [ ] Call `env.clear_cache()` before parallel rendering\n",
    "- [ ] Use `downsample=2` or higher for faster preview\n",
    "- [ ] Use `subsample_frames()` to reduce frame count\n",
    "- [ ] Adjust `cache_size` in VideoReader based on workflow\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Apply VideoOverlay to your own behavioral recordings\n",
    "- Combine with other overlays (Position, Bodypart, HeadDirection)\n",
    "- Export publication-quality videos with synchronized behavior\n",
    "\n",
    "For more details, see:\n",
    "- `examples/17_animation_with_overlays.ipynb` - Other overlay types\n",
    "- `examples/16_field_animation.ipynb` - Animation backends"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
