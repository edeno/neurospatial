# movement Analysis: Tracking Data Processing

**Package**: [neuroinformatics-unit/movement](https://github.com/neuroinformatics-unit/movement)
**Lab**: Neuroinformatics Unit, UCL
**Language**: Python 100%
**Version**: 0.10.0 (actively developed - last update November 6, 2025)
**License**: BSD 3-Clause
**Stars**: 211, **Forks**: 73, **Open Issues**: 145

---

## 1. Package Overview

**movement** is a modern Python toolbox for analyzing animal body movements across space and time. It provides "a consistent, modular interface for analysing motion tracks" generated by deep learning tracking tools.

**Core mission**: Fill the gap between pose tracking (DeepLabCut, SLEAP) and downstream analysis by providing standardized data processing, cleaning, visualization, and kinematic quantification.

**Target users**: Neuroscientists, ethologists, biomechanics researchers using pose estimation tools.

---

## 2. Package Structure

### Module Organization

```
movement/
├── __init__.py             # Package initialization
├── cli_entrypoint.py       # Command-line interface
├── filtering.py            # Data filtering (9.5 KB)
├── sample_data.py          # Sample data management (12.2 KB)
├── transforms.py           # Data transformations
├── io/                     # Input/output operations
│   ├── load_poses.py       # Load pose data (35 KB) ⭐
│   ├── save_poses.py       # Save pose data (19 KB)
│   ├── load_bboxes.py      # Load bounding boxes (27 KB)
│   ├── save_bboxes.py      # Save bounding boxes (18 KB)
│   └── nwb.py              # NWB format support (25 KB)
├── kinematics/             # Kinematic analysis ⭐
│   ├── kinematics.py       # Core kinematics (18 KB)
│   ├── distances.py        # Distance calculations (12 KB)
│   ├── orientation.py      # Orientation/heading (10 KB)
│   └── kinetic_energy.py   # Energy calculations (6 KB)
├── napari/                 # napari GUI integration
├── plots/                  # Visualization functions
├── roi/                    # Region of interest handling
├── utils/                  # Utility functions
└── validators/             # Data validation
```

---

## 3. Core Capabilities

### 3.1 Data Loading (io/)

**Supported pose tracking frameworks**:

| Framework | File Formats | Notes |
|-----------|--------------|-------|
| **DeepLabCut** | .h5, .csv | Most popular pose tracker |
| **SLEAP** | .h5, .slp | Multi-animal tracking |
| **LightningPose** | .csv | GPU-accelerated tracking |
| **Anipose** | .csv | 3D multi-camera tracking |
| **NWB** | .nwb | Neurodata Without Borders |
| **NumPy** | arrays | Direct array input |

**Main functions**:

```python
from movement.io import load_poses

# Universal loader
ds = load_poses.from_file(
    "tracks.h5",
    source_software="DeepLabCut",
    fps=30,
)

# Software-specific loaders
ds = load_poses.from_dlc_file("DLC_tracks.h5")
ds = load_poses.from_sleap_file("SLEAP_analysis.h5")
ds = load_poses.from_lp_file("lightning_pose.csv")
ds = load_poses.from_anipose_file("anipose_3d.csv")

# Multi-view merging
ds = load_poses.from_multiview_files(
    file_paths=["cam1.h5", "cam2.h5"],
    source_software="DeepLabCut",
)

# From NumPy arrays
ds = load_poses.from_numpy(
    position_array,     # (n_frames, n_space, n_keypoints, n_individuals)
    confidence_array,   # (n_frames, n_keypoints, n_individuals)
    fps=30,
)
```

**Data structure returned**: **xarray.Dataset** ⭐

```python
<xarray.Dataset>
Dimensions:
    time: n_frames
    space: (x, y) or (x, y, z)
    keypoints: (nose, left_ear, right_ear, ...)
    individuals: (ind1, ind2, ...)

Data variables:
    position: (time, space, keypoints, individuals)
        Spatial coordinates with units (pixels or cm)
    confidence: (time, keypoints, individuals)
        Point-wise confidence scores [0, 1]

Coordinates:
    time: frame timestamps
    space: x, y, z
    keypoints: body part names
    individuals: animal IDs

Attributes:
    source_software: "DeepLabCut"
    fps: 30.0
    time_unit: "seconds"
    source_file: "/path/to/tracks.h5"
```

**Why xarray**: Labeled multi-dimensional arrays with metadata (similar to pandas but for N-D data).

---

### 3.2 Kinematics (kinematics/)

**Core motion calculations**:

```python
from movement import kinematics

# Velocity (first derivative of position)
velocity = kinematics.compute_velocity(ds)
# Returns: xarray.DataArray, shape (time, space, keypoints, individuals)

# Acceleration (second derivative of position)
acceleration = kinematics.compute_acceleration(ds)

# Speed (Euclidean norm of velocity)
speed = kinematics.compute_speed(ds)
# Returns: scalar quantity (no direction)

# Path length (cumulative distance traveled)
path_length = kinematics.compute_path_length(
    ds,
    start=0,
    stop=100,  # Frame range
)
# Returns: total distance traveled between frames
```

**Displacement calculations**:

```python
# Forward displacement (position[t+1] - position[t])
forward_disp = kinematics.compute_forward_displacement(ds)

# Backward displacement (position[t] - position[t-1])
backward_disp = kinematics.compute_backward_displacement(ds)
```

**General derivatives**:

```python
# Nth-order time derivative
# Uses second-order accurate central differences
nth_derivative = kinematics.compute_time_derivative(
    ds,
    order=3,  # Third derivative (jerk)
)
```

**Distances** (`distances.py`):

```python
from movement.kinematics import distances

# Compute pairwise distances between keypoints
dist = distances.compute_pairwise_distances(ds)
# Example: Distance between nose and tail
```

**Orientation** (`orientation.py`):

```python
from movement.kinematics import orientation

# Compute heading/orientation
heading = orientation.compute_heading(
    ds,
    left_keypoint="left_ear",
    right_keypoint="right_ear",
)
# Returns: angle in radians
```

**Kinetic energy** (`kinetic_energy.py`):

```python
from movement.kinematics import kinetic_energy

# Compute kinetic energy
ke = kinetic_energy.compute_kinetic_energy(
    ds,
    mass=0.025,  # kg (e.g., mouse)
)
# KE = 0.5 * m * v^2
```

---

### 3.3 Filtering (filtering.py)

**Data cleaning and smoothing**:

```python
from movement import filtering

# Median filter (remove outliers)
ds_filtered = filtering.filter_by_median(
    ds,
    window=5,  # frames
)

# Savitzky-Golay filter (polynomial smoothing)
ds_smooth = filtering.filter_by_savgol(
    ds,
    window=11,
    polyorder=3,
)

# Interpolate missing data
ds_interp = filtering.interpolate_missing_data(
    ds,
    method="linear",  # or "cubic"
    max_gap=10,       # Max frames to interpolate
)
```

---

### 3.4 Regions of Interest (roi/)

**Define and analyze spatial regions**:

```python
from movement.roi import Polygon, Line

# Define ROI (polygon)
goal_region = Polygon(
    vertices=[[x1, y1], [x2, y2], [x3, y3], [x4, y4]]
)

# Check if points are in ROI
in_goal = goal_region.contains_point(ds.position)
# Returns: boolean xarray

# Distance to ROI
distance = goal_region.compute_distance_to(ds.position)

# Angle to ROI (allocentric - fixed reference)
angle = goal_region.compute_allocentric_angle_to_nearest_point(
    ds.position,
    reference_point=[0, 0],
)

# Angle to ROI (egocentric - based on heading)
ego_angle = goal_region.compute_egocentric_angle_to_nearest_point(
    ds.position,
    heading_vector=velocity,
)
```

**Geometric queries**:
- `contains_point()` - Check if position in region
- `compute_distance_to()` - Euclidean distance to region
- `compute_nearest_point_to()` - Closest point on region boundary
- `compute_approach_vector()` - Vector pointing toward region

**Limitations**:
- ❌ No time-in-region calculation
- ❌ No entry/exit detection
- ❌ No region crossing events
- ⚠️ Geometric queries only (no temporal analysis)

---

### 3.5 Visualization (plots/)

**Occupancy plotting** ⭐:

```python
from movement.plots import plot_occupancy

# Plot 2D occupancy heatmap
fig, ax, hist_info = plot_occupancy(
    ds.position,
    individuals="mouse1",
    keypoints="centroid",
    bins=50,  # Passed to matplotlib hist2d
    cmap='hot',
)

# Returns:
# - fig, ax: matplotlib objects
# - hist_info: dict with 'counts', 'x_edges', 'y_edges'
```

**How it works**:
- Wraps `matplotlib.pyplot.hist2d()`
- Computes 2D histogram of position data
- Supports filtering by individuals/keypoints
- Auto-computes centroid if multiple keypoints
- Removes NaN values automatically

**Limitations**:
- ⚠️ **Visualization only** - Returns plot, not occupancy data
- ⚠️ **No occupancy array** - Can't reuse for analysis
- ⚠️ **No normalization** - Raw counts, not time-normalized
- ⚠️ **Fixed bins** - Uses matplotlib's binning (no custom edges)

**Trajectory plotting**:

```python
from movement.plots import plot_trajectory

# Plot trajectory path
fig, ax = plot_trajectory(
    ds.position,
    individuals="mouse1",
    keypoints="nose",
)
```

**napari integration** (GUI):

```python
import napari
from movement.napari import add_tracks_to_napari

# Launch interactive viewer
viewer = napari.Viewer()
add_tracks_to_napari(viewer, ds)
napari.run()
```

---

### 3.6 Data Validation (validators/)

**Quality checks**:

```python
from movement import validators

# Validate dataset structure
validators.validate_dataset(ds)

# Check for missing data
missing_report = validators.check_missing_data(ds)

# Check confidence thresholds
low_confidence = validators.check_confidence_threshold(
    ds,
    threshold=0.9,
)
```

---

## 4. Comparison with neurospatial

### Complementary Positioning

| Aspect | movement | neurospatial |
|--------|----------|--------------|
| **Focus** | Tracking data → cleaned trajectories | Trajectories → spatial discretization |
| **Input** | Raw pose tracking files | Position time-series |
| **Output** | xarray.Dataset (continuous) | Discrete bins + graph |
| **Kinematics** | ✅ Velocity, acceleration, speed | ❌ (use movement) |
| **Spatial discretization** | ❌ | ✅ Any topology |
| **Place fields** | ❌ | ✅ Planned |
| **Grid cells** | ❌ | ✅ Planned |
| **Behavioral segmentation** | ⚠️ Basic ROI | ✅ Advanced (runs, laps, trials) |
| **Occupancy** | ⚠️ Plot only (hist2d) | ✅ Data + normalization |
| **File format support** | ✅ DLC, SLEAP, LP, etc. | ❌ (assumes clean data) |
| **Filtering/smoothing** | ✅ Extensive | ⚠️ Basic |
| **Visualization** | ✅ napari GUI + occupancy | ⚠️ matplotlib |

---

## 5. Integration Workflow

### Complete Pipeline: movement → neurospatial → analysis

```python
# ========================================
# Step 1: Load and clean tracking data (movement)
# ========================================
from movement.io import load_poses
from movement import filtering, kinematics

# Load DeepLabCut tracking
ds = load_poses.from_dlc_file("DLC_tracks.h5", fps=30)

# Filter outliers and smooth
ds = filtering.filter_by_median(ds, window=5)
ds = filtering.filter_by_savgol(ds, window=11, polyorder=3)

# Interpolate missing data
ds = filtering.interpolate_missing_data(ds, method="cubic", max_gap=10)

# Extract centroid position
centroid = ds.position.sel(keypoint="centroid")  # (time, space)
position_array = centroid.values  # (n_frames, 2) NumPy array
times = ds.coords["time"].values

# Compute velocity (for behavioral segmentation)
velocity = kinematics.compute_velocity(ds)
speed = kinematics.compute_speed(ds)

# ========================================
# Step 2: Spatial discretization (neurospatial)
# ========================================
from neurospatial import Environment
from neurospatial.segmentation import (
    detect_runs_between_regions,
    segment_by_velocity,
)

# Create environment
env = Environment.from_samples(
    position_array,
    bin_size=2.5,  # cm
    units='cm',
)

# Define regions
env.regions.add('nest', point=[10, 10])
env.regions.add('goal', point=[90, 90])

# ========================================
# Step 3: Behavioral segmentation (neurospatial)
# ========================================

# Segment by velocity (movement vs rest)
movement_epochs = segment_by_velocity(
    position_array,
    times,
    threshold=5.0,  # cm/s
)

# Detect goal-directed runs
runs = detect_runs_between_regions(
    position_array,
    times,
    env,
    source='nest',
    target='goal',
    velocity_threshold=5.0,
)

# ========================================
# Step 4: Neural analysis (pynapple + neurospatial)
# ========================================
import pynapple as nap
from neurospatial.metrics import skaggs_information

# Load spikes
spikes = nap.load_file('spikes.nwb')

# Restrict to movement periods
spikes_movement = spikes.restrict(movement_epochs)

# Compute tuning curves per run
for run in runs:
    spikes_run = spikes.restrict(run.start_time, run.end_time)

    # Map trajectory to bins
    run_bins = env.bin_at(run.trajectory_positions)

    # Compute occupancy
    occupancy = env.occupancy(run.times, run.trajectory_positions)

    # Compute firing rate
    firing_rate = compute_firing_rate(spikes_run, run_bins, occupancy)

    # Spatial information
    info = skaggs_information(firing_rate, occupancy)

    print(f"Run {run.id}: Skaggs info = {info:.3f} bits/spike")
```

---

## 6. What movement DOES Provide

### Strengths ✅

1. **Comprehensive file format support** - DLC, SLEAP, LightningPose, Anipose, NWB
2. **Data cleaning pipeline** - Filtering, smoothing, interpolation
3. **Kinematic analysis** - Velocity, acceleration, speed, path length
4. **Occupancy visualization** - 2D heatmap plotting (matplotlib hist2d wrapper)
5. **Multi-view tracking** - Merge data from multiple cameras
6. **xarray data structure** - Labeled N-D arrays with metadata
7. **napari GUI** - Interactive visualization
8. **Active development** - Modern Python package, well-maintained
9. **Standardized interface** - Consistent API across tracking tools

### Example Use Cases

**Use movement for**:
- ✅ Loading tracking data from DLC/SLEAP
- ✅ Cleaning noisy trajectories
- ✅ Computing velocity, acceleration
- ✅ Filtering outliers
- ✅ Multi-camera 3D tracking
- ✅ General kinematic analysis
- ✅ Interactive trajectory visualization

---

## 7. What movement DOES NOT Provide

### Gaps ❌

**Spatial analysis**:
1. ❌ **No spatial discretization** - Continuous coordinates only
2. ❌ **No environment abstraction** - No graph, no bins, no connectivity
3. ❌ **No spatial metrics** - No Skaggs info, grid score, place fields
4. ❌ **No graph operations** - No neighbors, paths, distances on graphs
5. ❌ **Occupancy for visualization only** - Returns plot, not occupancy data
   - Cannot compute firing rate (spikes/occupancy)
   - No time-normalized occupancy
   - No occupancy as reusable array
6. ❌ **Basic ROI only** - Geometric queries, no temporal analysis (time in region, entry/exit events)

**Behavioral analysis**:
7. ❌ **No lap detection** - Can't automatically find laps/trials
8. ❌ **No run segmentation** - Can't detect runs between regions
9. ❌ **No trial labeling** - Can't identify T-maze left/right trials
10. ❌ **No trajectory similarity** - Can't compare trajectory segments

**Neural analysis**:
11. ❌ **No spike train integration** - Assumes tracking only
12. ❌ **No place field detection**
13. ❌ **No replay analysis**
14. ❌ **No decoding**

**Platform limitations**:
15. ❌ **Irregular graphs not supported** - Assumes 2D/3D continuous space
16. ❌ **No track linearization** - Can't map complex mazes to 1D

---

## 8. Strategic Positioning

### Four-Tool Ecosystem

**Complete workflow**: tracking → cleaning → discretization → analysis

```
movement          →  neurospatial      →  pynapple       →  User's packages
(data cleaning)      (spatial bins)       (time-series)     (replay/decoding)

DLC/SLEAP files   →  Environment       →  Ts, Tsd        →  SortedSpikesClassifier
Filtering         →  Regions           →  IntervalSet    →  Kay_ripple_detector
Kinematics        →  Graph             →  Tuning curves
xarray.Dataset    →  Bin sequence      →  Restrict ops
```

| Tool | Role | Input | Output |
|------|------|-------|--------|
| **movement** | Track cleaning | DLC/SLEAP files | xarray.Dataset (continuous) |
| **neurospatial** | Spatial discretization | Position arrays | Environment (discrete bins + graph) |
| **pynapple** | Time-series | Spike times, epochs | Ts, Tsd, IntervalSet |
| **User packages** | Replay/decoding | Discretized environment + spikes | Decoded trajectories |

---

## 9. Integration Opportunities

### 9.1 Direct Integration

**movement → neurospatial**:

```python
# movement provides clean position data
ds = load_poses.from_dlc_file("tracks.h5")
ds = filtering.filter_by_median(ds, window=5)

# neurospatial discretizes into bins
position_array = ds.position.sel(keypoint="centroid").values
env = Environment.from_samples(position_array, bin_size=2.5)
```

**Seamless handoff**: xarray → NumPy array → neurospatial Environment

### 9.2 Velocity-Based Segmentation

**movement computes velocity**:

```python
velocity = kinematics.compute_velocity(ds)
speed = kinematics.compute_speed(ds)
```

**neurospatial segments by velocity**:

```python
# Extract speed array
speed_array = speed.sel(keypoint="centroid").values

# Segment movement vs rest
movement_epochs = segment_by_velocity(
    position_array,
    times,
    threshold=5.0,
)
```

**Benefit**: Use movement's validated velocity computation with neurospatial's spatial segmentation.

### 9.3 ROI Enhancement

**movement has basic ROI**:

```python
from movement import roi
in_goal = roi.is_in_region(ds, region=goal_polygon, keypoint="centroid")
```

**neurospatial provides advanced segmentation**:

```python
# Detect runs to goal (not just "in goal")
runs = detect_runs_between_regions(
    position_array, times, env,
    source='nest', target='goal'
)

# Each run has trajectory, duration, success, path length
```

**Benefit**: movement detects "in region", neurospatial detects "traveled from A to B".

---

## 10. What neurospatial Should NOT Replicate

### Leave to movement

**Don't implement in neurospatial**:
1. ❌ **File format loaders** - movement has DLC/SLEAP/etc. covered
2. ❌ **Data filtering** - movement's median/Savgol filters are excellent
3. ❌ **Outlier removal** - movement's validators handle this
4. ❌ **Interpolation** - movement has robust missing data handling
5. ❌ **Kinematic derivatives** - movement computes velocity/acceleration correctly
6. ❌ **Multi-view tracking** - movement merges camera views
7. ❌ **napari GUI** - movement provides interactive visualization

**Rationale**: movement is the SPECIALIST for tracking data processing. neurospatial should focus on SPATIAL PRIMITIVES.

---

## 11. What neurospatial Should Provide

### Unique value proposition

**Implement in neurospatial** (movement lacks):
1. ✅ **Spatial discretization** - Bin continuous space into graphs
2. ✅ **Environment representation** - Bins + connectivity + regions
3. ✅ **Behavioral segmentation** - Runs, laps, trials (spatial criteria)
4. ✅ **Spatial metrics** - Skaggs info, grid score, coherence
5. ✅ **Place field detection** - Detect spatial fields
6. ✅ **Graph operations** - Neighbors, paths, distances
7. ✅ **Irregular topologies** - Mazes, tracks, complex environments
8. ✅ **Trajectory similarity** - Compare replay to behavioral runs

**Rationale**: neurospatial fills gaps movement doesn't address.

---

## 12. Recommendations

### For neurospatial Documentation

**Add section**: "Integration with movement"

```markdown
# Integration with movement

neurospatial works seamlessly with the `movement` package for end-to-end tracking analysis.

## Workflow

1. **Load and clean** (movement): Load DLC/SLEAP files, filter, smooth
2. **Discretize** (neurospatial): Create Environment from cleaned positions
3. **Segment** (neurospatial): Detect runs, laps, trials
4. **Analyze** (neurospatial + pynapple): Compute spatial metrics

## Example

```python
# 1. Clean tracking data
from movement.io import load_poses
from movement import filtering

ds = load_poses.from_dlc_file("tracks.h5")
ds = filtering.filter_by_median(ds, window=5)

# 2. Extract position
position = ds.position.sel(keypoint="centroid").values

# 3. Discretize with neurospatial
from neurospatial import Environment
env = Environment.from_samples(position, bin_size=2.5)

# 4. Behavioral segmentation
from neurospatial.segmentation import detect_runs_between_regions
runs = detect_runs_between_regions(position, times, env, 'nest', 'goal')
```
```

### For movement Contributors

**Potential contribution**: Document integration with neurospatial for spatial neuroscience workflows.

**Example tutorial**: "From DLC to Place Fields: Using movement + neurospatial"

---

## 13. Key Takeaways

### What movement Validates

✅ **File format support is critical** - DLC/SLEAP are industry standard
✅ **xarray is appropriate** for continuous tracking data
✅ **Filtering/smoothing is essential** - Raw tracking data is noisy
✅ **Kinematic analysis is common** - Velocity, speed, path length are fundamental
✅ **ROI is basic** - movement's ROI is simple polygon checks (neurospatial can extend)

### What movement Lacks

❌ **Spatial discretization** - No bins, no graph
❌ **Behavioral segmentation** - No automatic lap/run/trial detection
❌ **Neural integration** - No spike trains, no place fields
❌ **Irregular topologies** - Assumes continuous 2D/3D space

### Strategic Position Confirmed

**movement and neurospatial are PERFECTLY COMPLEMENTARY**:

- **movement**: Tracking specialist (DLC → clean trajectories)
- **neurospatial**: Spatial specialist (trajectories → discrete environment)

**No overlap, no competition** - They solve different problems.

---

## 14. Impact on neurospatial Implementation Plan

### No Changes Needed ✅

**movement analysis confirms**:
1. ✅ Spatial discretization is unique value (movement doesn't do this)
2. ✅ Behavioral segmentation needed (movement's ROI is basic)
3. ✅ File format loaders NOT needed (movement has this covered)
4. ✅ Velocity computation NOT needed (movement has this)
5. ✅ Focus on spatial primitives (movement handles kinematics)

**Recommendation**: Document movement integration, don't replicate its functionality.

### Documentation Addition

**Add example notebook** (Phase 5): `15_movement_integration.ipynb`

```python
# Show complete workflow:
# movement → neurospatial → pynapple → user packages
```

**Effort**: 1 day (documentation only, no code changes)

---

## 15. Conclusion

### Summary

**movement** is a modern, well-designed Python package for:
- ✅ Loading tracking data (DLC, SLEAP, LightningPose, etc.)
- ✅ Cleaning and filtering trajectories
- ✅ Computing kinematics (velocity, acceleration, speed)
- ✅ Basic ROI analysis
- ✅ Interactive visualization (napari)

**However**, it is **limited to continuous coordinate space** and **lacks**:
- ❌ Spatial discretization (bins, graphs)
- ❌ Advanced behavioral segmentation (runs, laps, trials)
- ❌ Neural data integration
- ❌ Spatial metrics (Skaggs info, grid score)

**neurospatial fills these gaps PERFECTLY**.

### Complementary Ecosystem (FINAL)

| Package | Specialty | Relation to neurospatial |
|---------|-----------|--------------------------|
| **movement** ⭐ | Tracking data cleaning | Upstream (provides clean positions) |
| **neurospatial** | Spatial discretization | Core (bins + graph + metrics) |
| **pynapple** | Time-series infrastructure | Parallel (epochs, restrict operations) |
| **opexebo** | Spatial metrics (grids) | Validation target (regular grids) |
| **User packages** | Replay/decoding | Downstream (consumes environment) |

**The ecosystem is COMPLETE**: tracking → cleaning → discretization → time-series → analysis.

---

## 16. Package Metadata

| Property | Value |
|----------|-------|
| **GitHub** | neuroinformatics-unit/movement |
| **Version** | 0.10.0 |
| **Language** | Python 100% |
| **License** | BSD 3-Clause |
| **Stars** | 211 |
| **Forks** | 73 |
| **Issues** | 145 (active development) |
| **Last updated** | November 6, 2025 |
| **Documentation** | movement.neuroinformatics.dev |
| **Installation** | `conda install -c conda-forge movement` |
| **Dependencies** | xarray, numpy, pandas, napari (optional) |
| **Key maintainers** | Neuroinformatics Unit, UCL |

---

**Package comparison summary** (15 packages analyzed):

| Package | Language | Platform | Unique Value | Limitation |
|---------|----------|----------|--------------|------------|
| **movement** ⭐ | Python | Cross | Track cleaning, kinematics | No spatial discretization |
| **neurospatial** | Python | Cross | Any graph, spatial primitives | No tracking data support (use movement) |
| **pynapple** | Python | Cross | Time-series excellence | No spatial discretization |
| **opexebo** | Python | Cross | Nobel validation, metrics | Regular grids only |
| **neurocode** | MATLAB | MATLAB | Full pipeline, replay | Regular grids, MATLAB-only |
| **buzcode** | MATLAB | MATLAB | Preprocessing, 1D | MATLAB-only, minimal metrics |
| **vandermeerlab** | MATLAB | MATLAB | Task workflows | 1D only, MATLAB |

**neurospatial + movement + pynapple = Complete Python spatial neuroscience toolkit** ✅
